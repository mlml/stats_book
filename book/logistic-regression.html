<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Quantitative Methods for Linguistic Data</title>
  <meta name="description" content="Quantitative Methods for Linguistic Data">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Quantitative Methods for Linguistic Data" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Quantitative Methods for Linguistic Data" />
  
  
  

<meta name="author" content="Morgan Sonderegger, Michael Wagner, Francisco Torreira">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="cda.html">
<link rel="next" href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Quantitative Methods for Linguistic Data</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html"><i class="fa fa-check"></i><b>1</b> Inferential statistics: Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html#sample-population"><i class="fa fa-check"></i><b>1.1</b> Population vs. sample</a><ul>
<li class="chapter" data-level="1.1.1" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html#sample-to-population-high-level"><i class="fa fa-check"></i><b>1.1.1</b> Sample <span class="math inline">\(\to\)</span> population: High level</a></li>
<li class="chapter" data-level="1.1.2" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html#sdsm"><i class="fa fa-check"></i><b>1.1.2</b> Sampling distribution of the sample mean</a></li>
<li class="chapter" data-level="1.1.3" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html#sampling-from-a-non-normal-distribution"><i class="fa fa-check"></i><b>1.1.3</b> Sampling from a non-normal distribution</a></li>
<li class="chapter" data-level="" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html#exercises"><i class="fa fa-check"></i>Exercises</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html#confidence-intervals"><i class="fa fa-check"></i><b>1.2</b> Confidence intervals</a></li>
<li class="chapter" data-level="1.3" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html#t-distribution"><i class="fa fa-check"></i><b>1.3</b> <span class="math inline">\(t\)</span> distribution</a><ul>
<li class="chapter" data-level="1.3.1" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html#t-based-confidence-intervals"><i class="fa fa-check"></i><b>1.3.1</b> <span class="math inline">\(t\)</span>-based confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html#other-reading"><i class="fa fa-check"></i><b>1.4</b> Other reading</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>2</b> Hypothesis testing</a><ul>
<li class="chapter" data-level="2.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#hypothesis-testing-high-level"><i class="fa fa-check"></i><b>2.1</b> Hypothesis testing: High-level</a></li>
<li class="chapter" data-level="2.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#z-scores"><i class="fa fa-check"></i><b>2.2</b> <span class="math inline">\(z\)</span>-scores</a></li>
<li class="chapter" data-level="2.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#t-tests"><i class="fa fa-check"></i><b>2.3</b> <span class="math inline">\(t\)</span>-tests</a><ul>
<li class="chapter" data-level="2.3.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#single-sample-t-test-setup"><i class="fa fa-check"></i><b>2.3.1</b> Single-sample <span class="math inline">\(t\)</span>-test: Setup</a></li>
<li class="chapter" data-level="2.3.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#hypothesis-testing-in-general"><i class="fa fa-check"></i><b>2.3.2</b> Hypothesis testing in general</a></li>
<li class="chapter" data-level="2.3.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#two-sample-t-test"><i class="fa fa-check"></i><b>2.3.3</b> Two-sample <span class="math inline">\(t\)</span>-test</a></li>
<li class="chapter" data-level="2.3.4" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#welch-example"><i class="fa fa-check"></i><b>2.3.4</b> Unequal variances: Welch <span class="math inline">\(t\)</span>-test</a></li>
<li class="chapter" data-level="2.3.5" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#t-test-assumptions"><i class="fa fa-check"></i><b>2.3.5</b> Assumptions behind <span class="math inline">\(t\)</span>-test</a></li>
<li class="chapter" data-level="2.3.6" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#paired-t-test"><i class="fa fa-check"></i><b>2.3.6</b> Paired <span class="math inline">\(t\)</span>-test</a></li>
<li class="chapter" data-level="2.3.7" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#reporting-a-hypothesis-test"><i class="fa fa-check"></i><b>2.3.7</b> Reporting a hypothesis test</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#checking-normality"><i class="fa fa-check"></i><b>2.4</b> Checking normality</a><ul>
<li class="chapter" data-level="2.4.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#visual-methods"><i class="fa fa-check"></i><b>2.4.1</b> Visual methods</a></li>
<li class="chapter" data-level="2.4.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#q-q-plots"><i class="fa fa-check"></i><b>2.4.2</b> Q-Q plots</a></li>
<li class="chapter" data-level="2.4.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#shapiro-wilk-example"><i class="fa fa-check"></i><b>2.4.3</b> Hypothesis test</a></li>
<li class="chapter" data-level="2.4.4" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#other-parametric-tests"><i class="fa fa-check"></i><b>2.4.4</b> Other parametric tests</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#non-parametric-tests"><i class="fa fa-check"></i><b>2.5</b> Non-parametric tests</a><ul>
<li class="chapter" data-level="2.5.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#wilcoxson-tests"><i class="fa fa-check"></i><b>2.5.1</b> Wilcoxson tests</a></li>
<li class="chapter" data-level="2.5.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#two-sample-wilcoxson-test"><i class="fa fa-check"></i><b>2.5.2</b> Two-sample Wilcoxson test</a></li>
<li class="chapter" data-level="2.5.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#parametric-versus-non-parametric-tests"><i class="fa fa-check"></i><b>2.5.3</b> Parametric versus non-parametric tests</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#other-reading-1"><i class="fa fa-check"></i><b>2.6</b> Other reading</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>3</b> Linear regression</a><ul>
<li class="chapter" data-level="3.1" data-path="linear-regression.html"><a href="linear-regression.html#regression-general-introduction"><i class="fa fa-check"></i><b>3.1</b> Regression: General introduction</a><ul>
<li class="chapter" data-level="3.1.1" data-path="linear-regression.html"><a href="linear-regression.html#linear-models"><i class="fa fa-check"></i><b>3.1.1</b> Linear models</a></li>
<li class="chapter" data-level="3.1.2" data-path="linear-regression.html"><a href="linear-regression.html#terminology"><i class="fa fa-check"></i><b>3.1.2</b> Terminology</a></li>
<li class="chapter" data-level="3.1.3" data-path="linear-regression.html"><a href="linear-regression.html#steps-and-assumptions-of-regression-analysis"><i class="fa fa-check"></i><b>3.1.3</b> Steps and assumptions of regression analysis</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="linear-regression.html"><a href="linear-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>3.2</b> Simple linear regression</a><ul>
<li class="chapter" data-level="3.2.1" data-path="linear-regression.html"><a href="linear-regression.html#slr-continuous-predictor"><i class="fa fa-check"></i><b>3.2.1</b> SLR: Continuous predictor</a></li>
<li class="chapter" data-level="3.2.2" data-path="linear-regression.html"><a href="linear-regression.html#slr-parameter-estimation"><i class="fa fa-check"></i><b>3.2.2</b> SLR: Parameter estimation</a></li>
<li class="chapter" data-level="3.2.3" data-path="linear-regression.html"><a href="linear-regression.html#hypothesis-testing-1"><i class="fa fa-check"></i><b>3.2.3</b> Hypothesis testing</a></li>
<li class="chapter" data-level="3.2.4" data-path="linear-regression.html"><a href="linear-regression.html#quality-of-fit"><i class="fa fa-check"></i><b>3.2.4</b> Quality of fit</a></li>
<li class="chapter" data-level="3.2.5" data-path="linear-regression.html"><a href="linear-regression.html#categorical-predictor"><i class="fa fa-check"></i><b>3.2.5</b> Categorical predictor</a></li>
<li class="chapter" data-level="3.2.6" data-path="linear-regression.html"><a href="linear-regression.html#slr-with-a-binary-categorical-predictor-vs.two-sample-t-test"><i class="fa fa-check"></i><b>3.2.6</b> SLR with a binary categorical predictor vs. two-sample <span class="math inline">\(t\)</span>-test</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="linear-regression.html"><a href="linear-regression.html#multiple-linear-regression"><i class="fa fa-check"></i><b>3.3</b> Multiple linear regression</a><ul>
<li class="chapter" data-level="3.3.1" data-path="linear-regression.html"><a href="linear-regression.html#goodness-of-fit-metrics"><i class="fa fa-check"></i><b>3.3.1</b> Goodness of fit metrics</a></li>
<li class="chapter" data-level="3.3.2" data-path="linear-regression.html"><a href="linear-regression.html#interactions-and-factors"><i class="fa fa-check"></i><b>3.3.2</b> Interactions and factors</a></li>
<li class="chapter" data-level="3.3.3" data-path="linear-regression.html"><a href="linear-regression.html#plotting-interactions"><i class="fa fa-check"></i><b>3.3.3</b> Plotting interactions</a></li>
<li class="chapter" data-level="3.3.4" data-path="linear-regression.html"><a href="linear-regression.html#categorical-factors-with-more-than-two-levels"><i class="fa fa-check"></i><b>3.3.4</b> Categorical factors with more than two levels</a></li>
<li class="chapter" data-level="3.3.5" data-path="linear-regression.html"><a href="linear-regression.html#releveling-factors"><i class="fa fa-check"></i><b>3.3.5</b> Releveling factors</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="linear-regression.html"><a href="linear-regression.html#linear-regression-assumptions"><i class="fa fa-check"></i><b>3.4</b> Linear regression assumptions</a><ul>
<li class="chapter" data-level="3.4.1" data-path="linear-regression.html"><a href="linear-regression.html#visual-methods-1"><i class="fa fa-check"></i><b>3.4.1</b> Visual methods</a></li>
<li class="chapter" data-level="3.4.2" data-path="linear-regression.html"><a href="linear-regression.html#assumption-1-linearity"><i class="fa fa-check"></i><b>3.4.2</b> Assumption 1: Linearity</a></li>
<li class="chapter" data-level="3.4.3" data-path="linear-regression.html"><a href="linear-regression.html#c2ioe"><i class="fa fa-check"></i><b>3.4.3</b> Assumption 2: Independence of errors</a></li>
<li class="chapter" data-level="3.4.4" data-path="linear-regression.html"><a href="linear-regression.html#assumption-3-normality-of-errors"><i class="fa fa-check"></i><b>3.4.4</b> Assumption 3: Normality of errors</a></li>
<li class="chapter" data-level="3.4.5" data-path="linear-regression.html"><a href="linear-regression.html#assumtion-4-constancy-of-variance"><i class="fa fa-check"></i><b>3.4.5</b> Assumtion 4: Constancy of variance</a></li>
<li class="chapter" data-level="3.4.6" data-path="linear-regression.html"><a href="linear-regression.html#interim-summary"><i class="fa fa-check"></i><b>3.4.6</b> Interim summary</a></li>
<li class="chapter" data-level="3.4.7" data-path="linear-regression.html"><a href="linear-regression.html#transforming-to-normality"><i class="fa fa-check"></i><b>3.4.7</b> Transforming to normality</a></li>
<li class="chapter" data-level="3.4.8" data-path="linear-regression.html"><a href="linear-regression.html#assumption-5-linear-independence-of-predictors"><i class="fa fa-check"></i><b>3.4.8</b> Assumption 5: Linear independence of predictors</a></li>
<li class="chapter" data-level="3.4.9" data-path="linear-regression.html"><a href="linear-regression.html#collinearity"><i class="fa fa-check"></i><b>3.4.9</b> Collinearity</a></li>
<li class="chapter" data-level="3.4.10" data-path="linear-regression.html"><a href="linear-regression.html#assumption-6-observations"><i class="fa fa-check"></i><b>3.4.10</b> Assumption 6: Observations</a></li>
<li class="chapter" data-level="3.4.11" data-path="linear-regression.html"><a href="linear-regression.html#lin-reg-measuring-influence"><i class="fa fa-check"></i><b>3.4.11</b> Measuring influence</a></li>
<li class="chapter" data-level="3.4.12" data-path="linear-regression.html"><a href="linear-regression.html#outliers"><i class="fa fa-check"></i><b>3.4.12</b> Outliers</a></li>
<li class="chapter" data-level="3.4.13" data-path="linear-regression.html"><a href="linear-regression.html#regression-assumptions-reassurance"><i class="fa fa-check"></i><b>3.4.13</b> Regression assumptions: Reassurance</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="linear-regression.html"><a href="linear-regression.html#lm-model-comparison"><i class="fa fa-check"></i><b>3.5</b> Model comparison</a><ul>
<li class="chapter" data-level="3.5.1" data-path="linear-regression.html"><a href="linear-regression.html#nested-model-comparison"><i class="fa fa-check"></i><b>3.5.1</b> Nested model comparison</a></li>
<li class="chapter" data-level="3.5.2" data-path="linear-regression.html"><a href="linear-regression.html#non-nested-model-comparison"><i class="fa fa-check"></i><b>3.5.2</b> Non-nested model comparison</a></li>
<li class="chapter" data-level="3.5.3" data-path="linear-regression.html"><a href="linear-regression.html#c2varselect"><i class="fa fa-check"></i><b>3.5.3</b> Variable selection</a></li>
<li class="chapter" data-level="3.5.4" data-path="linear-regression.html"><a href="linear-regression.html#interpretability-issues"><i class="fa fa-check"></i><b>3.5.4</b> Interpretability issues</a></li>
<li class="chapter" data-level="3.5.5" data-path="linear-regression.html"><a href="linear-regression.html#interim-recipe-building-a-multiple-linear-regression-model"><i class="fa fa-check"></i><b>3.5.5</b> Interim recipe: Building a multiple linear regression model</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="linear-regression.html"><a href="linear-regression.html#c2solns"><i class="fa fa-check"></i><b>3.6</b> Solutions</a><ul>
<li class="chapter" data-level="3.6.1" data-path="linear-regression.html"><a href="linear-regression.html#multiple-linear-regression-solutions"><i class="fa fa-check"></i><b>3.6.1</b> Multiple linear regression: Solutions</a></li>
<li class="chapter" data-level="3.6.2" data-path="linear-regression.html"><a href="linear-regression.html#linear-regression-assumptions-solutions"><i class="fa fa-check"></i><b>3.6.2</b> Linear regression assumptions: Solutions</a></li>
<li class="chapter" data-level="3.6.3" data-path="linear-regression.html"><a href="linear-regression.html#model-comparison-solutions"><i class="fa fa-check"></i><b>3.6.3</b> Model comparison: Solutions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="cda.html"><a href="cda.html"><i class="fa fa-check"></i><b>4</b> Categorical data analysis: Preliminaries</a><ul>
<li class="chapter" data-level="4.1" data-path="cda.html"><a href="cda.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a><ul>
<li class="chapter" data-level="4.1.1" data-path="cda.html"><a href="cda.html#x2-contingency-tables"><i class="fa fa-check"></i><b>4.1.1</b> 2x2 contingency tables</a></li>
<li class="chapter" data-level="4.1.2" data-path="cda.html"><a href="cda.html#the-chi-squared-test"><i class="fa fa-check"></i><b>4.1.2</b> The chi-squared test</a></li>
<li class="chapter" data-level="4.1.3" data-path="cda.html"><a href="cda.html#fishers-exact-test"><i class="fa fa-check"></i><b>4.1.3</b> Fisher’s exact test</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="cda.html"><a href="cda.html#towards-logistic-regression"><i class="fa fa-check"></i><b>4.2</b> Towards logistic regression</a><ul>
<li class="chapter" data-level="4.2.1" data-path="cda.html"><a href="cda.html#odds"><i class="fa fa-check"></i><b>4.2.1</b> Odds</a></li>
<li class="chapter" data-level="4.2.2" data-path="cda.html"><a href="cda.html#log-odds"><i class="fa fa-check"></i><b>4.2.2</b> Log-odds</a></li>
<li class="chapter" data-level="4.2.3" data-path="cda.html"><a href="cda.html#odds-ratios"><i class="fa fa-check"></i><b>4.2.3</b> Odds ratios</a></li>
<li class="chapter" data-level="4.2.4" data-path="cda.html"><a href="cda.html#log-odds-sample-and-population"><i class="fa fa-check"></i><b>4.2.4</b> Log odds: sample and population</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="cda.html"><a href="cda.html#cda-other-readings"><i class="fa fa-check"></i><b>4.3</b> Other readings</a></li>
<li class="chapter" data-level="4.4" data-path="cda.html"><a href="cda.html#c3solns"><i class="fa fa-check"></i><b>4.4</b> Solutions</a><ul>
<li class="chapter" data-level="4.4.1" data-path="cda.html"><a href="cda.html#solutions-to-exercise-1"><i class="fa fa-check"></i><b>4.4.1</b> Solutions to Exercise 1:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>5</b> Logistic regression</a><ul>
<li class="chapter" data-level="5.1" data-path="logistic-regression.html"><a href="logistic-regression.html#simple-logistic-regression"><i class="fa fa-check"></i><b>5.1</b> Simple logistic regression</a><ul>
<li class="chapter" data-level="5.1.1" data-path="logistic-regression.html"><a href="logistic-regression.html#log-reg-hyp-test"><i class="fa fa-check"></i><b>5.1.1</b> Hypothesis testing</a></li>
<li class="chapter" data-level="5.1.2" data-path="logistic-regression.html"><a href="logistic-regression.html#interpreting-the-coefficients-logit-odds-and-probability"><i class="fa fa-check"></i><b>5.1.2</b> Interpreting the coefficients: Logit, odds, and probability</a></li>
<li class="chapter" data-level="5.1.3" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-regression-as-a-glm"><i class="fa fa-check"></i><b>5.1.3</b> Logistic regression as a GLM</a></li>
<li class="chapter" data-level="5.1.4" data-path="logistic-regression.html"><a href="logistic-regression.html#c4differences"><i class="fa fa-check"></i><b>5.1.4</b> Differences from linear regression: Fitting and interpretation</a></li>
<li class="chapter" data-level="5.1.5" data-path="logistic-regression.html"><a href="logistic-regression.html#fitting-a-logistic-regression-model"><i class="fa fa-check"></i><b>5.1.5</b> Fitting a logistic regression model</a></li>
<li class="chapter" data-level="5.1.6" data-path="logistic-regression.html"><a href="logistic-regression.html#interpretation"><i class="fa fa-check"></i><b>5.1.6</b> Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="logistic-regression.html"><a href="logistic-regression.html#evaluating-logistic-regression-models"><i class="fa fa-check"></i><b>5.2</b> Evaluating logistic regression models</a><ul>
<li class="chapter" data-level="5.2.1" data-path="logistic-regression.html"><a href="logistic-regression.html#c4lrt"><i class="fa fa-check"></i><b>5.2.1</b> Likelihood ratio test</a></li>
<li class="chapter" data-level="5.2.2" data-path="logistic-regression.html"><a href="logistic-regression.html#classification-accuracy"><i class="fa fa-check"></i><b>5.2.2</b> Classification accuracy</a></li>
<li class="chapter" data-level="5.2.3" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-regression-pseudo-r2"><i class="fa fa-check"></i><b>5.2.3</b> Pseudo-<span class="math inline">\(R^2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="logistic-regression.html"><a href="logistic-regression.html#multiple-logistic-regression"><i class="fa fa-check"></i><b>5.3</b> Multiple logistic regression</a><ul>
<li class="chapter" data-level="5.3.1" data-path="logistic-regression.html"><a href="logistic-regression.html#likelihood-ratio-test-general-case"><i class="fa fa-check"></i><b>5.3.1</b> Likelihood ratio test: General case</a></li>
<li class="chapter" data-level="5.3.2" data-path="logistic-regression.html"><a href="logistic-regression.html#log-reg-worked-example"><i class="fa fa-check"></i><b>5.3.2</b> Worked example</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="logistic-regression.html"><a href="logistic-regression.html#model-criticism-logistic-regression"><i class="fa fa-check"></i><b>5.4</b> Model criticism for logistic regression</a><ul>
<li class="chapter" data-level="5.4.1" data-path="logistic-regression.html"><a href="logistic-regression.html#residual-plots"><i class="fa fa-check"></i><b>5.4.1</b> Residual plots</a></li>
<li class="chapter" data-level="5.4.2" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-regression-cooks-distance"><i class="fa fa-check"></i><b>5.4.2</b> Cook’s distance</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="logistic-regression.html"><a href="logistic-regression.html#other-readings"><i class="fa fa-check"></i><b>5.5</b> Other readings</a></li>
<li class="chapter" data-level="5.6" data-path="logistic-regression.html"><a href="logistic-regression.html#c4solns"><i class="fa fa-check"></i><b>5.6</b> Solutions</a></li>
<li class="chapter" data-level="5.7" data-path="logistic-regression.html"><a href="logistic-regression.html#c4appendix2"><i class="fa fa-check"></i><b>5.7</b> Appendix: Other Generalized Linear Models</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><a href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><i class="fa fa-check"></i><b>6</b> Practical Regression Topics 1: Multi-level factors, contrast coding, interactions</a><ul>
<li class="chapter" data-level="6.1" data-path="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><a href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html#multi-level-factors-introduction"><i class="fa fa-check"></i><b>6.1</b> Multi-level factors: Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><a href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html#contrast-coding"><i class="fa fa-check"></i><b>6.2</b> Contrast coding</a><ul>
<li class="chapter" data-level="6.2.1" data-path="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><a href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html#first-examples"><i class="fa fa-check"></i><b>6.2.1</b> First examples</a></li>
<li class="chapter" data-level="6.2.2" data-path="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><a href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html#basic-interpretation-of-contrasts"><i class="fa fa-check"></i><b>6.2.2</b> Basic interpretation of contrasts</a></li>
<li class="chapter" data-level="6.2.3" data-path="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><a href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html#contrast-coding-schemes"><i class="fa fa-check"></i><b>6.2.3</b> Contrast coding schemes</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><a href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html#c5mlf"><i class="fa fa-check"></i><b>6.3</b> Assessing a multi-level factor’s contribution</a></li>
<li class="chapter" data-level="6.4" data-path="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><a href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html#practice-with-interactions"><i class="fa fa-check"></i><b>6.4</b> Practice with interactions</a></li>
<li class="chapter" data-level="6.5" data-path="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><a href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html#c5solns"><i class="fa fa-check"></i><b>6.5</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="lmem.html"><a href="lmem.html"><i class="fa fa-check"></i><b>7</b> Linear mixed models</a><ul>
<li class="chapter" data-level="7.1" data-path="lmem.html"><a href="lmem.html#mixed-effects-models-motivation"><i class="fa fa-check"></i><b>7.1</b> Mixed-effects models: Motivation</a><ul>
<li class="chapter" data-level="7.1.1" data-path="lmem.html"><a href="lmem.html#simpsons-paradox"><i class="fa fa-check"></i><b>7.1.1</b> Simpson’s paradox</a></li>
<li class="chapter" data-level="7.1.2" data-path="lmem.html"><a href="lmem.html#repeated-measure-anovas"><i class="fa fa-check"></i><b>7.1.2</b> Repeated-measure ANOVAs</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="lmem.html"><a href="lmem.html#linear-mixed-models-1-one-grouping-factor-random-intercepts"><i class="fa fa-check"></i><b>7.2</b> Linear mixed models 1: One grouping factor, random intercepts</a><ul>
<li class="chapter" data-level="7.2.1" data-path="lmem.html"><a href="lmem.html#c6model1A"><i class="fa fa-check"></i><b>7.2.1</b> Model 1A: Simple linear regression</a></li>
<li class="chapter" data-level="7.2.2" data-path="lmem.html"><a href="lmem.html#c6model1b"><i class="fa fa-check"></i><b>7.2.2</b> Model 1B: Random intercept only</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="lmem.html"><a href="lmem.html#c6lmm2"><i class="fa fa-check"></i><b>7.3</b> Linear mixed models 2: One grouping factor, random intercepts and slopes</a><ul>
<li class="chapter" data-level="7.3.1" data-path="lmem.html"><a href="lmem.html#c6model1c"><i class="fa fa-check"></i><b>7.3.1</b> Model 1C</a></li>
<li class="chapter" data-level="7.3.2" data-path="lmem.html"><a href="lmem.html#fitting-model-1c"><i class="fa fa-check"></i><b>7.3.2</b> Fitting Model 1C</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="lmem.html"><a href="lmem.html#linear-mixed-models-3-two-grouping-factors"><i class="fa fa-check"></i><b>7.4</b> Linear mixed models 3: Two grouping factors</a><ul>
<li class="chapter" data-level="7.4.1" data-path="lmem.html"><a href="lmem.html#c6model2A"><i class="fa fa-check"></i><b>7.4.1</b> Model 2A: By-participant and by-item random intercepts</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="lmem.html"><a href="lmem.html#evaluating-lmms"><i class="fa fa-check"></i><b>7.5</b> Evaluating LMMs</a><ul>
<li class="chapter" data-level="7.5.1" data-path="lmem.html"><a href="lmem.html#hypothesis-testing-2"><i class="fa fa-check"></i><b>7.5.1</b> Hypothesis testing</a></li>
<li class="chapter" data-level="7.5.2" data-path="lmem.html"><a href="lmem.html#significance-of-a-random-effect-term"><i class="fa fa-check"></i><b>7.5.2</b> Significance of a random effect term</a></li>
<li class="chapter" data-level="7.5.3" data-path="lmem.html"><a href="lmem.html#c6fixedp"><i class="fa fa-check"></i><b>7.5.3</b> Significance of fixed effects</a></li>
<li class="chapter" data-level="7.5.4" data-path="lmem.html"><a href="lmem.html#evaluating-goodness-of-fit"><i class="fa fa-check"></i><b>7.5.4</b> Evaluating goodness of fit</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="lmem.html"><a href="lmem.html#linear-mixed-models-4-multiple-predictors"><i class="fa fa-check"></i><b>7.6</b> Linear mixed models 4: Multiple predictors</a><ul>
<li class="chapter" data-level="7.6.1" data-path="lmem.html"><a href="lmem.html#types-of-predictors"><i class="fa fa-check"></i><b>7.6.1</b> Types of predictors</a></li>
<li class="chapter" data-level="7.6.2" data-path="lmem.html"><a href="lmem.html#c6model3A"><i class="fa fa-check"></i><b>7.6.2</b> Model 3A: Random intercepts only</a></li>
<li class="chapter" data-level="7.6.3" data-path="lmem.html"><a href="lmem.html#c6model3B"><i class="fa fa-check"></i><b>7.6.3</b> Model 3B: Random intercepts and all possible random slopes</a></li>
<li class="chapter" data-level="7.6.4" data-path="lmem.html"><a href="lmem.html#assessing-variability"><i class="fa fa-check"></i><b>7.6.4</b> Assessing variability</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="lmem.html"><a href="lmem.html#more-on-random-slopes"><i class="fa fa-check"></i><b>7.7</b> More on random slopes</a><ul>
<li class="chapter" data-level="7.7.1" data-path="lmem.html"><a href="lmem.html#what-does-adding-a-random-slope-term-do"><i class="fa fa-check"></i><b>7.7.1</b> What does adding a random slope term do?</a></li>
<li class="chapter" data-level="7.7.2" data-path="lmem.html"><a href="lmem.html#adding-a-random-slope"><i class="fa fa-check"></i><b>7.7.2</b> Discussion: Adding a random slope</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="lmem.html"><a href="lmem.html#random-effect-correlations"><i class="fa fa-check"></i><b>7.8</b> Random effect correlations</a><ul>
<li class="chapter" data-level="7.8.1" data-path="lmem.html"><a href="lmem.html#model-1e-correlated-random-slope-intercept"><i class="fa fa-check"></i><b>7.8.1</b> Model 1E: <strong>Correlated</strong> random slope &amp; intercept</a></li>
<li class="chapter" data-level="7.8.2" data-path="lmem.html"><a href="lmem.html#c6discuss"><i class="fa fa-check"></i><b>7.8.2</b> Dicussion: Adding a correlation</a></li>
</ul></li>
<li class="chapter" data-level="7.9" data-path="lmem.html"><a href="lmem.html#model-criticism-for-linear-mixed-models"><i class="fa fa-check"></i><b>7.9</b> Model criticism for linear mixed models</a><ul>
<li class="chapter" data-level="7.9.1" data-path="lmem.html"><a href="lmem.html#model-3b-residual-plots"><i class="fa fa-check"></i><b>7.9.1</b> Model 3B: Residual plots</a></li>
<li class="chapter" data-level="7.9.2" data-path="lmem.html"><a href="lmem.html#model-3b-random-effect-distribution"><i class="fa fa-check"></i><b>7.9.2</b> Model 3B: Random effect distribution</a></li>
</ul></li>
<li class="chapter" data-level="7.10" data-path="lmem.html"><a href="lmem.html#c6factorsissue"><i class="fa fa-check"></i><b>7.10</b> Random slopes for factors</a><ul>
<li class="chapter" data-level="7.10.1" data-path="lmem.html"><a href="lmem.html#model-with-random-effect-correlations"><i class="fa fa-check"></i><b>7.10.1</b> Model with random-effect correlations</a></li>
<li class="chapter" data-level="7.10.2" data-path="lmem.html"><a href="lmem.html#lmem-mwrec"><i class="fa fa-check"></i><b>7.10.2</b> Models without random-effect correlations</a></li>
</ul></li>
<li class="chapter" data-level="7.11" data-path="lmem.html"><a href="lmem.html#other-readings-1"><i class="fa fa-check"></i><b>7.11</b> Other readings</a></li>
<li class="chapter" data-level="7.12" data-path="lmem.html"><a href="lmem.html#c6extraexamples"><i class="fa fa-check"></i><b>7.12</b> Appendix: Extra examples</a><ul>
<li class="chapter" data-level="7.12.1" data-path="lmem.html"><a href="lmem.html#lmm-simulation-confint"><i class="fa fa-check"></i><b>7.12.1</b> Predicting confidence intervals by simulation</a></li>
<li class="chapter" data-level="7.12.2" data-path="lmem.html"><a href="lmem.html#random-intercept-and-slope-model-for-givenness-data"><i class="fa fa-check"></i><b>7.12.2</b> Random intercept and slope model for <code>givenness</code> data</a></li>
</ul></li>
<li class="chapter" data-level="7.13" data-path="lmem.html"><a href="lmem.html#c6extendedexercise"><i class="fa fa-check"></i><b>7.13</b> Appendix: Extended exercise</a></li>
<li class="chapter" data-level="7.14" data-path="lmem.html"><a href="lmem.html#c6solns"><i class="fa fa-check"></i><b>7.14</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html"><i class="fa fa-check"></i><b>8</b> Mixed-effects logistic regression</a><ul>
<li class="chapter" data-level="8.1" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#preliminaries"><i class="fa fa-check"></i><b>8.1</b> Preliminaries</a><ul>
<li class="chapter" data-level="8.1.1" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#motivation"><i class="fa fa-check"></i><b>8.1.1</b> Motivation</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#basics"><i class="fa fa-check"></i><b>8.2</b> Basics</a><ul>
<li class="chapter" data-level="8.2.1" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#c7m1"><i class="fa fa-check"></i><b>8.2.1</b> Model 1: <code>givenness</code> data, crossed random effects (intercepts + slopes)</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#hypothesis-testing-3"><i class="fa fa-check"></i><b>8.3</b> Hypothesis testing</a><ul>
<li class="chapter" data-level="8.3.1" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#fixed-effects"><i class="fa fa-check"></i><b>8.3.1</b> Fixed effects</a></li>
<li class="chapter" data-level="8.3.2" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#random-effects"><i class="fa fa-check"></i><b>8.3.2</b> Random effects</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#fixed-and-random-effects"><i class="fa fa-check"></i><b>8.4</b> Fixed and random effects</a></li>
<li class="chapter" data-level="8.5" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#melr-practice"><i class="fa fa-check"></i><b>8.5</b> MELR Practice</a><ul>
<li class="chapter" data-level="8.5.1" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#c7ex1"><i class="fa fa-check"></i><b>8.5.1</b> Exercise 1: tapping</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#model-criticism-for-mixed-effects-logistic-regression"><i class="fa fa-check"></i><b>8.6</b> Model criticism for mixed-effects logistic regression</a><ul>
<li class="chapter" data-level="8.6.1" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#random-effect-distributions"><i class="fa fa-check"></i><b>8.6.1</b> Random-effect distributions</a></li>
<li class="chapter" data-level="8.6.2" data-path="logistic-regression.html"><a href="logistic-regression.html#residual-plots"><i class="fa fa-check"></i><b>8.6.2</b> Residual plots</a></li>
<li class="chapter" data-level="8.6.3" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#influence"><i class="fa fa-check"></i><b>8.6.3</b> Influence</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#evaluation-measures"><i class="fa fa-check"></i><b>8.7</b> Evaluation measures</a><ul>
<li class="chapter" data-level="8.7.1" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#evaluation-measure-1-likelihood-ratio-test"><i class="fa fa-check"></i><b>8.7.1</b> Evaluation measure 1: Likelihood ratio test</a></li>
<li class="chapter" data-level="8.7.2" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#evaluation-measure-2-classification-accuracy"><i class="fa fa-check"></i><b>8.7.2</b> Evaluation measure 2: Classification accuracy</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#miscellaneous-mixed-effects-regression-topics"><i class="fa fa-check"></i><b>8.8</b> Miscellaneous mixed-effects regression topics</a><ul>
<li class="chapter" data-level="8.8.1" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#c7m2"><i class="fa fa-check"></i><b>8.8.1</b> Random-effect correlation issues</a></li>
</ul></li>
<li class="chapter" data-level="8.9" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#other-readings-2"><i class="fa fa-check"></i><b>8.9</b> Other readings</a></li>
<li class="chapter" data-level="8.10" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#appendices"><i class="fa fa-check"></i><b>8.10</b> Appendices</a><ul>
<li class="chapter" data-level="8.10.1" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#melr-random-slopes-for-factors"><i class="fa fa-check"></i><b>8.10.1</b> Appendix: Random slopes for factors</a></li>
<li class="chapter" data-level="8.10.2" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#c7appendix2"><i class="fa fa-check"></i><b>8.10.2</b> Appendix: Multi-level factors and uncorrelated random effects</a></li>
<li class="chapter" data-level="8.10.3" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#appendix-what-can-happen-if-a-random-slope-isnt-included"><i class="fa fa-check"></i><b>8.10.3</b> Appendix: What can happen if a random slope isn’t included?</a></li>
</ul></li>
<li class="chapter" data-level="8.11" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#c7solns"><i class="fa fa-check"></i><b>8.11</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><i class="fa fa-check"></i><b>9</b> Practical regression topics 2: Ordered factors, nonlinear effects, model predictions, post-hoc tests</a><ul>
<li class="chapter" data-level="9.1" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#introduction-1"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#ordered-factors"><i class="fa fa-check"></i><b>9.2</b> Ordered factors</a><ul>
<li class="chapter" data-level="9.2.1" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#orthogonal-polynomial-contrasts"><i class="fa fa-check"></i><b>9.2.1</b> Orthogonal polynomial contrasts</a></li>
<li class="chapter" data-level="9.2.2" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#using-an-ordered-factor-as-a-predictor"><i class="fa fa-check"></i><b>9.2.2</b> Using an ordered factor as a predictor</a></li>
<li class="chapter" data-level="9.2.3" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#further-points"><i class="fa fa-check"></i><b>9.2.3</b> Further points</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#nonlinear-effects"><i class="fa fa-check"></i><b>9.3</b> Nonlinear effects</a><ul>
<li class="chapter" data-level="9.3.1" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#splines-definition-and-benefits"><i class="fa fa-check"></i><b>9.3.1</b> Splines: Definition and benefits</a></li>
<li class="chapter" data-level="9.3.2" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#restricted-cubic-splines"><i class="fa fa-check"></i><b>9.3.2</b> Restricted cubic splines</a></li>
<li class="chapter" data-level="9.3.3" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#choosing-spline-complexity"><i class="fa fa-check"></i><b>9.3.3</b> Choosing spline complexity</a></li>
<li class="chapter" data-level="9.3.4" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#rcs-components"><i class="fa fa-check"></i><b>9.3.4</b> RCS components</a></li>
<li class="chapter" data-level="9.3.5" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#using-rcs-in-a-mixed-model"><i class="fa fa-check"></i><b>9.3.5</b> Using RCS in a mixed model</a></li>
<li class="chapter" data-level="9.3.6" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#random-slopes-for-rcs-terms"><i class="fa fa-check"></i><b>9.3.6</b> Random slopes for RCS terms</a></li>
<li class="chapter" data-level="9.3.7" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#nonlinear-effects-summary"><i class="fa fa-check"></i><b>9.3.7</b> Nonlinear effects: Summary</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#predictions-from-mixed-models"><i class="fa fa-check"></i><b>9.4</b> Predictions from mixed models</a><ul>
<li class="chapter" data-level="9.4.1" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#making-model-predictions"><i class="fa fa-check"></i><b>9.4.1</b> Making Model Predictions</a></li>
<li class="chapter" data-level="9.4.2" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#simulation-based-predictions"><i class="fa fa-check"></i><b>9.4.2</b> Simulation-based predictions</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#post-hoc-mult-comp"><i class="fa fa-check"></i><b>9.5</b> Post-hoc tests and multiple comparisons</a></li>
<li class="chapter" data-level="9.6" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#c8indivpreds"><i class="fa fa-check"></i><b>9.6</b> Appendix: Model predictions for indiviudal participants</a><ul>
<li class="chapter" data-level="9.6.1" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#predictions-incorporating-offsets-for-individual-speakers"><i class="fa fa-check"></i><b>9.6.1</b> Predictions incorporating offsets for individual speakers</a></li>
<li class="chapter" data-level="9.6.2" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#predicted-williams-effect-for-each-speaker"><i class="fa fa-check"></i><b>9.6.2</b> Predicted Williams effect for each speaker</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#c8slopesForFactors"><i class="fa fa-check"></i><b>9.7</b> Appendix: Random slopes for factors</a></li>
<li class="chapter" data-level="9.8" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#c8solns"><i class="fa fa-check"></i><b>9.8</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="datasets-appendix.html"><a href="datasets-appendix.html"><i class="fa fa-check"></i><b>10</b> Appendix: Datasets and packages</a><ul>
<li class="chapter" data-level="10.1" data-path="datasets-appendix.html"><a href="datasets-appendix.html#engdata"><i class="fa fa-check"></i><b>10.1</b> <code>english</code> lexical decision and naming latencies</a></li>
<li class="chapter" data-level="10.2" data-path="datasets-appendix.html"><a href="datasets-appendix.html#dutch-regularity"><i class="fa fa-check"></i><b>10.2</b> Dutch <code id="dregdata">regularity</code></a></li>
<li class="chapter" data-level="10.3" data-path="datasets-appendix.html"><a href="datasets-appendix.html#european-french-phrase-medial-vowel-devoicing"><i class="fa fa-check"></i><b>10.3</b> European French phrase-medial vowel <code id="devdata">devoicing</code></a><ul>
<li class="chapter" data-level="10.3.1" data-path="datasets-appendix.html"><a href="datasets-appendix.html#background"><i class="fa fa-check"></i><b>10.3.1</b> Background</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="datasets-appendix.html"><a href="datasets-appendix.html#north-american-english-tapping"><i class="fa fa-check"></i><b>10.4</b> North American English <code id="tapdata">tapping</code></a><ul>
<li class="chapter" data-level="10.4.1" data-path="datasets-appendix.html"><a href="datasets-appendix.html#background-1"><i class="fa fa-check"></i><b>10.4.1</b> Background</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="datasets-appendix.html"><a href="datasets-appendix.html#halfdata"><i class="fa fa-check"></i><b>10.5</b> <code>halfrhyme</code>: English half-rhymes</a></li>
<li class="chapter" data-level="10.6" data-path="datasets-appendix.html"><a href="datasets-appendix.html#givedata"><i class="fa fa-check"></i><b>10.6</b> <code>givenness</code> data: the Williams Effect</a><ul>
<li class="chapter" data-level="10.6.1" data-path="datasets-appendix.html"><a href="datasets-appendix.html#background-2"><i class="fa fa-check"></i><b>10.6.1</b> Background</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="datasets-appendix.html"><a href="datasets-appendix.html#alternatives"><i class="fa fa-check"></i><b>10.7</b> <code id="altdata">alternatives</code></a><ul>
<li class="chapter" data-level="10.7.1" data-path="datasets-appendix.html"><a href="datasets-appendix.html#background-3"><i class="fa fa-check"></i><b>10.7.1</b> Background</a></li>
</ul></li>
<li class="chapter" data-level="10.8" data-path="datasets-appendix.html"><a href="datasets-appendix.html#votdata"><i class="fa fa-check"></i><b>10.8</b> VOT</a><ul>
<li class="chapter" data-level="10.8.1" data-path="datasets-appendix.html"><a href="datasets-appendix.html#background-4"><i class="fa fa-check"></i><b>10.8.1</b> Background</a></li>
</ul></li>
<li class="chapter" data-level="10.9" data-path="datasets-appendix.html"><a href="datasets-appendix.html#transitionsdata"><i class="fa fa-check"></i><b>10.9</b> Transitions</a></li>
<li class="chapter" data-level="10.10" data-path="datasets-appendix.html"><a href="datasets-appendix.html#packages"><i class="fa fa-check"></i><b>10.10</b> Packages</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Quantitative Methods for Linguistic Data</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="logistic-regression" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Logistic regression</h1>
<p><strong>Preliminary code</strong></p>
<p>This code is needed to make other code below work:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(gridExtra) <span class="co"># for grid.arrange() to print plots side-by-side</span>
<span class="kw">library</span>(languageR)
<span class="kw">library</span>(ggplot2)
<span class="kw">library</span>(dplyr)
<span class="kw">library</span>(arm)
<span class="kw">library</span>(boot)

## loads givennessMcGillLing620.csv from OSF project for Wagner (2012) data
givenness &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="kw">url</span>(<span class="st">&quot;https://osf.io/q9e3a/download&quot;</span>))

## make standardized (numeric/centered/scaled) versions of &#39;givenness&#39; dataset predictors:
givenness &lt;-<span class="st"> </span><span class="kw">mutate</span>(givenness,
                    <span class="dt">conditionLabel.williams =</span> arm::<span class="kw">rescale</span>(conditionLabel),
                    <span class="dt">clabel.williams =</span> arm::<span class="kw">rescale</span>(conditionLabel),
                    <span class="dt">npType.pronoun =</span> arm::<span class="kw">rescale</span>(npType),
                    <span class="dt">npType.pron =</span> arm::<span class="kw">rescale</span>(npType),
                    <span class="dt">voice.passive =</span> arm::<span class="kw">rescale</span>(voice),
                    <span class="dt">order.std =</span> arm::<span class="kw">rescale</span>(order),
                    <span class="dt">stressshift.num =</span> (<span class="kw">as.numeric</span>(stressshift) -<span class="st"> </span><span class="dv">1</span>),
                    <span class="dt">acoustics.std =</span> arm::<span class="kw">rescale</span>(acoustics)
)</code></pre></div>
<script src="js/hideOutput.js"></script>
<p><strong>Note</strong>: Answers to some questions/exercises not listed in text are in <a href="logistic-regression.html#c4solns">Solutions</a></p>
<!-- TODO FUTURE: actually fill in full solutions -->
<div id="simple-logistic-regression" class="section level2">
<h2><span class="header-section-number">5.1</span> Simple logistic regression</h2>
<p>Analogously to “simple linear regression”, which is predicting a continuous <span class="math inline">\(Y\)</span> from a single predictor <span class="math inline">\(X\)</span>, in <em>simple logistic regression</em> we predict a binary <span class="math inline">\(Y\)</span>.</p>
<p>Notation:</p>
<ul>
<li><p>Our sample consists of <span class="math inline">\(n\)</span> observations: <span class="math inline">\((x_1, y_1), ..., (x_n, y_n)\)</span></p></li>
<li><p>The <strong>response</strong> <span class="math inline">\(Y\)</span> is binary: each <span class="math inline">\(y_i\)</span> = 1 or 0.</p></li>
<li><p>There is a single <strong>predictor</strong> <span class="math inline">\(X\)</span>, which may be continuous or discrete.</p></li>
</ul>
<p>We model the <strong>log-odds</strong> of <span class="math inline">\(Y = 1\)</span> vs. <span class="math inline">\(Y = 0\)</span> for the <span class="math inline">\(i^{\text{th}}\)</span> observation:</p>
<p><span class="math display">\[  
\log \left[ \frac{P(y_i = 1)}{P(y_i = 0)} \right] \equiv \text{logit}(P(y_i = 1))
\]</span> as a function of the single predictor: <span class="math display" id="eq:logit">\[
  \text{logit}(P(Y = 1)) = \beta_0 + \beta_1 X
  \tag{5.1}
\]</span> This is the model in “logit space”, where the right-hand side of the equation (called the <em>linear predictor</em>) looks the same as for simple linear regression except for the lack of an error term, while the left-hand side of the equation looks similar to SLR modulo the logit function (called the <em>link function</em>).</p>
The same model can be written in “probability space”:
<span class="math display">\[\begin{align}
  P(Y = 1) &amp; = \text{logit}^{-1}(\beta_0 + \beta_1 X) \\
  &amp; = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X)}}
\end{align}\]</span>
The model in Eq. <a href="logistic-regression.html#eq:logit">(5.1)</a> predicts the log-odds of <span class="math inline">\(Y=1\)</span> for a single observation as:
<span class="math display">\[\begin{equation*}
  \text{logit}(\hat{p}_i) = \hat{\beta}_0 + \hat{\beta}_1 x_i
\end{equation*}\]</span>
<div id="log-reg-hyp-test" class="section level3">
<h3><span class="header-section-number">5.1.1</span> Hypothesis testing</h3>
The test statistic
<span class="math display" id="eq:wald">\[\begin{equation}
    z = \frac{\hat{\beta}_1}{SE(\hat{\beta}_1)} 
    \tag{5.2}
\end{equation}\]</span>
<p>turns out to be approximately normally distributed (e.g. <span class="citation">Agresti (<a href="#ref-agresti2003categorical">2003</a>)</span> 5.5.2).<a href="#fn21" class="footnoteRef" id="fnref21"><sup>21</sup></a> This means we can perform a <em>Wald test</em> of the null hypothesis to show that there is no relationship between <span class="math inline">\(P(Y=1)\)</span> and <span class="math inline">\(X\)</span>: <span class="math display">\[
H_0~:~\hat{\beta}_1 = 0
\]</span> and get a <span class="math inline">\(p\)</span>-value and confidence interval for the slope (<span class="math inline">\(\beta_1\)</span>). The same is true for the intercept (<span class="math inline">\(\beta_0\)</span>).</p>
</div>
<div id="interpreting-the-coefficients-logit-odds-and-probability" class="section level3">
<h3><span class="header-section-number">5.1.2</span> Interpreting the coefficients: Logit, odds, and probability</h3>
<p>The regression coefficients, <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, can be interpreted in terms of log-odds, odds, or probability:</p>
<p>Intercept:</p>
<ul>
<li><p><span class="math inline">\(\beta_0\)</span>: <strong>predicted log-odds</strong> of <span class="math inline">\(Y = 1\)</span> when <span class="math inline">\(X = 0\)</span></p></li>
<li><p><span class="math inline">\(e^{\beta_0}\)</span>: predicted odds of <span class="math inline">\(Y = 1\)</span> when <span class="math inline">\(X = 0\)</span></p></li>
<li><p><span class="math inline">\(\text{logit}^{-1}(\beta_0)\)</span>: probability of <span class="math inline">\(Y = 1\)</span> when <span class="math inline">\(X = 0\)</span></p></li>
</ul>
<p>Slope:</p>
<ul>
<li><p><span class="math inline">\(\beta_1\)</span>: <strong>predicted change in log-odds</strong> of <span class="math inline">\(Y = 1\)</span> for a unit change in <span class="math inline">\(X\)</span></p></li>
<li><p><span class="math inline">\(e^{\beta_1}\)</span>: predicted amount odds of <span class="math inline">\(Y = 1\)</span> is multiplied by for a unit change in <span class="math inline">\(X\)</span></p></li>
<li><p>The meaning of <span class="math inline">\(\beta_1\)</span> in probability depends on the value of <span class="math inline">\(X\)</span></p></li>
</ul>
</div>
<div id="logistic-regression-as-a-glm" class="section level3">
<h3><span class="header-section-number">5.1.3</span> Logistic regression as a GLM</h3>
<p>Logistic regressions are fit in R using the <code>glm()</code> function with the option <code>family=&quot;binomial&quot;</code>.</p>
<p>Why? Logistic regression is one type of <em>generalized linear model</em> (GLM): a family of models that look like linear regression, but with different choices for each part of Eq. <a href="logistic-regression.html#eq:logit">(5.1)</a>: link function, linear predictor, probability distribution over <span class="math inline">\(Y\)</span>. Logistic regression assumes a binary response <span class="math inline">\(Y\)</span>, and uses the logit link function. Because the logit link is the most commonly used for binomially-distributed data (of which a binary response is a special case, with <span class="math inline">\(n=1\)</span>), specifying <code>family=&quot;binomial&quot;</code> gives a logit link by default (it’s the same as writing <code>family = binomial(link = &quot;logit&quot;)</code>).<a href="#fn22" class="footnoteRef" id="fnref22"><sup>22</sup></a> There are many types of GLM, including some described in an <a href="logistic-regression.html#c4appendix2">Appendix below</a>, but we’ll only discuss logistic regression in this book.</p>
<div id="c4ex1" class="section level4 unnumbered">
<h4>Example 1: Single continuous predictor</h4>
<p>In the <code>givenness</code> data (described in detail <a href="datasets-appendix.html#givedata">here</a>), whether stress shifted is captured by the continuous variable <code>acoustics</code>, which is a composite of prosodic measurements. We check using a simple logistic regression how the perceptual measure is related to the acoustic measure. That is:</p>
<ul>
<li><p><span class="math inline">\(Y\)</span> is <code>stressshift</code></p>
<ul>
<li><span class="math inline">\(=1\)</span> if shifted, 0 if not</li>
</ul></li>
<li><p><span class="math inline">\(X\)</span> is <code>acoustics</code></p>
<ul>
<li>(centered + scaled)</li>
</ul></li>
</ul>
<p>To fit and summarize this model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod1 &lt;-<span class="st"> </span><span class="kw">glm</span>(stressshift ~<span class="st"> </span>acoustics.std, <span class="dt">data=</span>givenness, <span class="dt">family=</span><span class="st">&quot;binomial&quot;</span>)
<span class="kw">summary</span>(mod1)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = stressshift ~ acoustics.std, family = &quot;binomial&quot;, 
##     data = givenness)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.6271  -0.8924  -0.6532   1.1213   2.2418  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)    -0.6897     0.1167  -5.908 3.47e-09 ***
## acoustics.std   1.6371     0.2588   6.325 2.54e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 496.24  on 381  degrees of freedom
## Residual deviance: 448.07  on 380  degrees of freedom
## AIC: 452.07
## 
## Number of Fisher Scoring iterations: 3</code></pre>
<p>A few notes about the model output, compared to the output of the linear regression models we’ve seen so far:</p>
<ul>
<li><p>There are no references to “residuals” or “residual standard error”. This is because the logistic regression equation Eq. <a href="logistic-regression.html#eq:logit">(5.1)</a> has no error term, so there are no residuals.</p></li>
<li><p>There are no “R-squared” values. <span class="math inline">\(R^2\)</span> is not a well-defined concept for logistic regression, for which different measures of goodness of fit are used (discussed <a href="logistic-regression.html#logistic-regression-pseudo-r2">below</a>).</p></li>
<li><p>The model table contains <code>z statistic</code> and <code>Pr(&gt;|z|)</code> columns rather than <code>t</code> and <code>Pr(&gt;|t|)</code> columns. This is because the test statistic <span class="math inline">\(\hat{\beta_i}/{SE(\hat{\beta}_i})\)</span> for logistic regression (<span class="math inline">\(z\)</span> in Eq. <a href="logistic-regression.html#eq:wald">(5.2)</a>) looks like a z-score and <a href="##log-reg-hyp-test">follows a normal distribution</a>, while the test statistic follows a <span class="math inline">\(t\)</span> distribution for linear regression.</p></li>
</ul>
</div>
<div id="interpreting-the-model-odds-and-log-odds" class="section level4">
<h4><span class="header-section-number">5.1.3.1</span> Interpreting the model: Odds and log-odds</h4>
<blockquote>
<p><strong>Questions</strong>:</p>
<ul>
<li>What are the log-odds of stress shifting when <code>acoustics</code> = 0 ? (Remember this means “mean value of <code>acoustics</code>”, because it’s centered.)</li>
</ul>
</blockquote>
<div class="fold o">
<pre><code>## [1] -0.6896608</code></pre>
</div>
<blockquote>
<ul>
<li>What is <span class="math inline">\(P\)</span>(stress shift) when <code>acoustics</code> = 0 ?</li>
</ul>
</blockquote>
<div class="fold o">
<pre><code>## [1] 0.3341085</code></pre>
</div>
<blockquote>
<ul>
<li>Increasing <code>acoustics</code> by 1 (= 2 SD) corresponds to increasing the log-odds of stress shifting by _______ ?</li>
</ul>
</blockquote>
<div class="fold o">
<pre><code>## [1] 1.637052</code></pre>
</div>
<blockquote>
<ul>
<li>Increasing <code>acoustics</code> by 1 (= 2 SD) corresponds to multiplying the odds of stress shifting by _______ ?</li>
</ul>
</blockquote>
<div class="fold o">
<pre><code>## [1] 5.139996</code></pre>
</div>
<!-- invlogit(-0.6897) = 0.334.  increasing acoustics.std by 1 corresponds to increase in logit by 1.637, = multiplying odds by 5.14 = exp(1.637) -->
</div>
<div id="interpreting-the-model-probability" class="section level4">
<h4><span class="header-section-number">5.1.3.2</span> Interpreting the model: Probability</h4>
<p>We can plot the predicted <span class="math inline">\(P\)</span>(stress shift) as a function of <code>acoustics</code> to get a sense of what the model predicts:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## set up dataframe with range of acoustics.std we want to predict over
newdata &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">acoustics.std=</span><span class="kw">seq</span>(-<span class="fl">1.75</span>,<span class="fl">1.75</span>, <span class="dt">by=</span><span class="fl">0.01</span>))
## get the model&#39;s predictions in log-odds space
newdata$predLogit &lt;-<span class="st"> </span><span class="kw">predict</span>(mod1, <span class="dt">newdata=</span>newdata)
## transform those preditions to probability space
newdata &lt;-<span class="st"> </span>newdata %&gt;%<span class="st"> </span><span class="kw">mutate</span>(<span class="dt">predP=</span><span class="kw">invlogit</span>(predLogit))

<span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>acoustics.std, <span class="dt">y=</span>predP), <span class="dt">data=</span>newdata) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>() +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data=</span>givenness, <span class="kw">aes</span>(<span class="dt">y=</span>(<span class="kw">as.numeric</span>(stressshift)-<span class="dv">1</span>)), <span class="dt">size=</span><span class="fl">0.5</span>) +
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;acoustics (standardized)&quot;</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;P(stress shift)&quot;</span>)</code></pre></div>
<p><img src="05-logistic-regression_files/figure-html/unnamed-chunk-7-1.png" width="336" style="display: block; margin: auto;" /></p>
<p>As expected, there is a strong relationship between the acoustic proxy for stress shift, and whether stress is perceived as having shifted.</p>
<!-- * Note: The data covers most of the range in probability space. Strong effect, good design. -->
<p>Let’s work out in more detail what “increase in log-odds” means in probability space for this model:</p>
<ul>
<li><p>When <code>acoustics</code> = 0:</p>
<ul>
<li><p>log-odds = -0.68</p></li>
<li><p>odds = 0.506</p></li>
<li><p><span class="math inline">\(P\)</span>(stress shift) = 0.336 = <span class="math inline">\(\text{logit}^{-1}\)</span>(-0.68)</p></li>
</ul></li>
<li><p>When <code>acoustics</code> = 1:</p>
<ul>
<li><p>log-odds = 0.947</p></li>
<li><p>odds = 2.58 = <span class="math inline">\(e^{0.947}\)</span></p></li>
<li><p><span class="math inline">\(P\)</span>(Y = 1) = 0.720 = <span class="math inline">\(\text{logit}^{-1}\)</span>(0.947)</p></li>
</ul></li>
</ul>
<p>Thus, the change in probability <span class="math inline">\(P\)</span>(Y = 1) when you increase <span class="math inline">\(X\)</span> by 1 is 0.348 here (<span class="math inline">\(X: 0 \to 1\)</span>) However, in general what change in probability results when <span class="math inline">\(X\)</span> is increased by 1 (a “unit change”) will depend on where you start from: <img src="05-logistic-regression_files/figure-html/unnamed-chunk-8-1.png" width="336" style="display: block; margin: auto;" /></p>
<p>Note that the greatest change in probability occurs when <span class="math inline">\(P(Y=1|X) = 0.5\)</span> (at <span class="math inline">\(X=0.69 \implies \beta_0 + \beta_1 X = 0\)</span>). The change in probability here (<span class="math inline">\(\approx 0.4\)</span>) is roughly 1/4 of the fitted slope value: <span class="math display">\[
\frac{\beta_1}{4} = \frac{1.637}{4} = 0.41
\]</span> It turns out that this relationship holds more generally, and the <em>divide-by-4 rule</em> can be used to roughly interpret logistic regression slopes in terms of probability (<span class="citation">Gelman &amp; Hill (<a href="#ref-gelman2007data">2007</a>)</span>, p. 82): the slope (in log-odds) is about 4x the maximum change in probability.</p>
</div>
<div id="c4ex2" class="section level4 unnumbered">
<h4>Example 2: Single categorical predictor</h4>
<p>Let’s now predict the probability of stress shifting (<code>stressshift</code>) for the <code>givenness</code> data, as a function of the NP type (<code>npType</code>). This predictor is “dummy coded”, with values:</p>
<ul>
<li><p><span class="math inline">\(X = 0\)</span>: Full NP</p></li>
<li><p><span class="math inline">\(X = 1\)</span>: Pronoun</p></li>
</ul>
<p>The regression model is still Eq. <a href="logistic-regression.html#eq:logit">(5.1)</a>.</p>
<blockquote>
<p><strong>Questions</strong>: What are the interpretations of the regression coefficients?</p>
<ul>
<li><p>Intercept (<span class="math inline">\(\beta_0\)</span>)</p></li>
<li><p>Slope (<span class="math inline">\(\beta_1\)</span>)</p></li>
</ul>
</blockquote>
<p>To fit and summarize this model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod2 &lt;-<span class="st"> </span><span class="kw">glm</span>(stressshift ~<span class="st"> </span>npType, <span class="dt">data=</span>givenness, <span class="dt">family=</span><span class="st">&quot;binomial&quot;</span>)
<span class="kw">summary</span>(mod2)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = stressshift ~ npType, family = &quot;binomial&quot;, data = givenness)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.014  -1.014  -0.850   1.350   1.545  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)    -0.8321     0.1587  -5.244 1.57e-07 ***
## npTypepronoun   0.4353     0.2159   2.016   0.0438 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 496.24  on 381  degrees of freedom
## Residual deviance: 492.14  on 380  degrees of freedom
## AIC: 496.14
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>The model predicts <span class="math inline">\(P\)</span>(stress shift) to be:</p>
<ul>
<li><p>Full NPs: <span class="math inline">\(\text{logit}^{-1}(-0.832)=\)</span> <strong>0.303</strong></p></li>
<li><p>Pronouns: <span class="math inline">\(\text{logit}^{-1}(-0.832 + 0.435)=\)</span> <strong>0.402</strong></p></li>
</ul>
<p>We can compare to the observed proportion of stress-shifted observations for each NP type:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Observed condtion means:
<span class="kw">prop.table</span>(<span class="kw">xtabs</span>(~npType +<span class="st"> </span>stressshift, <span class="dt">data=</span>givenness), <span class="dt">margin=</span><span class="dv">1</span>)</code></pre></div>
<pre><code>##          stressshift
## npType      noshift     shift
##   full    0.6968085 0.3031915
##   pronoun 0.5979381 0.4020619</code></pre>
<p>and see that the model matches these proportions exactly (<code>shift</code> column). This makes sense, as predicting these two numbers is all the model has to do for simple logistic regression with one categorical predictor.</p>
</div>
</div>
<div id="c4differences" class="section level3">
<h3><span class="header-section-number">5.1.4</span> Differences from linear regression: Fitting and interpretation</h3>
<p>There are two important differences between logistic regression and linear regression, for our purposes:</p>
<ol style="list-style-type: decimal">
<li><p>The <strong>relationship between the response and what’s being modeled</strong>. In linear regression, these are the same thing: <span class="math inline">\(Y\)</span> is the response variable, which is also what the regression is modeling. In logistic regression, these are not the same thing: we observe <span class="math inline">\(Y\)</span>, which takes on values 0 or 1, but we model the <em>expected value</em> of <span class="math inline">\(Y\)</span> (<span class="math inline">\(E(Y)\)</span>): the probability that <span class="math inline">\(Y=1\)</span>.</p></li>
<li><p>The <strong>presence of an error term</strong>. In linear regression, there is an error term <span class="math inline">\(\epsilon_i\)</span>, capturing the difference between the fitted value (<span class="math inline">\(\hat{y}_i\)</span>) and the actual value (<span class="math inline">\(y_i\)</span>) for each observation, along with the linear predictor (<span class="math inline">\(\beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip}\)</span>). In logistic regression, there is no error term: the right-hand side of Eq. <a href="logistic-regression.html#eq:logit">(5.1)</a> predicts <span class="math inline">\(P(Y=1)\)</span> using just the linear predictor.</p></li>
</ol>
<p>Why? For a Bernoulli random variable <span class="math inline">\(Y\)</span> (basically, a coin that comes up “heads” with some probability <span class="math inline">\(p\)</span>), the variance and expectation only depend on one unknown parameter, <span class="math inline">\(p\)</span>: <span class="math display">\[
E(Y) = p, \quad \text{Var}(Y) = p(1-p)
\]</span> So unlike for linear regression, where the mean (<span class="math inline">\(\mu\)</span>) and variance (<span class="math inline">\(\sigma\)</span>) in our model of the response can be tweaked independently to better fit the data, in logistic regression these two things are not independent. By fitting the mean (probability that <span class="math inline">\(Y=1\)</span>), we have already determined the variance.</p>
<p>Both differences make interpreting and fitting logistic regression models less straightforward than linear regression models.</p>
</div>
<div id="fitting-a-logistic-regression-model" class="section level3">
<h3><span class="header-section-number">5.1.5</span> Fitting a logistic regression model</h3>
<p>For linear regression, it was possible to estimate the regression coefficients by “least squares”: minimizing the difference between the model’s prediction and the observed data (<span class="math inline">\(\epsilon_i^2\)</span>), across all points. Least-squares has a <em>closed-form</em> solution: the regression coefficients can be solved for by plugging the predictor and response values for all observations into an equation.</p>
<p>For logistic regression, there are no residuals, so we can’t use a least-squares method. Instead, we use a likelihood-based method: determine the probability of the observed data, given values of the regression coefficients (the data <em>likelihood</em>), then find the regression coefficient values that maximize this probability.</p>
For simple logistic regression: for any <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, the likelihood of the data is:
<span class="math display" id="eq:likelihood">\[\begin{equation*}
  L(\beta_0, \beta_1) = \sum^n_{i = 1} \left[ \text{logit}^{-1}(\beta_0 + \beta_1 x_i) \right]^{y_i}
      \tag{5.3}
\end{equation*}\]</span>
<p>We choose <span class="math inline">\(\hat{\beta}_0, \hat{\beta}_1\)</span> that maximize the likelihood: the <em>maximum likelihood</em> (ML) estimates. There is no closed form solution for <span class="math inline">\(\hat{\beta}_0, \hat{\beta}_1\)</span>, which must instead be estimated numerically, using an estimation algorithm.<a href="#fn23" class="footnoteRef" id="fnref23"><sup>23</sup></a></p>
</div>
<div id="interpretation" class="section level3">
<h3><span class="header-section-number">5.1.6</span> Interpretation</h3>
<p>You of course don’t need to understand the math to fit <code>lm()</code> or <code>glm()</code> models—R will do this for you automatically. However, it can be helpful to know some basic aspects of how these models are fitted to interpret model output, and some otherwise cryptic error messages. For example:</p>
<ul>
<li><p><code>(Dispersion parameter for binomial family taken to be 1)</code> in logistic regression output: related to the relationship between mean and variance—the amount of variability in the data is assumed to be similar to what you’d expect based on the estimated mean. If it’s not, your data is “over-dispersed” or “under-dispersed.”<a href="#fn24" class="footnoteRef" id="fnref24"><sup>24</sup></a></p></li>
<li><p><code>Number of Fisher Scoring iterations: X</code> in logistic regression output: refers to the numeric algorithm used to fit logistic regression models (see footnote 3).</p></li>
<li><p>There is no <span class="math inline">\(F\)</span> statistic or <span class="math inline">\(p\)</span> value in logistic regression output: this is because there are no residuals, so the “full” and “reduced” models cannot be compared via their residual sums of squares (which are used to conduct an <span class="math inline">\(F\)</span> test for linear regression).</p></li>
<li><p>If you try to fit a logistic regression model with just <span class="math inline">\(Y=1\)</span> response values (no <span class="math inline">\(Y=0\)</span> values), you’ll get the message <code>glm.fit: algorithm did not converge</code>:</p></li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">badmodel &lt;-<span class="st"> </span><span class="kw">glm</span>(stressshift ~<span class="st"> </span>acoustics.std, <span class="dt">data=</span><span class="kw">filter</span>(givenness, stressshift==<span class="st">&#39;shift&#39;</span>), <span class="dt">family=</span><span class="st">&quot;binomial&quot;</span>)</code></pre></div>
<pre><code>## Warning: glm.fit: algorithm did not converge</code></pre>
<p>This means that the numeric algorithm used to maximize likelihood couldn’t find a good solution for the regression coefficients. This makes sense, because the model is trying to fit 100% probabilities (so log-odds = <span class="math inline">\(\infty\)</span>), and has no data on the basis of which it can estimate the slope (<span class="math inline">\(\beta_1\)</span>).</p>
</div>
</div>
<div id="evaluating-logistic-regression-models" class="section level2">
<h2><span class="header-section-number">5.2</span> Evaluating logistic regression models</h2>
<p>We would like a measure of goodness-of-fit for logistic regression models: how well does the model predict <span class="math inline">\(Y\)</span>, compared to a baseline model? For linear regression, we used <span class="math inline">\(R^2\)</span> to quantify how similar the model’s predictions (<span class="math inline">\(\hat{y}_i\)</span>) were to the observations (<span class="math inline">\(y_i\)</span>), relative to a baseline model where the model’s prediction is always the grand mean—<span class="math inline">\(R^2\)</span> was simply the correlation between observed and predicted values.</p>
<p>However, for logistic regression we cannot directly compare model predictions (<span class="math inline">\(p_i\)</span>: (log-odds of) probabilities) to observations (<span class="math inline">\(y_i\)</span>: 0 or 1), so we need a different measure of goodness-of-fit. We consider three commonly used options:</p>
<ol style="list-style-type: decimal">
<li><p>Likelihood ratio test</p></li>
<li><p>Classification accuracy</p></li>
<li><p>Pseudo-<span class="math inline">\(R^2\)</span></p></li>
</ol>
<p>to compare a logistic regression model to a baseline (intercept-only model). We assume here that the logistic regression model has just one predictor <span class="math inline">\(X\)</span>, and denote this model by <span class="math inline">\(M_1\)</span> and the baseline model by <span class="math inline">\(M_0\)</span>.</p>
<div id="c4lrt" class="section level3">
<h3><span class="header-section-number">5.2.1</span> Likelihood ratio test</h3>
<p>This option is analogous to the <span class="math inline">\(F\)</span> test for linear regression models, where the full model is compared to the intercept-only model, and we test whether the change in sum-of-squares is significant, given the added predictors in the full model.</p>
<p>Logistic regression models don’t have sums-of-squares because they don’t have residuals. Instead, we compare the <strong>likelihood</strong> (Eq. <a href="logistic-regression.html#eq:likelihood">(5.3)</a>) of the two models: is the change in likelihood between <span class="math inline">\(M_0\)</span> and <span class="math inline">\(M_1\)</span> significant?</p>
Most methods involving likelihood use log-transformed likelihood, mostly to avoid numerical issues when computing with very small numbers. Instead of working with straight log-likelihood, it is also customary to define the <em>deviance</em> of a model <span class="math inline">\(M\)</span> (<span class="citation">Agresti (<a href="#ref-agresti2007">2007</a>)</span> 3.4.3), as
<span class="math display">\[\begin{equation*}
  D = -2 \log L(M)
\end{equation*}\]</span>
<p>Thus, a better model (higher likelihood) has lower deviance.</p>
The difference in deviance (<span class="math inline">\(\Delta D\)</span>) between models with (<span class="math inline">\(M_1\)</span>) and without (<span class="math inline">\(M_0\)</span>) the single predictor is related to the log of their <em>likelihood ratio</em>:
<span class="math display">\[\begin{equation*}
  \Delta D = -2 \log \frac{L(M_0)}{L(M_1)}
\end{equation*}\]</span>
<p>For large enough sample size, <span class="math inline">\(\Delta D\)</span> follows a <span class="math inline">\(\chi^2(1)\)</span> distribution if the slope of <span class="math inline">\(X\)</span> (<span class="math inline">\(\beta_1\)</span>) is 0. Thus, we can use <span class="math inline">\(\Delta D\)</span> as a test statistics for the null hypothesis that <span class="math inline">\(\beta_1 = 0\)</span>. This kind of test is called a <em>likelihood ratio test</em>. The <span class="math inline">\(p\)</span> value of this test gives a measure of goodness of fit, while <span class="math inline">\(\Delta D\)</span> gives a sort of measure of variance accounted for (if you think of deviance as capturing “variance” from the perfect model).</p>
<p>For our <a href="logistic-regression.html#c4ex1">Example 1</a>, an LR test is conducted in R as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod1 &lt;-<span class="st"> </span><span class="kw">glm</span>(stressshift ~<span class="st"> </span>acoustics.std, <span class="dt">data=</span>givenness, <span class="dt">family=</span><span class="st">&quot;binomial&quot;</span>)
mod0 &lt;-<span class="st"> </span><span class="kw">glm</span>(stressshift ~<span class="st"> </span><span class="dv">1</span>, <span class="dt">data=</span>givenness, <span class="dt">family=</span><span class="st">&quot;binomial&quot;</span>)
## LR test of the effect of acoustics.std in mod1
<span class="kw">anova</span>(mod0, mod1, <span class="dt">test=</span><span class="st">&#39;Chisq&#39;</span>)</code></pre></div>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: stressshift ~ 1
## Model 2: stressshift ~ acoustics.std
##   Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    
## 1       381     496.24                          
## 2       380     448.07  1    48.17 3.909e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Note that for simple linear regression, this is just another way to test the hypothesis that <span class="math inline">\(\beta_1 = 0\)</span>, in addition to the <span class="math inline">\(z\)</span> test reported in the regression summary.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(mod1)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = stressshift ~ acoustics.std, family = &quot;binomial&quot;, 
##     data = givenness)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.6271  -0.8924  -0.6532   1.1213   2.2418  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)    -0.6897     0.1167  -5.908 3.47e-09 ***
## acoustics.std   1.6371     0.2588   6.325 2.54e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 496.24  on 381  degrees of freedom
## Residual deviance: 448.07  on 380  degrees of freedom
## AIC: 452.07
## 
## Number of Fisher Scoring iterations: 3</code></pre>
<p>However, for multiple regression models with more than one predictor, the LR test gives a measure of overall model fit, and is not testing the same thing as the <span class="math inline">\(z\)</span> tests reported by <code>summary(model)</code> for each predictor.</p>
</div>
<div id="classification-accuracy" class="section level3">
<h3><span class="header-section-number">5.2.2</span> Classification accuracy</h3>
A second way to assess model quality comes from thinking of the model as predicting <span class="math inline">\(Y\)</span>, as follows:
<span class="math display">\[\begin{equation*}
  \hat{y}_i = 
  \begin{cases}
    1 &amp; \text{if } \hat{p}_i &gt; 0.5 \\
    0 &amp; \text{if } \hat{p}_i \leq 0.5
  \end{cases}
\end{equation*}\]</span>
<p>That is, just predict <span class="math inline">\(Y=1\)</span> if the predicted log-odds are positive, and <span class="math inline">\(Y=0\)</span> if the predicted log-odds are negative. We can then define <em>classification accuracy</em> as the percentage of observations where the predicted and observed values are the same (<span class="math inline">\(\hat{y}_i = y_i\)</span>).</p>
Classification accuracy always needs to be compared to a baseline, usually “how often would we classify correctly if we chose the most common case?” Formally, if <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span> are the number of observations where <span class="math inline">\(y_i = 0\)</span> and <span class="math inline">\(y_i = 1\)</span> in the data, the baseline classification accuracy is:
<span class="math display">\[\begin{equation*}
  \max \left( \frac{n_1}{n}, \frac{n_2}{n} \right)
\end{equation*}\]</span>
<p>Here are functions for computing the classification accuracy and baseline accuracy of a logistic regression model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## function for computing accuracy of a logistic regression model 
## (on the dataset used to fit the model)
## lrMod = fitted model
## responseVar = name of response variable for lrMod
##
## adapted from: https://www.r-bloggers.com/evaluating-logistic-regression-models/
lrAcc &lt;-<span class="st"> </span>function(lrMod, responseVar){
  ## convert response variable into a factor if it&#39;s not one
  if(!<span class="kw">is.factor</span>(<span class="kw">model.frame</span>(lrMod)[,responseVar])){
    <span class="kw">model.frame</span>(lrMod)[,responseVar] &lt;-<span class="st"> </span><span class="kw">as.factor</span>(<span class="kw">model.frame</span>(lrMod)[,responseVar])
  }
    ## model predictions in log-odds
    preds =<span class="st"> </span><span class="kw">predict</span>(lrMod, <span class="dt">newdata=</span><span class="kw">model.frame</span>(lrMod))
    ## transform to 0/1 prediction
    preds &lt;-<span class="st"> </span>((<span class="kw">sign</span>(preds)/<span class="dv">2</span>)+<span class="fl">0.5</span>)
    
    ## response variable values, transformed to 0/1
    y &lt;-<span class="st"> </span>(<span class="kw">as.numeric</span>(<span class="kw">model.frame</span>(lrMod)[,responseVar])-<span class="dv">1</span>)
    
    ## how often is prediction the same as the actual response
    acc &lt;-<span class="st"> </span><span class="kw">sum</span>(preds==y)/<span class="kw">length</span>(preds)
    
    <span class="kw">return</span>(acc)
}

## baseline accuracy for a logisitic regression model lrMod
## with a given response variable
baselineAcc &lt;-<span class="st"> </span>function(lrMod, responseVar){
    response &lt;-<span class="st"> </span><span class="kw">model.frame</span>(lrMod)[,responseVar]
    tab &lt;-<span class="st"> </span><span class="kw">table</span>(response)
    <span class="kw">return</span>(<span class="kw">max</span>(tab)/<span class="kw">sum</span>(tab))
}

## baseline accuracy</code></pre></div>
<p>For our <a href="logistic-regression.html#c4ex1">Example 1</a> (<code>stresshift ~ acoustics.std</code>), the classification accuracy is</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">lrAcc</span>(mod1, <span class="st">&#39;stressshift&#39;</span>)</code></pre></div>
<pre><code>## [1] 0.7015707</code></pre>
<p>Compared to a baseline of 0.65 (using <code>baselineAcc(mod1, 'stressshift')</code>). The model performs better than the baseline, but its performance is less impressive than it seems (70%) given that the most common case (not shifting stress) accounts for 65% of the data.</p>
</div>
<div id="logistic-regression-pseudo-r2" class="section level3">
<h3><span class="header-section-number">5.2.3</span> Pseudo-<span class="math inline">\(R^2\)</span></h3>
<p>There is no equivalent to <span class="math inline">\(R^2\)</span> for logistic regression, meaning a quantity with similar properties and multiple interpretations:</p>
<ul>
<li><p>Fraction reduction in sum-of-squares (there is no sum-of-squares)</p></li>
<li><p>Degree of “variance” accounted for (“variance” isn’t well-defined)</p></li>
<li><p>Squared correlation between fitted and observed values (different scales: probabilities versus 0/1)</p></li>
</ul>
<p>Nonetheless, we might want an <span class="math inline">\(R^2\)</span>-like quantity that at least has similar properties, if we find such measures easier to interpret than classification accuracy or an LR test result. A number of <em>pseudo-<span class="math inline">\(R^2\)</span></em> measures exist, of which the two most common are:</p>
<ul>
<li><p><strong>Cox-Snell</strong> pseudo-<span class="math inline">\(R^2\)</span>: a value <span class="math inline">\(\geq 0\)</span></p></li>
<li><p><strong>Nagelkerke</strong> pseudo-<span class="math inline">\(R^2\)</span>: a value between 0 and 1 (like <span class="math inline">\(R^2\)</span> for linear regression)</p></li>
</ul>
<p>Both measures are related to the likelihood ratio of the full and reduced (intercept-only) model. Pseudo-<span class="math inline">\(R^2\)</span> measures should not be taken too seriously, but can be useful, for example for comparing goodness of fit between logistic regression and linear regression models. We won’t consider these methods further, but they are reported in some papers.</p>
<div id="example-15" class="section level4 unnumbered">
<h4>Example</h4>
<p>How well does our <a href="logistic-regression.html#c4ex2">Example 2</a> model (<code>stressshift ~ npType</code>) do at predicting whether stress shifts?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod2 &lt;-<span class="st"> </span><span class="kw">glm</span>(stressshift ~<span class="st"> </span>npType, <span class="dt">data=</span>givenness, <span class="dt">family=</span><span class="st">&quot;binomial&quot;</span>)</code></pre></div>
<p>By the likelihood ratio test:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## LR test of the effect of npType in mod2
<span class="kw">anova</span>(mod2,mod0, <span class="dt">test=</span><span class="st">&#39;Chisq&#39;</span>)</code></pre></div>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: stressshift ~ npType
## Model 2: stressshift ~ 1
##   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)  
## 1       380     492.14                       
## 2       381     496.24 -1  -4.0972  0.04295 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>there is a (barely) significant effect of <code>npType</code>, corresponding to a difference in deviance of <span class="math inline">\(\Delta D = 4.1\)</span>.</p>
<p>However, classification accuracy:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## accuracy of mod2
<span class="kw">lrAcc</span>(mod2, <span class="st">&#39;stressshift&#39;</span>)</code></pre></div>
<pre><code>## [1] 0.6465969</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## it&#39;s the same as the baseline&#39;s accuracy
<span class="kw">baselineAcc</span>(mod1, <span class="st">&#39;stressshift&#39;</span>)</code></pre></div>
<pre><code>## [1] 0.6465969</code></pre>
<p>does not differ between the baseline and full models. This example illustrates a couple of points:</p>
<ul>
<li>Different methods for comparing two models won’t necessarily give the same qualitative answers—as we <a href="linear-regression.html#non-nested-model-comparison">already saw</a> with AIC versus BIC-based model selection.</li>
</ul>
<!-- (See Sec. \@ref(non-nested-model-comparison) for more details.) -->
<ul>
<li>Classification accuracy is a blunter tool than an LR test—it does not reflect effects that are small compared to baseline accuracy, because in order to affect classification accuracy an effect has to be big enough to change the sign of the predicted log odds.</li>
</ul>
</div>
</div>
</div>
<div id="multiple-logistic-regression" class="section level2">
<h2><span class="header-section-number">5.3</span> Multiple logistic regression</h2>
<p>Like for linear regression, generalizing from one predictor to multiple predictors is straightforward for logistic regression.</p>
We assume there are <span class="math inline">\(p\)</span> predictors (<span class="math inline">\(X_1, ..., X_p\)</span>) of a binary response <span class="math inline">\(Y\)</span>, where each <span class="math inline">\(X_i\)</span> can be continuous or categorical. The logistic regression model is now:
<span class="math display">\[\begin{equation*}
  \text{logit}(P(Y = 1)) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p
\end{equation*}\]</span>
Rewriting this model in terms of odds by exponentiating each side:
<span class="math display">\[\begin{equation*}
  \frac{P(Y = 1)}{P(Y = 0)} = e^{\beta_0}e^{\beta_1 X_1}\cdots e^{\beta_p X_p}
\end{equation*}\]</span>
or, rewriting with <span class="math inline">\(\alpha_i = e^{\beta_i}\)</span>:
<span class="math display">\[\begin{equation*}
  \frac{P(Y = 1)}{P(Y = 0)} = \alpha_0 \alpha_1^{X_1}\cdots \alpha_p^{X_p}
\end{equation*}\]</span>
<p>That is, each predictor’s effect on the odds are <strong>multiplicative</strong>. A predictor that has no effect corresponds to multiplying by 1 (<span class="math inline">\(\alpha_i = 1\)</span>), while predictors with “negative” effects decrease the odds (<span class="math inline">\(\alpha_i &lt; 1\)</span>) and predictors with “positive”&quot; effects increase the odds (<span class="math inline">\(\alpha_i &gt; 1\)</span>).</p>
<div id="likelihood-ratio-test-general-case" class="section level3">
<h3><span class="header-section-number">5.3.1</span> Likelihood ratio test: General case</h3>
<p>The likelihood ratio test generalizes to the case of comparing two nested logistic regression models, with <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> predictors:</p>
<span class="math display">\[\begin{align}
M_0 &amp; :  X_1, ..., X_p  \\
M_1 &amp; : X_1, ..., X_p,  X_{p + 1}, ..., X_q
\end{align}\]</span>
<p>With the difference in deviance again defined as <span class="math display">\[
  \Delta D = - 2\log\frac{L(M_0)}{L(M_1)}
\]</span></p>
<p><span class="math inline">\(\Delta D\)</span> approximately follows a <span class="math inline">\(\chi^2(q - p)\)</span> distribution (<span class="citation">Agresti (<a href="#ref-agresti2003categorical">2003</a>)</span> 3.4.4), if <span class="math display">\[
  \beta_{p + 1} = \beta_{p + 2} = \cdots = \beta_q = 0
\]</span></p>
<p>Thus, we can use <span class="math inline">\(\Delta D\)</span> as a test statistic for the null hypothesis that all the added predictors have no effect: <span class="math display">\[
H_0: \beta_{p + 1} = \beta_{p + 2} = \cdots = \beta_q = 0
\]</span></p>
</div>
<div id="log-reg-worked-example" class="section level3">
<h3><span class="header-section-number">5.3.2</span> Worked example</h3>
<p>In this example, we model the probability of shifting stress (<code>stressshift</code>) for the <code>givenness</code> data, as a function of four predictors—all standardized (see code chunk at the beginning of this chapter):</p>
<ol style="list-style-type: decimal">
<li><p>Condition: <code>conditionLabel.williams</code></p></li>
<li><p>NP Type: <code>npType.pronoun</code></p></li>
<li><p>Voicing: <code>voice.passive</code></p></li>
<li><p>Trial number: <code>order</code></p></li>
</ol>
<p>We will carry out a full analysis (except model criticism):</p>
<ul>
<li><p>Exploratory data analysis, motivating which terms to include in a model</p></li>
<li><p>Fit a first model</p></li>
<li><p>Variable selection: adding and dropping terms</p></li>
<li><p>Model evaluation</p></li>
</ul>
<div id="exploratory-data-analysis" class="section level4 unnumbered">
<h4>Exploratory data analysis</h4>
<p>In this empirical plot of how <code>order</code> affects the probability of stress shifting, our actual observations appear as points at <span class="math inline">\(y=0\)</span> or <span class="math inline">\(y=1\)</span>, while the prediction from a simple logistic regression (on just <code>order</code>) appears as the fitted blue line:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>order, <span class="dt">y=</span>stressshift.num), <span class="dt">data=</span>givenness) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method=</span><span class="st">&quot;glm&quot;</span>, <span class="dt">method.args =</span> <span class="kw">list</span>(<span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>))  +
<span class="st">  </span><span class="kw">geom_jitter</span>(<span class="dt">position=</span><span class="kw">position_jitter</span>(<span class="dt">width=</span><span class="fl">0.2</span>,<span class="dt">height=</span><span class="fl">0.03</span>),<span class="dt">size=</span><span class="dv">1</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Proportion shifted&quot;</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Order vs. proportion with shifted prominence&quot;</span>) +
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">hjust =</span> <span class="fl">0.5</span>))</code></pre></div>
<p><img src="05-logistic-regression_files/figure-html/unnamed-chunk-19-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>Proportion tables showing how the probability of stress shifting depends on each categorical predictor (plots would be better):</p>
<ul>
<li><code>conditionLabel</code>:</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">prop.table</span>(<span class="kw">xtabs</span>(~conditionLabel+stressshift, <span class="dt">data=</span>givenness), <span class="dt">margin=</span><span class="dv">1</span>)</code></pre></div>
<pre><code>##               stressshift
## conditionLabel    noshift      shift
##       Contrast 0.91919192 0.08080808
##       Williams 0.35326087 0.64673913</code></pre>
<ul>
<li><code>npType</code>:</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">prop.table</span>(<span class="kw">xtabs</span>(~npType+stressshift, <span class="dt">data=</span>givenness), <span class="dt">margin=</span><span class="dv">1</span>)</code></pre></div>
<pre><code>##          stressshift
## npType      noshift     shift
##   full    0.6968085 0.3031915
##   pronoun 0.5979381 0.4020619</code></pre>
<ul>
<li><code>voice</code>:</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">prop.table</span>(<span class="kw">xtabs</span>(~voice+stressshift, <span class="dt">data=</span>givenness), <span class="dt">margin=</span><span class="dv">1</span>)</code></pre></div>
<pre><code>##          stressshift
## voice       noshift     shift
##   active  0.7005348 0.2994652
##   passive 0.5948718 0.4051282</code></pre>
<p>The expected directions of effects of the predictors on percent stress-shifted are:</p>
<ul>
<li><p><code>order</code>: positive</p></li>
<li><p><code>conditionLabel.williams</code>, <code>npType.pronoun</code>, <code>voice.passive</code>: positive</p></li>
</ul>
</div>
<div id="model" class="section level4 unnumbered">
<h4>Model</h4>
<p>We fit a first multiple logistic regression model using just main effects of the four predictors:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod3 &lt;-<span class="st"> </span><span class="kw">glm</span>(stressshift ~<span class="st"> </span>npType.pron +<span class="st"> </span>clabel.williams +<span class="st"> </span>voice.passive +<span class="st"> </span>order.std, 
            <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>, 
            <span class="dt">data =</span> givenness)
<span class="kw">summary</span>(mod3)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = stressshift ~ npType.pron + clabel.williams + voice.passive + 
##     order.std, family = &quot;binomial&quot;, data = givenness)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.8358  -0.5249  -0.3509   0.7644   2.6344  
## 
## Coefficients:
##                 Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)      -1.0092     0.1580  -6.389 1.67e-10 ***
## npType.pron       0.5985     0.2746   2.179   0.0293 *  
## clabel.williams   3.1848     0.3179  10.018  &lt; 2e-16 ***
## voice.passive     0.8026     0.2803   2.863   0.0042 ** 
## order.std         0.3044     0.2742   1.110   0.2669    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 496.24  on 381  degrees of freedom
## Residual deviance: 335.90  on 377  degrees of freedom
## AIC: 345.9
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>Note that because the predictors are standardized, we can interpret their values as relative effect sizes: the <code>conditionLabel</code> effect is much stronger than the others, then <code>voice</code> &gt; <code>npType</code> &gt; <code>order</code>. The direction of all effects is positive, as predicted.</p>
<p>Let’s interpret a couple terms from the model, in terms of log-odds and probability:</p>
<ol style="list-style-type: decimal">
<li><p>Intercept</p>
<ul>
<li><p><span class="math inline">\(\beta_0 = -1.01\)</span>, corresponds to probability <span class="math inline">\(0.267\)</span></p></li>
<li><p>Thus, 27% stress shift is predicted overall (all predictors held at mean values)</p></li>
</ul></li>
<li><p>Slope for <code>conditionLabel.williams</code></p>
<ul>
<li><p>Coefficient = 3.18</p></li>
<li><p>Interpretation: the odds of stress shifting are multiplied by <strong>24.0</strong> (<span class="math inline">\(e^{3.18}\)</span>) in Williams condition (with other predictors at mean values). This is a huge effect!</p></li>
<li><p>Corresponds to a maximum change of 0.80 in probability (divide-by-4-rule: 3.18/4=0.80).</p></li>
</ul></li>
</ol>
<p><strong>Exercise</strong>:</p>
<p>What are the predicted log-odds of stress shifting when:</p>
<ul>
<li><p><code>order</code> = mean value</p></li>
<li><p><code>npType</code> = pronoun</p></li>
<li><p><code>conditionLabel</code> = Williams</p></li>
<li><p><code>voice</code> = active</p></li>
</ul>
<p>Hint: assume that the three categorical variables just take on values -0.5 and 0.5.</p>
<div class="fold o">
<pre><code>## [1] 0.6818186</code></pre>
</div>
</div>
<div id="model-evaluation" class="section level4 unnumbered">
<h4>Model evaluation</h4>
<p>To evaluate the model in terms of how much likelihood improves from a null model, we first set the null model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod3<span class="fl">.0</span> &lt;-<span class="st"> </span><span class="kw">glm</span>(stressshift ~<span class="st"> </span><span class="dv">1</span>, <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>, <span class="dt">data =</span> givenness)</code></pre></div>
<p>Then carry out the likelihood ratio test:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(mod3, mod3<span class="fl">.0</span>, <span class="dt">test=</span><span class="st">&quot;Chisq&quot;</span>)</code></pre></div>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: stressshift ~ npType.pron + clabel.williams + voice.passive + 
##     order.std
## Model 2: stressshift ~ 1
##   Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    
## 1       377     335.90                          
## 2       381     496.24 -4  -160.34 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Note that the degrees of freedom of this comparison is <span class="math inline">\(p-q = 4\)</span>, since there are four predictors in the full model. The difference in deviance is <span class="math inline">\(\Delta D = 160.34\)</span>. Since we find <code>Pr(&gt;Chisq) &lt; 2.2e-16</code> we may conclude that our four terms make a very significant contribution to the overall model likelihood.</p>
<p>To assess model quality using classification accuracy:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## classification accuracy
<span class="kw">lrAcc</span>(mod3, <span class="st">&#39;stressshift&#39;</span>)</code></pre></div>
<pre><code>## [1] 0.8089005</code></pre>
<p>Compared to baseline accuracy:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## baseline accuracy
<span class="kw">baselineAcc</span>(mod3, <span class="st">&#39;stressshift&#39;</span>)</code></pre></div>
<pre><code>## [1] 0.6465969</code></pre>
<p>Thus, the four predictors improve classification accuracy by 15%.</p>
</div>
<div id="variable-selection" class="section level4 unnumbered">
<h4>Variable selection</h4>
<p>In this exercise we will follow the variable selection guidelines from Gelman &amp; Hill discussed <a href="linear-regression.html#c2varselect">in the previous chapter</a>.</p>
<p>First two steps:</p>
<ol style="list-style-type: decimal">
<li><p>Include main effects for predictors expected to affect the response (done)</p></li>
<li><p>Consider interactions between terms with large effects (we do this now)</p></li>
</ol>
<p>These empirical plots, for each pair of variables with the largest effects, tell us which interactions are most suggestive:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">day14plt1 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>conditionLabel, <span class="dt">y=</span>stressshift.num), <span class="dt">data =</span> givenness) +<span class="st"> </span>
<span class="st">  </span><span class="kw">stat_summary</span>(<span class="dt">fun.data=</span><span class="st">&quot;mean_cl_boot&quot;</span>, <span class="dt">geom=</span><span class="st">&quot;errorbar&quot;</span>, <span class="dt">width=</span><span class="fl">0.25</span>, <span class="kw">aes</span>(<span class="dt">color=</span>voice)) +
<span class="st">  </span><span class="kw">facet_wrap</span>(~voice) +<span class="st"> </span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;% shifted stress&quot;</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">ylim</span>(<span class="dv">0</span>,<span class="dv">1</span>)

day14plt2 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>voice, <span class="dt">y=</span>stressshift.num), <span class="dt">data =</span> givenness) +<span class="st"> </span>
<span class="st">  </span><span class="kw">stat_summary</span>(<span class="dt">fun.data=</span><span class="st">&quot;mean_cl_boot&quot;</span>, <span class="dt">geom=</span><span class="st">&quot;errorbar&quot;</span>, <span class="dt">width=</span><span class="fl">0.25</span>, <span class="kw">aes</span>(<span class="dt">color=</span>npType)) +
<span class="st">  </span><span class="kw">facet_wrap</span>(~npType) +<span class="st"> </span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;% shifted stress&quot;</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">ylim</span>(<span class="dv">0</span>,<span class="dv">1</span>)

day14plt3 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>conditionLabel, <span class="dt">y=</span>stressshift.num), <span class="dt">data =</span> givenness) +<span class="st"> </span>
<span class="st">  </span><span class="kw">stat_summary</span>(<span class="dt">fun.data=</span><span class="st">&quot;mean_cl_boot&quot;</span>, <span class="dt">geom=</span><span class="st">&quot;errorbar&quot;</span>, <span class="dt">width=</span><span class="fl">0.25</span>, <span class="kw">aes</span>(<span class="dt">color=</span>npType)) +
<span class="st">  </span><span class="kw">facet_wrap</span>(~npType) +<span class="st"> </span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;% shifted stress&quot;</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">ylim</span>(<span class="dv">0</span>,<span class="dv">1</span>)

<span class="kw">grid.arrange</span>(day14plt1, day14plt2, day14plt3, <span class="dt">ncol =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="05-logistic-regression_files/figure-html/unnamed-chunk-29-1.png" width="768" style="display: block; margin: auto;" /></p>
<p>Let’s try adding the <code>voice:conditionLabel</code> interaction, which looks the largest:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod3 &lt;-<span class="st"> </span><span class="kw">glm</span>(stressshift ~<span class="st"> </span>npType.pron +<span class="st"> </span>clabel.williams +<span class="st"> </span>voice.passive +<span class="st"> </span>order.std, 
            <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>, 
            <span class="dt">data =</span> givenness)
mod3<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="kw">glm</span>(stressshift ~<span class="st"> </span>npType.pron +<span class="st"> </span>clabel.williams *<span class="st"> </span>voice.passive +<span class="st"> </span>order.std,
              <span class="dt">data =</span> givenness, 
              <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)
<span class="kw">anova</span>(mod3, mod3<span class="fl">.1</span>, <span class="dt">test =</span> <span class="st">&#39;Chisq&#39;</span>)</code></pre></div>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: stressshift ~ npType.pron + clabel.williams + voice.passive + 
##     order.std
## Model 2: stressshift ~ npType.pron + clabel.williams * voice.passive + 
##     order.std
##   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)   
## 1       377     335.90                        
## 2       376     325.81  1   10.085 0.001495 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>This model comparison suggests we should include the <code>voice:conditionLabel</code> interaction in the model.</p>
<p>Why, pedantically:</p>
<ul>
<li><p><span class="math inline">\(\Delta D = 10.08\)</span></p></li>
<li><p>Probability of <span class="math inline">\(\Delta D\)</span> at least this larger under <span class="math inline">\(\chi^2(1)\)</span> distribution is 0.0015</p></li>
<li><p><span class="math inline">\(\implies\)</span> <strong>reject</strong> <span class="math inline">\(H_0\)</span> that <span class="math inline">\(\beta_{\texttt{voice:conditionLabel}} = 0\)</span></p></li>
<li><p><span class="math inline">\(\implies\)</span> <strong>include</strong> <code>voice:conditionLabel</code> interaction in the model.</p></li>
</ul>
<p>We can carry out similar tests for the other two possible two-way interactions, and get:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod3<span class="fl">.2</span> &lt;-<span class="st"> </span><span class="kw">glm</span>(stressshift ~<span class="st"> </span>npType.pron +<span class="st"> </span>clabel.williams +<span class="st"> </span>voice.passive +<span class="st"> </span>order.std +<span class="st"> </span>npType.pron *<span class="st"> </span>clabel.williams, 
              <span class="dt">data =</span> givenness, 
              <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)
<span class="kw">anova</span>(mod3, mod3<span class="fl">.2</span>, <span class="dt">test=</span><span class="st">&quot;Chisq&quot;</span>)</code></pre></div>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: stressshift ~ npType.pron + clabel.williams + voice.passive + 
##     order.std
## Model 2: stressshift ~ npType.pron + clabel.williams + voice.passive + 
##     order.std + npType.pron * clabel.williams
##   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)
## 1       377     335.90                     
## 2       376     335.31  1   0.5867   0.4437</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod3<span class="fl">.3</span> &lt;-<span class="st"> </span><span class="kw">glm</span>(stressshift ~<span class="st"> </span>npType.pron +<span class="st"> </span>clabel.williams+voice.passive +<span class="st"> </span>order.std +<span class="st"> </span>voice.passive *<span class="st"> </span>npType.pron, 
              <span class="dt">data =</span> givenness, 
              <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)
<span class="kw">anova</span>(mod3, mod3<span class="fl">.3</span>, <span class="dt">test=</span><span class="st">&quot;Chisq&quot;</span>)    </code></pre></div>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: stressshift ~ npType.pron + clabel.williams + voice.passive + 
##     order.std
## Model 2: stressshift ~ npType.pron + clabel.williams + voice.passive + 
##     order.std + voice.passive * npType.pron
##   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)
## 1       377     335.90                     
## 2       376     334.99  1   0.9108   0.3399</code></pre>
<p>Thus, <span class="math inline">\(p&gt;0.3\)</span> for the <code>npType:conditionLabel</code> and <code>voice:npType</code> interactions, so we don’t add these interactions to the model.</p>
<p>The new model is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(mod3<span class="fl">.1</span>)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = stressshift ~ npType.pron + clabel.williams * voice.passive + 
##     order.std, family = &quot;binomial&quot;, data = givenness)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.9933  -0.4987  -0.3543   0.6757   2.6080  
## 
## Coefficients:
##                               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)                    -0.9742     0.1625  -5.993 2.06e-09 ***
## npType.pron                     0.6302     0.2810   2.242  0.02493 *  
## clabel.williams                 3.2054     0.3230   9.923  &lt; 2e-16 ***
## voice.passive                   0.3209     0.3239   0.991  0.32172    
## order.std                       0.3650     0.2876   1.269  0.20437    
## clabel.williams:voice.passive   1.9850     0.6370   3.116  0.00183 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 496.24  on 381  degrees of freedom
## Residual deviance: 325.81  on 376  degrees of freedom
## AIC: 337.81
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>Examining the coefficient values, and comparing to our prior expectations—from EDA and from the study design—there are no coefficients to consider removing by Gelman &amp; Hill’s guidelines. (Make sure you understand the different reasons we don’t drop the <code>order.std</code> or <code>voice.passive</code> main effects.)</p>
<p>The classification accuracy for this new model is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">lrAcc</span>(mod3<span class="fl">.1</span>, <span class="st">&#39;stressshift&#39;</span>)</code></pre></div>
<pre><code>## [1] 0.8062827</code></pre>
<p>Recall that without the <code>voice:conditionLabel</code> interaction, we had the classification accuracy:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">lrAcc</span>(mod3, <span class="st">&#39;stressshift&#39;</span>)</code></pre></div>
<pre><code>## [1] 0.8089005</code></pre>
<p>So accuracy actually <strong>decreases</strong> slightly when this interaction is added. This can happen, because improving model likelihood and improving classification accuracy are not the same thing.<a href="#fn25" class="footnoteRef" id="fnref25"><sup>25</sup></a></p>
</div>
</div>
</div>
<div id="model-criticism-logistic-regression" class="section level2">
<h2><span class="header-section-number">5.4</span> Model criticism for logistic regression</h2>
<p>Regression assumptions and model diagnostics are just as necessary to assess a fitted model (“model criticism”) as for linear regression, where <a href="linear-regression.html#linear-regression-assumptions">we covered these topics</a> in depth. The assumptions and diagnostics differ somewhat for logistic regression, but not at a qualitative level. For example:</p>
<ul>
<li><p>Linearity: The predictors are assumed to be linearly related to <strong>log-odds of <span class="math inline">\(Y=1\)</span></strong> (rather than to <span class="math inline">\(Y\)</span> itself, for linear regression).</p></li>
<li><p>Collinearity: The predictors are assumed to not be linearly dependent, and the presence of collinearity causes similar issues for regression coefficients as for linear regression.</p></li>
<li><p>Residual plots and measures of influence: conceptually very similar for logistic regression, except “residuals” needs to be defined differently because there is no error term (<span class="math inline">\(e_i\)</span>) for logistic regression.</p></li>
</ul>
<p>We will only briefly discuss these topics here, but you should read more (e.g. <span class="citation">Chatterjee &amp; Hadi (<a href="#ref-chatterjee2012regression">2012</a>)</span>, <span class="citation">R. Baayen (<a href="#ref-baayen2008analyzing">2008</a>)</span>, <span class="citation">Gelman &amp; Hill (<a href="#ref-gelman2007data">2007</a>)</span>) if you are using logistic regression to analyze data.</p>
<div id="residual-plots" class="section level3">
<h3><span class="header-section-number">5.4.1</span> Residual plots</h3>
<p>The <strong>raw</strong> residuals for logistic regression would consist of the model prediction (log-odds, or probabilities) minus the observation (0 or 1) for point <span class="math inline">\(i\)</span>. R will give you these residuals if you call <code>resid</code> on a logistic regression model, but any plot using these residuals is not very informative, because the model’s predictions and the observations are conceptually different (as discussed in Sec. <a href="logistic-regression.html#c4differences">5.1.4</a>). For example, here is a fitted-residuals plot for a model we fit above, of the kind used to assess homoscedasticity for linear regression models:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(mod1,<span class="dt">which =</span> <span class="dv">1</span>)</code></pre></div>
<p><img src="05-logistic-regression_files/figure-html/unnamed-chunk-36-1.png" width="672" /></p>
<p>We get a pattern of two lines in plots using raw residuals, corresponding to 0/1 observations. This plot is not very useful for assessing whether there is constant “variance” in some sense throughout the dataset.</p>
<p>There are several alternative residuals that can be defined for logistic regression, including:</p>
<ul>
<li><p>Deviance residuals (use <code>glm.diag()</code> in the <code>boot</code> package)</p></li>
<li><p>Binned residuals (use <code>binned.resids()</code> in the <code>arm</code> package)</p></li>
</ul>
<p>Using either kind of residuals, you can evaluate a logistic regression model using similar diagnostic plots as for linear regression: Q-Q plots, fitted-residual plots, and plots of each predictor versus residuals.</p>
<div id="example-binned-residuals-versus-expected-values" class="section level4 unnumbered">
<h4>Example: Binned residuals versus expected values</h4>
<p><em>Binned residuals</em> are the average (raw) residual over observations within a certain range of expected values (log-odds). That is, you chop the plot immediately above into vertical slices, and average the “Residuals” value within each slice to get a single <span class="math inline">\(y\)</span>-axis number for each slice.</p>
<p>We expect these residuals to be roughly normally distributed with mean 0 and constant variance across different expected values—that is, we expect an expected-value/binned-residual plot to look like the ideal fitted/residual plot for a linear regression.</p>
<p>Consider the final model we ended up with in <a href="logistic-regression.html#log-reg-worked-example">the example above</a>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ex1.mod &lt;-<span class="st"> </span><span class="kw">glm</span>(stressshift ~<span class="st"> </span>npType.pron +<span class="st"> </span>conditionLabel.williams +<span class="st"> </span>voice.passive +<span class="st"> </span>order.std +<span class="st"> </span>conditionLabel.williams:voice.passive, 
               <span class="dt">family=</span><span class="st">&quot;binomial&quot;</span>, 
               <span class="dt">data=</span>givenness)</code></pre></div>
<p>To make a plot of expected values (= fitted values) versus binned residuals:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## binned residual plot
<span class="kw">binnedplot</span>(<span class="kw">predict</span>(ex1.mod), <span class="kw">resid</span>(ex1.mod))</code></pre></div>
<p><img src="05-logistic-regression_files/figure-html/unnamed-chunk-38-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>The diagnostic plot doesn’t suggest any major issue (like a “funnel” shape would), but the observations around expected value = -2.5 merit further investigation.</p>
</div>
</div>
<div id="logistic-regression-cooks-distance" class="section level3">
<h3><span class="header-section-number">5.4.2</span> Cook’s distance</h3>
<p>Cook’s distance (CD) can be defined for each observation in a logistic regression model, similarly <a href="linear-regression.html#lin-reg-measuring-influence">to linear regression</a>. In R, the <code>glm.diag()</code> function in the <code>boot</code> package computes CD. Similarly to linear regression, the distribution of CD values can be plotted and used to identify observations with extreme values.</p>
<p>Here is an example using the multiple logistic regression model considered above:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## add a column showing Cook&#39;s distance values to dataframe
givenness &lt;-<span class="st"> </span><span class="kw">mutate</span>(givenness, <span class="dt">cooksDistance=</span><span class="kw">glm.diag</span>(ex1.mod)$cook)

<span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>cooksDistance), <span class="dt">data=</span>givenness) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>()</code></pre></div>
<p><img src="05-logistic-regression_files/figure-html/unnamed-chunk-39-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>It turns out that the points with highest Cook’s Distance are passive voice observations in contrast condition where stress was shifted:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>cooksDistance), <span class="dt">data=</span>givenness) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="kw">aes</span>(<span class="dt">fill=</span>conditionLabel)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">facet_wrap</span>(~stressshift+voice)</code></pre></div>
<p><img src="05-logistic-regression_files/figure-html/unnamed-chunk-40-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>In fact, all shifted observations in the <em>contrast</em> condition are influential, as well as many non-shifted observations in the <em>williams</em> condition (top-right panel). This is related to the fact that the Williams effect in this data is very strong: basically, any observation that is unexpected—does not show the Williams effect in the <em>williams</em> condition, or does show the effect in the control (<em>contrast</em>) condition—is influential, because it goes against the overwhelming majority of the data.</p>
<p>This example illustrates why it may be preferable to use visual inspection of the distribution of CD values to flag “highly influential” points, rather than a rigid cutoff (<span class="citation">Chatterjee &amp; Hadi (<a href="#ref-chatterjee2012regression">2012</a>)</span>, 4.9.1). One common suggested cutoff is 4/<span class="math inline">\(n\)</span> (<span class="math inline">\(n\)</span> = number of observations), which would be 0.01 for this dataset. This cutoff would flag as influential <em>most</em> data where stress doesn’t shift as expected by the Williams effect. This is correct, in some sense (those points are more influential than others), but not helpful in terms of deciding which observations are “highly influential”—these are the exact observations which let us estimate the size of the Williams effect.</p>
</div>
</div>
<div id="other-readings" class="section level2">
<h2><span class="header-section-number">5.5</span> Other readings</h2>
<p>Some good sources for general audiences covering the same logistic regression topics in more detail:</p>
<ul>
<li><p><span class="citation">Gelman &amp; Hill (<a href="#ref-gelman2007data">2007</a>)</span> Ch. 4.6, 5.1-5.5 (The rest of Ch. 5, on additional topics, is excellent.)</p></li>
<li><p><span class="citation">Chatterjee &amp; Hadi (<a href="#ref-chatterjee2012regression">2012</a>)</span> 12.1-12.5</p></li>
<li><p><span class="citation">Agresti (<a href="#ref-agresti2007">2007</a>)</span> Ch. 4-5</p></li>
</ul>
<p>For language scientists and psychologists specifically, we are not familiar with in-depth treatments. Some shorter ones:</p>
<ul>
<li><p><span class="citation">Johnson (<a href="#ref-johnson2008quantitative">2008</a>)</span> 5.4-5.7 (5.7 is interesting if you are familiar with GoldVarb from sociolinguistics)</p></li>
<li><p><span class="citation">Jaeger (<a href="#ref-jaeger08">2008</a>)</span></p></li>
<li><p><span class="citation">R. Baayen (<a href="#ref-baayen2008analyzing">2008</a>)</span> 6.3.1</p></li>
</ul>
</div>
<div id="c4solns" class="section level2">
<h2><span class="header-section-number">5.6</span> Solutions</h2>
<p><strong>Q</strong>: What are the interpretations of the regression coefficients? * Intercept (<span class="math inline">\(\beta_0\)</span>) * Slope (<span class="math inline">\(\beta_1\)</span>)</p>
<p><strong>A</strong>: <span class="math inline">\(\beta_0\)</span> is the probability of a stress shift when the NP type is ‘Full NP’. <span class="math inline">\(\beta_1\)</span> is the change in probability between when the NP type is ‘Full NP’ or ‘Pronoun’.</p>
</div>
<div id="c4appendix2" class="section level2">
<h2><span class="header-section-number">5.7</span> Appendix: Other Generalized Linear Models</h2>
<p>Logistic regression is the only type of generalized linear model we will cover in this book However, other types of generalized linear model are very useful and cover many other types of data. For example:</p>
<ul>
<li><p><strong>Multinomial regression</strong>: response = multiple discrete outcomes</p>
<ul>
<li><p>Example: Verb <code>regularity</code> data (outcomes: “zijn”, “zijnheb”, “hebben”)</p></li>
<li><p>Multinomial regression models are approximately equivalent to “maximum entropy models” widely used in linguistics (especially phonology)</p></li>
</ul></li>
<li><p><strong>Poisson regression</strong>: response = counts (0, 1, 2, 3, …)</p>
<ul>
<li>Example: Number of words in a lexicon with a given phonotactic shape, or a number of times a word occurs in a corpus (e.g. <span class="citation">R. Baayen (<a href="#ref-baayen2001wfd">2001</a>)</span>)</li>
</ul></li>
<li><p><strong>Linear model log link</strong>: response = positive values only, somewhat right-skewed data.</p>
<ul>
<li><p>Example: Many phonetic parameters (e.g. VOT)</p></li>
<li><p>Apparently this model is often more appropriate for log-normal data than just transforming the response to <span class="math inline">\(\log(Y)\)</span> and modeling that; see <a href="https://stats.stackexchange.com/questions/47840/linear-model-with-log-transformed-response-vs-generalized-linear-model-with-log/48679#48679">here</a>.</p></li>
</ul></li>
</ul>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-agresti2003categorical">
<p>Agresti, A. (2003). <em>Categorical data analysis</em>. Wiley.</p>
</div>
<div id="ref-gelman2007data">
<p>Gelman, A., &amp; Hill, J. (2007). <em>Data analysis using regression and multilevel/hierarchical models</em>. Cambridge: Cambridge University Press.</p>
</div>
<div id="ref-agresti2007">
<p>Agresti, A. (2007). <em>An introduction to categorical data analysis</em>. Wiley.</p>
</div>
<div id="ref-chatterjee2012regression">
<p>Chatterjee, S., &amp; Hadi, A. (2012). <em>Regression Analysis by Example</em> (5th ed.). Hoboken, NJ: John Wiley &amp; Sons.</p>
</div>
<div id="ref-baayen2008analyzing">
<p>Baayen, R. (2008). <em>Analyzing linguistic data</em>. Cambridge: Cambridge University Press.</p>
</div>
<div id="ref-johnson2008quantitative">
<p>Johnson, K. (2008). <em>Quantitative methods in linguistics</em>. Malden, MA: Wiley-Blackwell.</p>
</div>
<div id="ref-jaeger08">
<p>Jaeger, T. (2008). Categorical data analysis: Away from ANOVAs (transformation or not) and towards logit mixed models. <em>Journal of Memory and Language</em>, <em>59</em>(4), 434–446. <a href="https://doi.org/10.1016/j.jml.2007.11.007" class="uri">https://doi.org/10.1016/j.jml.2007.11.007</a></p>
</div>
<div id="ref-baayen2001wfd">
<p>Baayen, R. (2001). <em>Word Frequency Distributions</em>. Kluwer Academic Publishers.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="21">
<li id="fn21"><p>This “large-sample approximation” is what is assumed by <code>glm</code> in R in calculating logistic regression coefficient significances. However the Wald test becomes unreliable for small enough samples (<span class="math inline">\(n &lt; 100\)</span> is sometimes given) or very large <span class="math inline">\(|\beta|\)</span>, and for these reasons <span class="citation">Agresti (<a href="#ref-agresti2003categorical">2003</a>)</span> (5.2.1) recommends always using a likelihood ratio test to assess coefficient significances. The LR and Wald tests will give very similar significances for large samples.<a href="logistic-regression.html#fnref21">↩</a></p></li>
<li id="fn22"><p>There is a further complication here: we have presented logistic regression as modeling data with a binary outcome, that is a <em>Bernoulli</em> random variable <span class="math inline">\(y_i\)</span> for the <span class="math inline">\(i^{\text{th}}\)</span> observation, where we model <span class="math inline">\(P(y_i=1)\)</span>. Why is the option to <code>glm()</code> then <code>family = &quot;binomial&quot;</code>, given that a binomial distribution refers to <span class="math inline">\(n {\ge} 0\)</span> trials with probability <span class="math inline">\(p\)</span> of any outcome being 1? (The Bernoulli case is when <span class="math inline">\(n=1\)</span>.) It turns out that logistic regression can be characterized either as the probability of a binary outcome for each observation, or as modeling the number of “hits” for a Bernoulli distribution over <span class="math inline">\(n\)</span> outcomes, for each set of observations that share the same predictor values. The <code>glm()</code> with <code>family = &quot;binomial&quot;</code> can handle data in either format (<span class="math inline">\(y_i\)</span> binary or giving the number of 0’s and 1’s for a set of predictor values), and if the response variable in the data passed to <code>glm()</code> only has two values, R assumes that you’re modeling a binary outcome.<a href="logistic-regression.html#fnref22">↩</a></p></li>
<li id="fn23"><p><code>glm()</code> uses the <em>Fisher scoring algorithm</em> by default, hence the reference to “Number of Fisher Scoring iterations” in the <code>summary()</code> output of a model fitted with <code>glm()</code>.<a href="logistic-regression.html#fnref23">↩</a></p></li>
<li id="fn24"><p>Dispersion is not a concern for logistic regression models where each observation is just 0 or 1, the only kind of logistic regression covered in this chapter. (As opposed to more complex flavors, like observing proportions between 0 and 1 over multiple observations.)<a href="logistic-regression.html#fnref24">↩</a></p></li>
<li id="fn25"><p>It’s worth noting that classification accuracy is very similar for the two models—close enough to be effectively the same, taking finite sample size into account. For a proportion estimated at <span class="math inline">\(\hat{p} = 0.80\)</span> (like classification accuracy, here) from <span class="math inline">\(n=382\)</span> observations, standard error is <span class="math inline">\(\sqrt{p(1-p)/n} = 0.02\)</span>, so the 95% CIs of classification accuracy of the two models overlap.<a href="logistic-regression.html#fnref25">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="cda.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
