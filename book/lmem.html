<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Quantitative Methods for Linguistic Data</title>
  <meta name="description" content="Quantitative Methods for Linguistic Data">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Quantitative Methods for Linguistic Data" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Quantitative Methods for Linguistic Data" />
  
  
  

<meta name="author" content="Morgan Sonderegger, Michael Wagner, Francisco Torreira">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html">
<link rel="next" href="mixed-effects-logistic-regression.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Quantitative Methods for Linguistic Data</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html"><i class="fa fa-check"></i><b>1</b> Inferential statistics: Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html#sample-population"><i class="fa fa-check"></i><b>1.1</b> Population vs. sample</a><ul>
<li class="chapter" data-level="1.1.1" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html#sample-to-population-high-level"><i class="fa fa-check"></i><b>1.1.1</b> Sample <span class="math inline">\(\to\)</span> population: High level</a></li>
<li class="chapter" data-level="1.1.2" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html#sdsm"><i class="fa fa-check"></i><b>1.1.2</b> Sampling distribution of the sample mean</a></li>
<li class="chapter" data-level="1.1.3" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html#sampling-from-a-non-normal-distribution"><i class="fa fa-check"></i><b>1.1.3</b> Sampling from a non-normal distribution</a></li>
<li class="chapter" data-level="" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html#exercises"><i class="fa fa-check"></i>Exercises</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html#confidence-intervals"><i class="fa fa-check"></i><b>1.2</b> Confidence intervals</a></li>
<li class="chapter" data-level="1.3" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html#t-distribution"><i class="fa fa-check"></i><b>1.3</b> <span class="math inline">\(t\)</span> distribution</a><ul>
<li class="chapter" data-level="1.3.1" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html#t-based-confidence-intervals"><i class="fa fa-check"></i><b>1.3.1</b> <span class="math inline">\(t\)</span>-based confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="inferential-statistics-introduction.html"><a href="inferential-statistics-introduction.html#other-reading"><i class="fa fa-check"></i><b>1.4</b> Other reading</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>2</b> Hypothesis testing</a><ul>
<li class="chapter" data-level="2.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#hypothesis-testing-high-level"><i class="fa fa-check"></i><b>2.1</b> Hypothesis testing: High-level</a></li>
<li class="chapter" data-level="2.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#z-scores"><i class="fa fa-check"></i><b>2.2</b> <span class="math inline">\(z\)</span>-scores</a></li>
<li class="chapter" data-level="2.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#t-tests"><i class="fa fa-check"></i><b>2.3</b> <span class="math inline">\(t\)</span>-tests</a><ul>
<li class="chapter" data-level="2.3.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#single-sample-t-test-setup"><i class="fa fa-check"></i><b>2.3.1</b> Single-sample <span class="math inline">\(t\)</span>-test: Setup</a></li>
<li class="chapter" data-level="2.3.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#hypothesis-testing-in-general"><i class="fa fa-check"></i><b>2.3.2</b> Hypothesis testing in general</a></li>
<li class="chapter" data-level="2.3.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#two-sample-t-test"><i class="fa fa-check"></i><b>2.3.3</b> Two-sample <span class="math inline">\(t\)</span>-test</a></li>
<li class="chapter" data-level="2.3.4" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#welch-example"><i class="fa fa-check"></i><b>2.3.4</b> Unequal variances: Welch <span class="math inline">\(t\)</span>-test</a></li>
<li class="chapter" data-level="2.3.5" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#t-test-assumptions"><i class="fa fa-check"></i><b>2.3.5</b> Assumptions behind <span class="math inline">\(t\)</span>-test</a></li>
<li class="chapter" data-level="2.3.6" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#paired-t-test"><i class="fa fa-check"></i><b>2.3.6</b> Paired <span class="math inline">\(t\)</span>-test</a></li>
<li class="chapter" data-level="2.3.7" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#reporting-a-hypothesis-test"><i class="fa fa-check"></i><b>2.3.7</b> Reporting a hypothesis test</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#checking-normality"><i class="fa fa-check"></i><b>2.4</b> Checking normality</a><ul>
<li class="chapter" data-level="2.4.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#visual-methods"><i class="fa fa-check"></i><b>2.4.1</b> Visual methods</a></li>
<li class="chapter" data-level="2.4.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#q-q-plots"><i class="fa fa-check"></i><b>2.4.2</b> Q-Q plots</a></li>
<li class="chapter" data-level="2.4.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#shapiro-wilk-example"><i class="fa fa-check"></i><b>2.4.3</b> Hypothesis test</a></li>
<li class="chapter" data-level="2.4.4" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#other-parametric-tests"><i class="fa fa-check"></i><b>2.4.4</b> Other parametric tests</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#non-parametric-tests"><i class="fa fa-check"></i><b>2.5</b> Non-parametric tests</a><ul>
<li class="chapter" data-level="2.5.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#wilcoxson-tests"><i class="fa fa-check"></i><b>2.5.1</b> Wilcoxson tests</a></li>
<li class="chapter" data-level="2.5.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#two-sample-wilcoxson-test"><i class="fa fa-check"></i><b>2.5.2</b> Two-sample Wilcoxson test</a></li>
<li class="chapter" data-level="2.5.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#parametric-versus-non-parametric-tests"><i class="fa fa-check"></i><b>2.5.3</b> Parametric versus non-parametric tests</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#other-reading-1"><i class="fa fa-check"></i><b>2.6</b> Other reading</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>3</b> Linear regression</a><ul>
<li class="chapter" data-level="3.1" data-path="linear-regression.html"><a href="linear-regression.html#regression-general-introduction"><i class="fa fa-check"></i><b>3.1</b> Regression: General introduction</a><ul>
<li class="chapter" data-level="3.1.1" data-path="linear-regression.html"><a href="linear-regression.html#linear-models"><i class="fa fa-check"></i><b>3.1.1</b> Linear models</a></li>
<li class="chapter" data-level="3.1.2" data-path="linear-regression.html"><a href="linear-regression.html#terminology"><i class="fa fa-check"></i><b>3.1.2</b> Terminology</a></li>
<li class="chapter" data-level="3.1.3" data-path="linear-regression.html"><a href="linear-regression.html#steps-and-assumptions-of-regression-analysis"><i class="fa fa-check"></i><b>3.1.3</b> Steps and assumptions of regression analysis</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="linear-regression.html"><a href="linear-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>3.2</b> Simple linear regression</a><ul>
<li class="chapter" data-level="3.2.1" data-path="linear-regression.html"><a href="linear-regression.html#slr-continuous-predictor"><i class="fa fa-check"></i><b>3.2.1</b> SLR: Continuous predictor</a></li>
<li class="chapter" data-level="3.2.2" data-path="linear-regression.html"><a href="linear-regression.html#slr-parameter-estimation"><i class="fa fa-check"></i><b>3.2.2</b> SLR: Parameter estimation</a></li>
<li class="chapter" data-level="3.2.3" data-path="linear-regression.html"><a href="linear-regression.html#hypothesis-testing-1"><i class="fa fa-check"></i><b>3.2.3</b> Hypothesis testing</a></li>
<li class="chapter" data-level="3.2.4" data-path="linear-regression.html"><a href="linear-regression.html#quality-of-fit"><i class="fa fa-check"></i><b>3.2.4</b> Quality of fit</a></li>
<li class="chapter" data-level="3.2.5" data-path="linear-regression.html"><a href="linear-regression.html#categorical-predictor"><i class="fa fa-check"></i><b>3.2.5</b> Categorical predictor</a></li>
<li class="chapter" data-level="3.2.6" data-path="linear-regression.html"><a href="linear-regression.html#slr-with-a-binary-categorical-predictor-vs.two-sample-t-test"><i class="fa fa-check"></i><b>3.2.6</b> SLR with a binary categorical predictor vs. two-sample <span class="math inline">\(t\)</span>-test</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="linear-regression.html"><a href="linear-regression.html#multiple-linear-regression"><i class="fa fa-check"></i><b>3.3</b> Multiple linear regression</a><ul>
<li class="chapter" data-level="3.3.1" data-path="linear-regression.html"><a href="linear-regression.html#goodness-of-fit-metrics"><i class="fa fa-check"></i><b>3.3.1</b> Goodness of fit metrics</a></li>
<li class="chapter" data-level="3.3.2" data-path="linear-regression.html"><a href="linear-regression.html#interactions-and-factors"><i class="fa fa-check"></i><b>3.3.2</b> Interactions and factors</a></li>
<li class="chapter" data-level="3.3.3" data-path="linear-regression.html"><a href="linear-regression.html#plotting-interactions"><i class="fa fa-check"></i><b>3.3.3</b> Plotting interactions</a></li>
<li class="chapter" data-level="3.3.4" data-path="linear-regression.html"><a href="linear-regression.html#categorical-factors-with-more-than-two-levels"><i class="fa fa-check"></i><b>3.3.4</b> Categorical factors with more than two levels</a></li>
<li class="chapter" data-level="3.3.5" data-path="linear-regression.html"><a href="linear-regression.html#releveling-factors"><i class="fa fa-check"></i><b>3.3.5</b> Releveling factors</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="linear-regression.html"><a href="linear-regression.html#linear-regression-assumptions"><i class="fa fa-check"></i><b>3.4</b> Linear regression assumptions</a><ul>
<li class="chapter" data-level="3.4.1" data-path="linear-regression.html"><a href="linear-regression.html#visual-methods-1"><i class="fa fa-check"></i><b>3.4.1</b> Visual methods</a></li>
<li class="chapter" data-level="3.4.2" data-path="linear-regression.html"><a href="linear-regression.html#assumption-1-linearity"><i class="fa fa-check"></i><b>3.4.2</b> Assumption 1: Linearity</a></li>
<li class="chapter" data-level="3.4.3" data-path="linear-regression.html"><a href="linear-regression.html#c2ioe"><i class="fa fa-check"></i><b>3.4.3</b> Assumption 2: Independence of errors</a></li>
<li class="chapter" data-level="3.4.4" data-path="linear-regression.html"><a href="linear-regression.html#assumption-3-normality-of-errors"><i class="fa fa-check"></i><b>3.4.4</b> Assumption 3: Normality of errors</a></li>
<li class="chapter" data-level="3.4.5" data-path="linear-regression.html"><a href="linear-regression.html#assumtion-4-constancy-of-variance"><i class="fa fa-check"></i><b>3.4.5</b> Assumtion 4: Constancy of variance</a></li>
<li class="chapter" data-level="3.4.6" data-path="linear-regression.html"><a href="linear-regression.html#interim-summary"><i class="fa fa-check"></i><b>3.4.6</b> Interim summary</a></li>
<li class="chapter" data-level="3.4.7" data-path="linear-regression.html"><a href="linear-regression.html#transforming-to-normality"><i class="fa fa-check"></i><b>3.4.7</b> Transforming to normality</a></li>
<li class="chapter" data-level="3.4.8" data-path="linear-regression.html"><a href="linear-regression.html#assumption-5-linear-independence-of-predictors"><i class="fa fa-check"></i><b>3.4.8</b> Assumption 5: Linear independence of predictors</a></li>
<li class="chapter" data-level="3.4.9" data-path="linear-regression.html"><a href="linear-regression.html#collinearity"><i class="fa fa-check"></i><b>3.4.9</b> Collinearity</a></li>
<li class="chapter" data-level="3.4.10" data-path="linear-regression.html"><a href="linear-regression.html#assumption-6-observations"><i class="fa fa-check"></i><b>3.4.10</b> Assumption 6: Observations</a></li>
<li class="chapter" data-level="3.4.11" data-path="linear-regression.html"><a href="linear-regression.html#lin-reg-measuring-influence"><i class="fa fa-check"></i><b>3.4.11</b> Measuring influence</a></li>
<li class="chapter" data-level="3.4.12" data-path="linear-regression.html"><a href="linear-regression.html#outliers"><i class="fa fa-check"></i><b>3.4.12</b> Outliers</a></li>
<li class="chapter" data-level="3.4.13" data-path="linear-regression.html"><a href="linear-regression.html#regression-assumptions-reassurance"><i class="fa fa-check"></i><b>3.4.13</b> Regression assumptions: Reassurance</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="linear-regression.html"><a href="linear-regression.html#lm-model-comparison"><i class="fa fa-check"></i><b>3.5</b> Model comparison</a><ul>
<li class="chapter" data-level="3.5.1" data-path="linear-regression.html"><a href="linear-regression.html#nested-model-comparison"><i class="fa fa-check"></i><b>3.5.1</b> Nested model comparison</a></li>
<li class="chapter" data-level="3.5.2" data-path="linear-regression.html"><a href="linear-regression.html#non-nested-model-comparison"><i class="fa fa-check"></i><b>3.5.2</b> Non-nested model comparison</a></li>
<li class="chapter" data-level="3.5.3" data-path="linear-regression.html"><a href="linear-regression.html#c2varselect"><i class="fa fa-check"></i><b>3.5.3</b> Variable selection</a></li>
<li class="chapter" data-level="3.5.4" data-path="linear-regression.html"><a href="linear-regression.html#interpretability-issues"><i class="fa fa-check"></i><b>3.5.4</b> Interpretability issues</a></li>
<li class="chapter" data-level="3.5.5" data-path="linear-regression.html"><a href="linear-regression.html#interim-recipe-building-a-multiple-linear-regression-model"><i class="fa fa-check"></i><b>3.5.5</b> Interim recipe: Building a multiple linear regression model</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="linear-regression.html"><a href="linear-regression.html#c2solns"><i class="fa fa-check"></i><b>3.6</b> Solutions</a><ul>
<li class="chapter" data-level="3.6.1" data-path="linear-regression.html"><a href="linear-regression.html#multiple-linear-regression-solutions"><i class="fa fa-check"></i><b>3.6.1</b> Multiple linear regression: Solutions</a></li>
<li class="chapter" data-level="3.6.2" data-path="linear-regression.html"><a href="linear-regression.html#linear-regression-assumptions-solutions"><i class="fa fa-check"></i><b>3.6.2</b> Linear regression assumptions: Solutions</a></li>
<li class="chapter" data-level="3.6.3" data-path="linear-regression.html"><a href="linear-regression.html#model-comparison-solutions"><i class="fa fa-check"></i><b>3.6.3</b> Model comparison: Solutions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="cda.html"><a href="cda.html"><i class="fa fa-check"></i><b>4</b> Categorical data analysis: Preliminaries</a><ul>
<li class="chapter" data-level="4.1" data-path="cda.html"><a href="cda.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a><ul>
<li class="chapter" data-level="4.1.1" data-path="cda.html"><a href="cda.html#x2-contingency-tables"><i class="fa fa-check"></i><b>4.1.1</b> 2x2 contingency tables</a></li>
<li class="chapter" data-level="4.1.2" data-path="cda.html"><a href="cda.html#the-chi-squared-test"><i class="fa fa-check"></i><b>4.1.2</b> The chi-squared test</a></li>
<li class="chapter" data-level="4.1.3" data-path="cda.html"><a href="cda.html#fishers-exact-test"><i class="fa fa-check"></i><b>4.1.3</b> Fisher’s exact test</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="cda.html"><a href="cda.html#towards-logistic-regression"><i class="fa fa-check"></i><b>4.2</b> Towards logistic regression</a><ul>
<li class="chapter" data-level="4.2.1" data-path="cda.html"><a href="cda.html#odds"><i class="fa fa-check"></i><b>4.2.1</b> Odds</a></li>
<li class="chapter" data-level="4.2.2" data-path="cda.html"><a href="cda.html#log-odds"><i class="fa fa-check"></i><b>4.2.2</b> Log-odds</a></li>
<li class="chapter" data-level="4.2.3" data-path="cda.html"><a href="cda.html#odds-ratios"><i class="fa fa-check"></i><b>4.2.3</b> Odds ratios</a></li>
<li class="chapter" data-level="4.2.4" data-path="cda.html"><a href="cda.html#log-odds-sample-and-population"><i class="fa fa-check"></i><b>4.2.4</b> Log odds: sample and population</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="cda.html"><a href="cda.html#cda-other-readings"><i class="fa fa-check"></i><b>4.3</b> Other readings</a></li>
<li class="chapter" data-level="4.4" data-path="cda.html"><a href="cda.html#c3solns"><i class="fa fa-check"></i><b>4.4</b> Solutions</a><ul>
<li class="chapter" data-level="4.4.1" data-path="cda.html"><a href="cda.html#solutions-to-exercise-1"><i class="fa fa-check"></i><b>4.4.1</b> Solutions to Exercise 1:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>5</b> Logistic regression</a><ul>
<li class="chapter" data-level="5.1" data-path="logistic-regression.html"><a href="logistic-regression.html#simple-logistic-regression"><i class="fa fa-check"></i><b>5.1</b> Simple logistic regression</a><ul>
<li class="chapter" data-level="5.1.1" data-path="logistic-regression.html"><a href="logistic-regression.html#log-reg-hyp-test"><i class="fa fa-check"></i><b>5.1.1</b> Hypothesis testing</a></li>
<li class="chapter" data-level="5.1.2" data-path="logistic-regression.html"><a href="logistic-regression.html#interpreting-the-coefficients-logit-odds-and-probability"><i class="fa fa-check"></i><b>5.1.2</b> Interpreting the coefficients: Logit, odds, and probability</a></li>
<li class="chapter" data-level="5.1.3" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-regression-as-a-glm"><i class="fa fa-check"></i><b>5.1.3</b> Logistic regression as a GLM</a></li>
<li class="chapter" data-level="5.1.4" data-path="logistic-regression.html"><a href="logistic-regression.html#c4differences"><i class="fa fa-check"></i><b>5.1.4</b> Differences from linear regression: Fitting and interpretation</a></li>
<li class="chapter" data-level="5.1.5" data-path="logistic-regression.html"><a href="logistic-regression.html#fitting-a-logistic-regression-model"><i class="fa fa-check"></i><b>5.1.5</b> Fitting a logistic regression model</a></li>
<li class="chapter" data-level="5.1.6" data-path="logistic-regression.html"><a href="logistic-regression.html#interpretation"><i class="fa fa-check"></i><b>5.1.6</b> Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="logistic-regression.html"><a href="logistic-regression.html#evaluating-logistic-regression-models"><i class="fa fa-check"></i><b>5.2</b> Evaluating logistic regression models</a><ul>
<li class="chapter" data-level="5.2.1" data-path="logistic-regression.html"><a href="logistic-regression.html#c4lrt"><i class="fa fa-check"></i><b>5.2.1</b> Likelihood ratio test</a></li>
<li class="chapter" data-level="5.2.2" data-path="logistic-regression.html"><a href="logistic-regression.html#classification-accuracy"><i class="fa fa-check"></i><b>5.2.2</b> Classification accuracy</a></li>
<li class="chapter" data-level="5.2.3" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-regression-pseudo-r2"><i class="fa fa-check"></i><b>5.2.3</b> Pseudo-<span class="math inline">\(R^2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="logistic-regression.html"><a href="logistic-regression.html#multiple-logistic-regression"><i class="fa fa-check"></i><b>5.3</b> Multiple logistic regression</a><ul>
<li class="chapter" data-level="5.3.1" data-path="logistic-regression.html"><a href="logistic-regression.html#likelihood-ratio-test-general-case"><i class="fa fa-check"></i><b>5.3.1</b> Likelihood ratio test: General case</a></li>
<li class="chapter" data-level="5.3.2" data-path="logistic-regression.html"><a href="logistic-regression.html#log-reg-worked-example"><i class="fa fa-check"></i><b>5.3.2</b> Worked example</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="logistic-regression.html"><a href="logistic-regression.html#model-criticism-logistic-regression"><i class="fa fa-check"></i><b>5.4</b> Model criticism for logistic regression</a><ul>
<li class="chapter" data-level="5.4.1" data-path="logistic-regression.html"><a href="logistic-regression.html#residual-plots"><i class="fa fa-check"></i><b>5.4.1</b> Residual plots</a></li>
<li class="chapter" data-level="5.4.2" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-regression-cooks-distance"><i class="fa fa-check"></i><b>5.4.2</b> Cook’s distance</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="logistic-regression.html"><a href="logistic-regression.html#other-readings"><i class="fa fa-check"></i><b>5.5</b> Other readings</a></li>
<li class="chapter" data-level="5.6" data-path="logistic-regression.html"><a href="logistic-regression.html#c4solns"><i class="fa fa-check"></i><b>5.6</b> Solutions</a></li>
<li class="chapter" data-level="5.7" data-path="logistic-regression.html"><a href="logistic-regression.html#c4appendix2"><i class="fa fa-check"></i><b>5.7</b> Appendix: Other Generalized Linear Models</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><a href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><i class="fa fa-check"></i><b>6</b> Practical Regression Topics 1: Multi-level factors, contrast coding, interactions</a><ul>
<li class="chapter" data-level="6.1" data-path="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><a href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html#multi-level-factors-introduction"><i class="fa fa-check"></i><b>6.1</b> Multi-level factors: Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><a href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html#contrast-coding"><i class="fa fa-check"></i><b>6.2</b> Contrast coding</a><ul>
<li class="chapter" data-level="6.2.1" data-path="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><a href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html#first-examples"><i class="fa fa-check"></i><b>6.2.1</b> First examples</a></li>
<li class="chapter" data-level="6.2.2" data-path="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><a href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html#basic-interpretation-of-contrasts"><i class="fa fa-check"></i><b>6.2.2</b> Basic interpretation of contrasts</a></li>
<li class="chapter" data-level="6.2.3" data-path="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><a href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html#contrast-coding-schemes"><i class="fa fa-check"></i><b>6.2.3</b> Contrast coding schemes</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><a href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html#c5mlf"><i class="fa fa-check"></i><b>6.3</b> Assessing a multi-level factor’s contribution</a></li>
<li class="chapter" data-level="6.4" data-path="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><a href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html#practice-with-interactions"><i class="fa fa-check"></i><b>6.4</b> Practice with interactions</a></li>
<li class="chapter" data-level="6.5" data-path="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html"><a href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html#c5solns"><i class="fa fa-check"></i><b>6.5</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="lmem.html"><a href="lmem.html"><i class="fa fa-check"></i><b>7</b> Linear mixed models</a><ul>
<li class="chapter" data-level="7.1" data-path="lmem.html"><a href="lmem.html#mixed-effects-models-motivation"><i class="fa fa-check"></i><b>7.1</b> Mixed-effects models: Motivation</a><ul>
<li class="chapter" data-level="7.1.1" data-path="lmem.html"><a href="lmem.html#simpsons-paradox"><i class="fa fa-check"></i><b>7.1.1</b> Simpson’s paradox</a></li>
<li class="chapter" data-level="7.1.2" data-path="lmem.html"><a href="lmem.html#repeated-measure-anovas"><i class="fa fa-check"></i><b>7.1.2</b> Repeated-measure ANOVAs</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="lmem.html"><a href="lmem.html#linear-mixed-models-1-one-grouping-factor-random-intercepts"><i class="fa fa-check"></i><b>7.2</b> Linear mixed models 1: One grouping factor, random intercepts</a><ul>
<li class="chapter" data-level="7.2.1" data-path="lmem.html"><a href="lmem.html#c6model1A"><i class="fa fa-check"></i><b>7.2.1</b> Model 1A: Simple linear regression</a></li>
<li class="chapter" data-level="7.2.2" data-path="lmem.html"><a href="lmem.html#c6model1b"><i class="fa fa-check"></i><b>7.2.2</b> Model 1B: Random intercept only</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="lmem.html"><a href="lmem.html#c6lmm2"><i class="fa fa-check"></i><b>7.3</b> Linear mixed models 2: One grouping factor, random intercepts and slopes</a><ul>
<li class="chapter" data-level="7.3.1" data-path="lmem.html"><a href="lmem.html#c6model1c"><i class="fa fa-check"></i><b>7.3.1</b> Model 1C</a></li>
<li class="chapter" data-level="7.3.2" data-path="lmem.html"><a href="lmem.html#fitting-model-1c"><i class="fa fa-check"></i><b>7.3.2</b> Fitting Model 1C</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="lmem.html"><a href="lmem.html#linear-mixed-models-3-two-grouping-factors"><i class="fa fa-check"></i><b>7.4</b> Linear mixed models 3: Two grouping factors</a><ul>
<li class="chapter" data-level="7.4.1" data-path="lmem.html"><a href="lmem.html#c6model2A"><i class="fa fa-check"></i><b>7.4.1</b> Model 2A: By-participant and by-item random intercepts</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="lmem.html"><a href="lmem.html#evaluating-lmms"><i class="fa fa-check"></i><b>7.5</b> Evaluating LMMs</a><ul>
<li class="chapter" data-level="7.5.1" data-path="lmem.html"><a href="lmem.html#hypothesis-testing-2"><i class="fa fa-check"></i><b>7.5.1</b> Hypothesis testing</a></li>
<li class="chapter" data-level="7.5.2" data-path="lmem.html"><a href="lmem.html#significance-of-a-random-effect-term"><i class="fa fa-check"></i><b>7.5.2</b> Significance of a random effect term</a></li>
<li class="chapter" data-level="7.5.3" data-path="lmem.html"><a href="lmem.html#c6fixedp"><i class="fa fa-check"></i><b>7.5.3</b> Significance of fixed effects</a></li>
<li class="chapter" data-level="7.5.4" data-path="lmem.html"><a href="lmem.html#evaluating-goodness-of-fit"><i class="fa fa-check"></i><b>7.5.4</b> Evaluating goodness of fit</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="lmem.html"><a href="lmem.html#linear-mixed-models-4-multiple-predictors"><i class="fa fa-check"></i><b>7.6</b> Linear mixed models 4: Multiple predictors</a><ul>
<li class="chapter" data-level="7.6.1" data-path="lmem.html"><a href="lmem.html#types-of-predictors"><i class="fa fa-check"></i><b>7.6.1</b> Types of predictors</a></li>
<li class="chapter" data-level="7.6.2" data-path="lmem.html"><a href="lmem.html#c6model3A"><i class="fa fa-check"></i><b>7.6.2</b> Model 3A: Random intercepts only</a></li>
<li class="chapter" data-level="7.6.3" data-path="lmem.html"><a href="lmem.html#c6model3B"><i class="fa fa-check"></i><b>7.6.3</b> Model 3B: Random intercepts and all possible random slopes</a></li>
<li class="chapter" data-level="7.6.4" data-path="lmem.html"><a href="lmem.html#assessing-variability"><i class="fa fa-check"></i><b>7.6.4</b> Assessing variability</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="lmem.html"><a href="lmem.html#more-on-random-slopes"><i class="fa fa-check"></i><b>7.7</b> More on random slopes</a><ul>
<li class="chapter" data-level="7.7.1" data-path="lmem.html"><a href="lmem.html#what-does-adding-a-random-slope-term-do"><i class="fa fa-check"></i><b>7.7.1</b> What does adding a random slope term do?</a></li>
<li class="chapter" data-level="7.7.2" data-path="lmem.html"><a href="lmem.html#adding-a-random-slope"><i class="fa fa-check"></i><b>7.7.2</b> Discussion: Adding a random slope</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="lmem.html"><a href="lmem.html#random-effect-correlations"><i class="fa fa-check"></i><b>7.8</b> Random effect correlations</a><ul>
<li class="chapter" data-level="7.8.1" data-path="lmem.html"><a href="lmem.html#model-1e-correlated-random-slope-intercept"><i class="fa fa-check"></i><b>7.8.1</b> Model 1E: <strong>Correlated</strong> random slope &amp; intercept</a></li>
<li class="chapter" data-level="7.8.2" data-path="lmem.html"><a href="lmem.html#c6discuss"><i class="fa fa-check"></i><b>7.8.2</b> Dicussion: Adding a correlation</a></li>
</ul></li>
<li class="chapter" data-level="7.9" data-path="lmem.html"><a href="lmem.html#model-criticism-for-linear-mixed-models"><i class="fa fa-check"></i><b>7.9</b> Model criticism for linear mixed models</a><ul>
<li class="chapter" data-level="7.9.1" data-path="lmem.html"><a href="lmem.html#model-3b-residual-plots"><i class="fa fa-check"></i><b>7.9.1</b> Model 3B: Residual plots</a></li>
<li class="chapter" data-level="7.9.2" data-path="lmem.html"><a href="lmem.html#model-3b-random-effect-distribution"><i class="fa fa-check"></i><b>7.9.2</b> Model 3B: Random effect distribution</a></li>
</ul></li>
<li class="chapter" data-level="7.10" data-path="lmem.html"><a href="lmem.html#c6factorsissue"><i class="fa fa-check"></i><b>7.10</b> Random slopes for factors</a><ul>
<li class="chapter" data-level="7.10.1" data-path="lmem.html"><a href="lmem.html#model-with-random-effect-correlations"><i class="fa fa-check"></i><b>7.10.1</b> Model with random-effect correlations</a></li>
<li class="chapter" data-level="7.10.2" data-path="lmem.html"><a href="lmem.html#lmem-mwrec"><i class="fa fa-check"></i><b>7.10.2</b> Models without random-effect correlations</a></li>
</ul></li>
<li class="chapter" data-level="7.11" data-path="lmem.html"><a href="lmem.html#other-readings-1"><i class="fa fa-check"></i><b>7.11</b> Other readings</a></li>
<li class="chapter" data-level="7.12" data-path="lmem.html"><a href="lmem.html#c6extraexamples"><i class="fa fa-check"></i><b>7.12</b> Appendix: Extra examples</a><ul>
<li class="chapter" data-level="7.12.1" data-path="lmem.html"><a href="lmem.html#lmm-simulation-confint"><i class="fa fa-check"></i><b>7.12.1</b> Predicting confidence intervals by simulation</a></li>
<li class="chapter" data-level="7.12.2" data-path="lmem.html"><a href="lmem.html#random-intercept-and-slope-model-for-givenness-data"><i class="fa fa-check"></i><b>7.12.2</b> Random intercept and slope model for <code>givenness</code> data</a></li>
</ul></li>
<li class="chapter" data-level="7.13" data-path="lmem.html"><a href="lmem.html#c6extendedexercise"><i class="fa fa-check"></i><b>7.13</b> Appendix: Extended exercise</a></li>
<li class="chapter" data-level="7.14" data-path="lmem.html"><a href="lmem.html#c6solns"><i class="fa fa-check"></i><b>7.14</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html"><i class="fa fa-check"></i><b>8</b> Mixed-effects logistic regression</a><ul>
<li class="chapter" data-level="8.1" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#preliminaries"><i class="fa fa-check"></i><b>8.1</b> Preliminaries</a><ul>
<li class="chapter" data-level="8.1.1" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#motivation"><i class="fa fa-check"></i><b>8.1.1</b> Motivation</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#basics"><i class="fa fa-check"></i><b>8.2</b> Basics</a><ul>
<li class="chapter" data-level="8.2.1" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#c7m1"><i class="fa fa-check"></i><b>8.2.1</b> Model 1: <code>givenness</code> data, crossed random effects (intercepts + slopes)</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#hypothesis-testing-3"><i class="fa fa-check"></i><b>8.3</b> Hypothesis testing</a><ul>
<li class="chapter" data-level="8.3.1" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#fixed-effects"><i class="fa fa-check"></i><b>8.3.1</b> Fixed effects</a></li>
<li class="chapter" data-level="8.3.2" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#random-effects"><i class="fa fa-check"></i><b>8.3.2</b> Random effects</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#fixed-and-random-effects"><i class="fa fa-check"></i><b>8.4</b> Fixed and random effects</a></li>
<li class="chapter" data-level="8.5" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#melr-practice"><i class="fa fa-check"></i><b>8.5</b> MELR Practice</a><ul>
<li class="chapter" data-level="8.5.1" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#c7ex1"><i class="fa fa-check"></i><b>8.5.1</b> Exercise 1: tapping</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#model-criticism-for-mixed-effects-logistic-regression"><i class="fa fa-check"></i><b>8.6</b> Model criticism for mixed-effects logistic regression</a><ul>
<li class="chapter" data-level="8.6.1" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#random-effect-distributions"><i class="fa fa-check"></i><b>8.6.1</b> Random-effect distributions</a></li>
<li class="chapter" data-level="8.6.2" data-path="logistic-regression.html"><a href="logistic-regression.html#residual-plots"><i class="fa fa-check"></i><b>8.6.2</b> Residual plots</a></li>
<li class="chapter" data-level="8.6.3" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#influence"><i class="fa fa-check"></i><b>8.6.3</b> Influence</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#evaluation-measures"><i class="fa fa-check"></i><b>8.7</b> Evaluation measures</a><ul>
<li class="chapter" data-level="8.7.1" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#evaluation-measure-1-likelihood-ratio-test"><i class="fa fa-check"></i><b>8.7.1</b> Evaluation measure 1: Likelihood ratio test</a></li>
<li class="chapter" data-level="8.7.2" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#evaluation-measure-2-classification-accuracy"><i class="fa fa-check"></i><b>8.7.2</b> Evaluation measure 2: Classification accuracy</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#miscellaneous-mixed-effects-regression-topics"><i class="fa fa-check"></i><b>8.8</b> Miscellaneous mixed-effects regression topics</a><ul>
<li class="chapter" data-level="8.8.1" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#c7m2"><i class="fa fa-check"></i><b>8.8.1</b> Random-effect correlation issues</a></li>
</ul></li>
<li class="chapter" data-level="8.9" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#other-readings-2"><i class="fa fa-check"></i><b>8.9</b> Other readings</a></li>
<li class="chapter" data-level="8.10" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#appendices"><i class="fa fa-check"></i><b>8.10</b> Appendices</a><ul>
<li class="chapter" data-level="8.10.1" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#melr-random-slopes-for-factors"><i class="fa fa-check"></i><b>8.10.1</b> Appendix: Random slopes for factors</a></li>
<li class="chapter" data-level="8.10.2" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#c7appendix2"><i class="fa fa-check"></i><b>8.10.2</b> Appendix: Multi-level factors and uncorrelated random effects</a></li>
<li class="chapter" data-level="8.10.3" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#appendix-what-can-happen-if-a-random-slope-isnt-included"><i class="fa fa-check"></i><b>8.10.3</b> Appendix: What can happen if a random slope isn’t included?</a></li>
</ul></li>
<li class="chapter" data-level="8.11" data-path="mixed-effects-logistic-regression.html"><a href="mixed-effects-logistic-regression.html#c7solns"><i class="fa fa-check"></i><b>8.11</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><i class="fa fa-check"></i><b>9</b> Practical regression topics 2: Ordered factors, nonlinear effects, model predictions, post-hoc tests</a><ul>
<li class="chapter" data-level="9.1" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#introduction-1"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#ordered-factors"><i class="fa fa-check"></i><b>9.2</b> Ordered factors</a><ul>
<li class="chapter" data-level="9.2.1" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#orthogonal-polynomial-contrasts"><i class="fa fa-check"></i><b>9.2.1</b> Orthogonal polynomial contrasts</a></li>
<li class="chapter" data-level="9.2.2" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#using-an-ordered-factor-as-a-predictor"><i class="fa fa-check"></i><b>9.2.2</b> Using an ordered factor as a predictor</a></li>
<li class="chapter" data-level="9.2.3" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#further-points"><i class="fa fa-check"></i><b>9.2.3</b> Further points</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#nonlinear-effects"><i class="fa fa-check"></i><b>9.3</b> Nonlinear effects</a><ul>
<li class="chapter" data-level="9.3.1" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#splines-definition-and-benefits"><i class="fa fa-check"></i><b>9.3.1</b> Splines: Definition and benefits</a></li>
<li class="chapter" data-level="9.3.2" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#restricted-cubic-splines"><i class="fa fa-check"></i><b>9.3.2</b> Restricted cubic splines</a></li>
<li class="chapter" data-level="9.3.3" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#choosing-spline-complexity"><i class="fa fa-check"></i><b>9.3.3</b> Choosing spline complexity</a></li>
<li class="chapter" data-level="9.3.4" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#rcs-components"><i class="fa fa-check"></i><b>9.3.4</b> RCS components</a></li>
<li class="chapter" data-level="9.3.5" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#using-rcs-in-a-mixed-model"><i class="fa fa-check"></i><b>9.3.5</b> Using RCS in a mixed model</a></li>
<li class="chapter" data-level="9.3.6" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#random-slopes-for-rcs-terms"><i class="fa fa-check"></i><b>9.3.6</b> Random slopes for RCS terms</a></li>
<li class="chapter" data-level="9.3.7" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#nonlinear-effects-summary"><i class="fa fa-check"></i><b>9.3.7</b> Nonlinear effects: Summary</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#predictions-from-mixed-models"><i class="fa fa-check"></i><b>9.4</b> Predictions from mixed models</a><ul>
<li class="chapter" data-level="9.4.1" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#making-model-predictions"><i class="fa fa-check"></i><b>9.4.1</b> Making Model Predictions</a></li>
<li class="chapter" data-level="9.4.2" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#simulation-based-predictions"><i class="fa fa-check"></i><b>9.4.2</b> Simulation-based predictions</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#post-hoc-mult-comp"><i class="fa fa-check"></i><b>9.5</b> Post-hoc tests and multiple comparisons</a></li>
<li class="chapter" data-level="9.6" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#c8indivpreds"><i class="fa fa-check"></i><b>9.6</b> Appendix: Model predictions for indiviudal participants</a><ul>
<li class="chapter" data-level="9.6.1" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#predictions-incorporating-offsets-for-individual-speakers"><i class="fa fa-check"></i><b>9.6.1</b> Predictions incorporating offsets for individual speakers</a></li>
<li class="chapter" data-level="9.6.2" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#predicted-williams-effect-for-each-speaker"><i class="fa fa-check"></i><b>9.6.2</b> Predicted Williams effect for each speaker</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#c8slopesForFactors"><i class="fa fa-check"></i><b>9.7</b> Appendix: Random slopes for factors</a></li>
<li class="chapter" data-level="9.8" data-path="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html"><a href="practical-regression-topics-2-ordered-factors-nonlinear-effects-model-predictions-post-hoc-tests.html#c8solns"><i class="fa fa-check"></i><b>9.8</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="datasets-appendix.html"><a href="datasets-appendix.html"><i class="fa fa-check"></i><b>10</b> Appendix: Datasets and packages</a><ul>
<li class="chapter" data-level="10.1" data-path="datasets-appendix.html"><a href="datasets-appendix.html#engdata"><i class="fa fa-check"></i><b>10.1</b> <code>english</code> lexical decision and naming latencies</a></li>
<li class="chapter" data-level="10.2" data-path="datasets-appendix.html"><a href="datasets-appendix.html#dutch-regularity"><i class="fa fa-check"></i><b>10.2</b> Dutch <code id="dregdata">regularity</code></a></li>
<li class="chapter" data-level="10.3" data-path="datasets-appendix.html"><a href="datasets-appendix.html#european-french-phrase-medial-vowel-devoicing"><i class="fa fa-check"></i><b>10.3</b> European French phrase-medial vowel <code id="devdata">devoicing</code></a><ul>
<li class="chapter" data-level="10.3.1" data-path="datasets-appendix.html"><a href="datasets-appendix.html#background"><i class="fa fa-check"></i><b>10.3.1</b> Background</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="datasets-appendix.html"><a href="datasets-appendix.html#north-american-english-tapping"><i class="fa fa-check"></i><b>10.4</b> North American English <code id="tapdata">tapping</code></a><ul>
<li class="chapter" data-level="10.4.1" data-path="datasets-appendix.html"><a href="datasets-appendix.html#background-1"><i class="fa fa-check"></i><b>10.4.1</b> Background</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="datasets-appendix.html"><a href="datasets-appendix.html#halfdata"><i class="fa fa-check"></i><b>10.5</b> <code>halfrhyme</code>: English half-rhymes</a></li>
<li class="chapter" data-level="10.6" data-path="datasets-appendix.html"><a href="datasets-appendix.html#givedata"><i class="fa fa-check"></i><b>10.6</b> <code>givenness</code> data: the Williams Effect</a><ul>
<li class="chapter" data-level="10.6.1" data-path="datasets-appendix.html"><a href="datasets-appendix.html#background-2"><i class="fa fa-check"></i><b>10.6.1</b> Background</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="datasets-appendix.html"><a href="datasets-appendix.html#alternatives"><i class="fa fa-check"></i><b>10.7</b> <code id="altdata">alternatives</code></a><ul>
<li class="chapter" data-level="10.7.1" data-path="datasets-appendix.html"><a href="datasets-appendix.html#background-3"><i class="fa fa-check"></i><b>10.7.1</b> Background</a></li>
</ul></li>
<li class="chapter" data-level="10.8" data-path="datasets-appendix.html"><a href="datasets-appendix.html#votdata"><i class="fa fa-check"></i><b>10.8</b> VOT</a><ul>
<li class="chapter" data-level="10.8.1" data-path="datasets-appendix.html"><a href="datasets-appendix.html#background-4"><i class="fa fa-check"></i><b>10.8.1</b> Background</a></li>
</ul></li>
<li class="chapter" data-level="10.9" data-path="datasets-appendix.html"><a href="datasets-appendix.html#transitionsdata"><i class="fa fa-check"></i><b>10.9</b> Transitions</a></li>
<li class="chapter" data-level="10.10" data-path="datasets-appendix.html"><a href="datasets-appendix.html#packages"><i class="fa fa-check"></i><b>10.10</b> Packages</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Quantitative Methods for Linguistic Data</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lmem" class="section level1">
<h1><span class="header-section-number">Chapter 7</span> Linear mixed models</h1>
<p><strong>Preliminary code</strong></p>
<p>This code is needed to make other code below work:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ggplot2)
<span class="kw">library</span>(dplyr)
<span class="kw">library</span>(arm)
<span class="kw">library</span>(languageR)
<span class="kw">library</span>(Hmisc)


## loads givennessMcGillLing620.csv from OSF project for Wagner (2012) data
givenness &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="kw">url</span>(<span class="st">&quot;https://osf.io/q9e3a/download&quot;</span>))

## define numeric versions of factors, for convenience
givenness &lt;-<span class="st"> </span><span class="kw">mutate</span>(givenness,
                    <span class="dt">conditionLabel.williams =</span> <span class="kw">rescale</span>(conditionLabel),
                    <span class="dt">npType.pronoun =</span> <span class="kw">rescale</span>(npType),
                    <span class="dt">npType.pron =</span> <span class="kw">rescale</span>(npType),
                    <span class="dt">voice.passive =</span> <span class="kw">rescale</span>(voice),
                    <span class="dt">order.std =</span> <span class="kw">rescale</span>(order),
                    <span class="dt">stressshift.num =</span> (<span class="kw">as.numeric</span>(stressshift) -<span class="st"> </span><span class="dv">1</span>))
                    

## make non-mixed-effect model prediction for examples below (just one prediction per
## level of conditionLabel)
newdata &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">conditionLabel.williams=</span><span class="kw">sort</span>(<span class="kw">unique</span>(givenness$conditionLabel.williams)))

mod1a &lt;-<span class="st"> </span><span class="kw">lm</span>(acoustics ~<span class="st"> </span>conditionLabel.williams, <span class="dt">data=</span>givenness)

newdata$pred &lt;-<span class="st"> </span><span class="kw">predict</span>(mod1a, <span class="dt">newdata=</span>newdata)

newdata$conditionLabel &lt;-<span class="st"> </span><span class="kw">factor</span>(<span class="kw">levels</span>(givenness$conditionLabel))


## loads halfrhymeMcGillLing620.csv from OSF project for Harder (2013) data
halfrhyme &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="kw">url</span>(<span class="st">&quot;https://osf.io/37uqt/download&quot;</span>))

## need to do this because the relDuration variable is only defined when conditionLabel is &#39;voice&#39;
halfrhyme &lt;-<span class="st">  </span><span class="kw">subset</span>(halfrhyme, conditionLabel==<span class="st">&#39;voice&#39;</span>)</code></pre></div>
<script src="js/hideOutput.js"></script>
<p><strong>Note</strong>: Answers to some questions/exercises not listed in text are in <a href="lmem.html#c6solns">Solutions</a></p>
<div id="mixed-effects-models-motivation" class="section level2">
<h2><span class="header-section-number">7.1</span> Mixed-effects models: Motivation</h2>
<p>Data analysis can be split into two parts: exploratory (EDA), and confirmatory (CDA).<a href="#fn29" class="footnoteRef" id="fnref29"><sup>29</sup></a> <!-- TODO FUTURE: add link to EDA/CDA discussion when done --> In this book, EDA always accompanies CDA. But the ultimate goal of a study of linguistic data is usually <strong>confirmatory data analysis</strong>: we want to make generalizations about units drawn from a population, based on finite data. As such, we want our statistical analysis techniques—such as regression models—to generalize to new data, not just describe the sample we happened to draw.</p>
<p>A key assumption of regression models, which <a href="linear-regression.html#c2ioe">we discussed</a> in the context of linear regression, is <em>independence of errors</em>. When we fit a regression model of <span class="math inline">\(Y\)</span> as a function of some predictors, it is assumed that the “errors” for each observation—how off the model’s prediction is, after taking predictor values into account—are independent. That is, knowing how off the model is for observation 1 doesn’t tell us anything about how off it is for observation 10, and so on.</p>
<p>However, at least for linguistic data, usually the <strong>unit</strong> over which we want to generalize is not observations; it is some higher-level grouping, such as participants, “items”, sentences, and so on. We usually take more than one observation per unit level <span class="citation">(R. Baayen, Davidson, &amp; Bates, <a href="#ref-baayen2008mixed">2008</a>)</span>:</p>
<ul>
<li><p>Multiple observations per participant (since we’ve already paid them to do the experiment)</p></li>
<li><p>Multiple observations per item (because designing items is time-consuming)</p></li>
</ul>
<p>and so on.</p>
<p>But in general, observations from within a unit level do not have independent errors! For example, in a study of lexical decision reaction time as a function of participant age and word frequency (such as the <code>[english</code> dataset](#engdata)), certain participants will be characteristically fast (low RT), compared to other participants of the same age—perhaps because they just had coffee, are highly motivated to finish the experiment, or some other reason. Similarly, errors won’t be independent from multiple observations of the same word. Certain words will just take longer to recognize, beyond the effect of word frequency—such as (orthographically) longer words, or words less familiar to an undergraduate participant population.</p>
<p>Grouping by one or more units, or <em>grouping factors</em>, is the norm in linguistic data—as well as in most data from behavioral sciences.</p>
<p>We discussed many different assumptions made by regression models in the <a href="linear-regression.html#linear-regression-assumptions">linear regression chapter</a>, but the independence-of-errors assumption is particularly crucial. Let’s see a few examples of what can happen when grouping by unit is not taken into account.</p>
<div id="simpsons-paradox" class="section level3">
<h3><span class="header-section-number">7.1.1</span> Simpson’s paradox</h3>
<center>
<img src="images/simpsons-paradox.png" alt="Source: https://hlplab.wordpress.com/2011/05/31/mixed-models-and-simpsons-paradox/" />
</center>
<p>Source: <a href="https://hlplab.wordpress.com/2011/05/31/mixed-models-and-simpsons-paradox/" class="uri">https://hlplab.wordpress.com/2011/05/31/mixed-models-and-simpsons-paradox/</a></p>
<p>This data is from a study examining the relationship between a language’s geographic distance from a fixed point (<span class="math inline">\(X\)</span>) and its phonological diversity (<span class="math inline">\(Y\)</span>). Languages are grouped into families (the “grouping factor”), because related languages will have very similar values of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, for reasons not of interest here. Of interest is how <span class="math inline">\(X\)</span> affects <span class="math inline">\(Y\)</span>—after accounting for family differences.</p>
<p>Without taking grouping into account, the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> would look negative. However, once grouping is taken into account, we can see that the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is actually positive, within language families. (In addition, there is a negative relationship between a family’s mean <span class="math inline">\(X\)</span> and mean <span class="math inline">\(Y\)</span>.)</p>
<p>That is: we would make the <strong>opposite conclusion</strong> if grouping information were not taken into account. This situation, where a trend is reversed or disappears within groups compared to across groups, is called <em>Simpson’s paradox</em>. A famous example related to graduate admissions at UC Berkeley is described <a href="http://vudlab.com/simpsons/">here</a>, with useful visualizations.</p>
<p>Simpson’s paradox is an extreme example, where the actual effect direction is flipped. More commonly, not taking grouping into account can result in Type I errors (spurious effects) or Type II errors (missing an effect that is really there):</p>
<!-- TODO FUTURE: actual examples (simulated) -->
<ul>
<li><p>Type I error: A couple items drive an otherwise null effect</p></li>
<li><p>Type II error: Consistent effect within items, but not in pooled data</p></li>
</ul>
<p>In a future version of these notes there will be actual illustrative examples, but for now one can see these possibilities by mentally adjusting the Simpson’s paradox example shown above:</p>
<ul>
<li><p>Type I: Keep all green points where they are; move points of all other colors so they have y-axis value of 0.5</p></li>
<li><p>Type II: Move each cluster of points vertically so it’s centered at y-axis = 0.5.</p></li>
</ul>
<p>In technical terms, Type I errors can result from non-independence of errors because the <em>degrees of freedom</em> assumed by the model are wrong. Intuitively, <span class="math inline">\(n\)</span> different observations from the same item should not count as <span class="math inline">\(n\)</span> independent pieces of information about how a property of items (like word frequency) affects the response—as <span class="math inline">\(n\)</span> different observations all from different items would count.</p>
</div>
<div id="repeated-measure-anovas" class="section level3">
<h3><span class="header-section-number">7.1.2</span> Repeated-measure ANOVAs</h3>
<p>One technique for analyzing grouped data is <em>repeated measure ANOVAs</em> (RM-ANOVA). We will not discuss how to do RM-ANOVA’s, but they are widely used.<a href="#fn30" class="footnoteRef" id="fnref30"><sup>30</sup></a> For experimental linguistics in particular, especially psycholinguistics, RM-ANOVAs were the norm for statistical analysis from the mid 1970s to the early 2010s. Thus, some context is helpful.</p>
<p>An RM-ANOVA analysis assumes (relatively) equal sample size in different levels of the same grouping factor (e.g. same number of observations per participant), and that there is a single grouping factor (e.g. participants <strong>or</strong> items). The most common case for linguistic data is to have two or more grouping factors, such as participant and item. The analyst carries out one RM-ANOVA per grouping factor: separate “by-participant” and “by-item” RM-ANOVAs.<a href="#fn31" class="footnoteRef" id="fnref31"><sup>31</sup></a> Each RM-ANOVA results in an <span class="math inline">\(F\)</span> statistic value, often called <em>F1</em> (“by-participant”) and <em>F2</em> (“by-item”). F1 and F2 are combined into a single statistic, <em>minF</em>. In practice, the by-participant and by-item RM-ANOVAs are often computed and interpreted without considering minF, as in the example below.</p>
<p>Here is a sample RM-ANOVA analysis report (from <span class="citation">Salverda, Dahan, &amp; McQueen (<a href="#ref-salverda2003role">2003</a>)</span>):</p>
<center>
<img src="images/rm_anova_report.png" />
</center>
<p>Note that the report discusses significance “by participant” versus “by item”, referring to the F1 and F2 RM-ANOVA results for the same effect.</p>
<p>RM-ANOVA is a perfectly fine methodology for analyzing grouped data, especially for relatively simple experiments with balanced designs. Statistical power will be lower than for an equivalent mixed model, but ideally power should be high anyway in a simple experiment. Some discussion of RM-ANOVAs, including contextualization with respect to mixed models, is given by <span class="citation">Barr, Levy, Scheepers, &amp; Tily (<a href="#ref-barr2013random">2013</a>)</span>; <span class="citation">R. Baayen (<a href="#ref-baayen2008analyzing">2008</a>)</span>, Sec. 7.2.1; <span class="citation">R. Baayen et al. (<a href="#ref-baayen2008mixed">2008</a>)</span>.<a href="#fn32" class="footnoteRef" id="fnref32"><sup>32</sup></a></p>
<p>Mixed-effects regression models, or <em>mixed models</em>, are another way of analyzing grouped data. Mixed models are much more powerful than RM-ANOVA, and have become the standard for analyzing grouped data in many areas of language sciences, as well as other fields. The advantages of mixed models over RM-ANOVAs for analyzing grouped data include:</p>
<ul>
<li><p>More than one grouping unit can be included in the same model</p></li>
<li><p>Unequal number of observations per level is OK</p></li>
<li><p><strong>Explicitly model variability</strong> among levels of the same grouping factor</p>
<ul>
<li><p>Ex: By participant variability (“individual differences”)</p></li>
<li><p>This means the analyst can explore and test questions about the variability itself (as opposed to effects averaging across participants, etc.), which is not possible using RM-ANOVA, or other methods we’ve covered so far.</p></li>
</ul></li>
</ul>
</div>
</div>
<div id="linear-mixed-models-1-one-grouping-factor-random-intercepts" class="section level2">
<h2><span class="header-section-number">7.2</span> Linear mixed models 1: One grouping factor, random intercepts</h2>
<p>We will introduce each part of mixed models through a series of examples, using just one predictor (<span class="math inline">\(X\)</span>) to keep things simple.</p>
<p>We start with the simplest case of mixed models—where there is variability between groups just in the value of the intercept. This first example uses the <a href="datasets-appendix.html#givedata"><code>givenness</code> dataset</a>, taking into account by-participant variability (grouping factor = <code>participant</code>).</p>
<p>First, some notation:</p>
<ul>
<li><p><span class="math inline">\(n\)</span>: number of observations.</p></li>
<li><p>The grouping factor is <code>participant</code>.</p>
<ul>
<li><p>There are <span class="math inline">\(J\)</span> levels/groups (i.e., <span class="math inline">\(J\)</span> participants)</p></li>
<li><p>Observation <span class="math inline">\(i\)</span> is in group <span class="math inline">\(j[i]\)</span> (<span class="math inline">\(j[i] \in \{1, \ldots, J\}\)</span>)</p></li>
</ul></li>
<li><p>The response is <code>acoustics</code> (<span class="math inline">\(Y\)</span>)</p>
<ul>
<li><span class="math inline">\(y_i\)</span>: value for the <span class="math inline">\(i^{\text{th}}\)</span> observation</li>
</ul></li>
<li><p>There is a single predictor,<code>conditionLabel.williams</code> (<span class="math inline">\(X\)</span>)</p>
<ul>
<li><p><span class="math inline">\(x_i\)</span>: value for the <span class="math inline">\(i^{\text{th}}\)</span> observation</p></li>
<li><p>0.5 = <em>Williams</em>, -0.5 = <em>Contrast</em></p></li>
</ul></li>
</ul>
<p>(Italics are used to refer to levels of a factor, like <em>Williams</em> for the <code>conditionLabel</code> factor.)</p>
<p>We first consider a simple linear regression model <em>without</em> by-participant variability (<a href="lmem.html#c6model1A">Model 1A</a>), then introduce the first mixed model (<a href="#c6model1B">Model 1B</a>).</p>
<div id="c6model1A" class="section level3">
<h3><span class="header-section-number">7.2.1</span> Model 1A: Simple linear regression</h3>
<p>The model for simple linear regression is:</p>
<p><span class="math display">\[
y_i = \beta_0 + \beta_1 x_i + \epsilon_i, \quad i = 1, 2,..., n
\]</span></p>
<p>The errors are assumed to be normally distributed (and independent): <span class="math display">\[
\epsilon_i \sim N(0, \sigma^2)
\]</span></p>
<p>This model assumes the same slope and intercept for <strong>all groups</strong> (i.e., all participants), as schematized here: <img src="07-linear-mixed-models_files/figure-html/unnamed-chunk-2-1.png" width="480" style="display: block; margin: auto;" /> The same <code>acoustics</code> value is predicted for all observations in each condition, regardless of which participant an observation comes from.</p>
<p>To fit and summarize this model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod1a &lt;-<span class="st"> </span><span class="kw">lm</span>(acoustics ~<span class="st"> </span>conditionLabel.williams, <span class="dt">data =</span> givenness)
<span class="kw">summary</span>(mod1a)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = acoustics ~ conditionLabel.williams, data = givenness)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.31843 -0.56264  0.01977  0.53651  2.50851 
## 
## Coefficients:
##                         Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)             -0.72205    0.04109 -17.570  &lt; 2e-16 ***
## conditionLabel.williams  0.31774    0.08224   3.863 0.000131 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.8032 on 380 degrees of freedom
## Multiple R-squared:  0.03779,    Adjusted R-squared:  0.03526 
## F-statistic: 14.93 on 1 and 380 DF,  p-value: 0.0001315</code></pre>
<p>Since the data is grouped by participant (and item), this model violates the <strong>independent errors assumption</strong> of linear regression. Participants might differ in:</p>
<ol style="list-style-type: decimal">
<li><p>The mean value of <code>acoustics</code></p></li>
<li><p>The effect of <code>conditionLabel.williams</code></p></li>
</ol>
<p>Our goal in fitting the model is to assess whether there’s an effect of <code>conditionLabel.williams</code>, beyond individual differences: <span class="math display">\[
H_0\,:\,\beta_1 = 0
\]</span></p>
<p>Either type of differences between participants could lead to falsely rejecting or accepting <span class="math inline">\(H_0\)</span>.</p>
<p>Mixed models deal with non-independence of errors by using two types of coefficients:</p>
<ol style="list-style-type: decimal">
<li><p><em>Random effects</em>: Coefficients which vary between groups</p></li>
<li><p><em>Fixed effects</em>: Coefficients which don’t vary between groups</p></li>
</ol>
<p>Fixed effects are basically what we have referred to as “regression coefficients” in the linear regression and logistic regression models covered in previous chapters.</p>
<p>The next two models we consider have two different kinds of random effects:</p>
<ul>
<li><p><a href="lmem.html#c6model1b">Model 1B</a>: Intercept varies between participants</p></li>
<li><p><a href="lmem.html#c6model1c">Model 1C</a>: Intercept and slope vary between participants</p></li>
</ul>
</div>
<div id="c6model1b" class="section level3">
<h3><span class="header-section-number">7.2.2</span> Model 1B: Random intercept only</h3>
<p>This model makes predictions as in Fig. <a href="lmem.html#fig:mod1bFig1">7.1</a>: one line per participant, differing in intercept but not in slope.</p>
<p>The regression model for a simple linear regression with a <em>random intercept</em> is:</p>
<p><span class="math display">\[
y_i = \beta_0 + \alpha_{j[i]} + \beta_1 x_i + \epsilon_i, \quad i = 1, 2, ..., n
\]</span> <span class="math inline">\(\alpha_{j[i]}\)</span> is the random intercept, which captures the offset of group <span class="math inline">\(j[i]\)</span> from the grand mean (<span class="math inline">\(\beta_0\)</span>). The random intercept values for different participants are assumed to be normally distributed: <span class="math display">\[ 
\alpha_j \sim N(0, \sigma^2_s), \quad j = 1, 2, ..., J
\]</span> where <span class="math inline">\(\sigma_s\)</span> captures the amount of between-participant variation in the intercept value.</p>
<p>The errors <span class="math inline">\(\epsilon_i\)</span> are still assumed to be normally distributed, as in a non-mixed-effects linear regression: <span class="math display">\[ 
\epsilon_i \sim N(0, \sigma^2_e), \quad j = 1, 2, ..., n
\]</span> where we now write the error variance as <span class="math inline">\(\sigma_e\)</span> (instead of <span class="math inline">\(\sigma\)</span>), to distinguish it from <span class="math inline">\(\sigma_s\)</span>.</p>
<p>This regression equation implies different models of how <span class="math inline">\(Y\)</span> depends on <span class="math inline">\(X\)</span>, for different participants. For example, for participant #5:</p>
<p><span class="math display">\[
Y = \beta_0 + \alpha_5 + \beta_1 X+ \epsilon
\]</span> where <span class="math inline">\((\beta_0 + \alpha_5)\)</span> is the value of the intercept for participant 5.</p>
<p>The model “across participants”, or “for an average participant”, is: <span class="math display">\[
Y = \beta_0 + \beta_1 X + \epsilon
\]</span> Here, the intercept is <span class="math inline">\(\beta_0\)</span>: the overall intercept, averaged across participants. This model is identical to Model 1A—simple logistic regression, without random effects.</p>
<p>What is the same in both models is the slope of <span class="math inline">\(X\)</span> (and the variance of the error term).</p>
<p>Fitting Model 1B means estimating:</p>
<ol style="list-style-type: decimal">
<li><p>The <em>fixed effect coefficients</em>:</p>
<ul>
<li><p><span class="math inline">\(\beta_0\)</span>: grand mean</p></li>
<li><p><span class="math inline">\(\beta_1\)</span>: slope of <code>conditionLabel.williams</code></p></li>
</ul></li>
<li><p>The <em>variance components</em>:</p>
<ul>
<li><p><span class="math inline">\(\sigma^2_s\)</span>: variability of intercept offset across participants</p></li>
<li><p><span class="math inline">\(\sigma^2_e\)</span>: residual error variance</p></li>
</ul></li>
</ol>
<div id="detour-fitting-linear-mixed-effects-models" class="section level4 unnumbered">
<h4>Detour: Fitting linear mixed-effects models</h4>
<p>We will use the <code>lme4</code> package for fitting (<strong>l</strong>inear) <strong>m</strong>ixed <strong>e</strong>ffects models, which is the most widely-used R package for such models. <code>lme4</code> is both widely used and frequently updated. (Because of the latter, it’s important to cite the exact version of <code>lme4</code> used when reporting a mixed-effects model in a paper. The version of lme4 used to fit models in this chapter is 1.1.18.1.) See <span class="citation">D. Bates, Mächler, Bolker, &amp; Walker (<a href="#ref-bates2014fitting">2014</a>)</span> for the mathematical details of fitting these models.</p>
<p>Functions in this package fit mixed models by maximizing either:</p>
<ul>
<li><p><em>maximum likelihood</em> (<em>ML</em>), which gives biased variance component estimates, or</p></li>
<li><p><em>restricted ML</em> (<em>REML</em>), which gives unbiased variance component estimates, and is typically the default</p></li>
</ul>
<p>The difference between ML and REML only matters for small sample sizes. Technically you must use ML fits for model comparison to make sense, and if you try to compare two <code>lme4</code> models using <code>anova()</code> in R, the models will automatically be re-fit using ML before comparing (as of 2018).<a href="#fn33" class="footnoteRef" id="fnref33"><sup>33</sup></a></p>
</div>
<div id="fitting-model-1b" class="section level4">
<h4><span class="header-section-number">7.2.2.1</span> Fitting Model 1B</h4>
<p>We fit the mixed model described above using the <code>lmer</code> function (from the <code>lme4</code> package):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod1b &lt;-<span class="st"> </span>lme4::<span class="kw">lmer</span>(acoustics ~<span class="st"> </span>conditionLabel.williams +<span class="st"> </span>(<span class="dv">1</span>|participant), <span class="dt">data =</span> givenness)</code></pre></div>
<p>where <code>(1|participant)</code> is <code>lme4</code> notation for a by-participant random intercept (<code>1</code> means “intercept”, <code>|participant</code> means “grouped by participant”).</p>
<p><strong>Note</strong>: You do <strong>not</strong> need to write <code>lme4::lmer</code> in general, just <code>lmer</code>. The notation <code>lme4::lmer</code> is used here to make sure we use the <code>lmer</code> function from the <code>lme4</code> package, rather than the redefined version of <code>lmer</code> from the <code>lmerTest</code> package discussed in Sec. <a href="lmem.html#c6sattapprox">7.5.3.3</a>.</p>
<p>The model’s output is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(mod1b)</code></pre></div>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: acoustics ~ conditionLabel.williams + (1 | participant)
##    Data: givenness
## 
## REML criterion at convergence: 897.9
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -3.0538 -0.7129  0.0083  0.6540  3.3136 
## 
## Random effects:
##  Groups      Name        Variance Std.Dev.
##  participant (Intercept) 0.08937  0.299   
##  Residual                0.55800  0.747   
## Number of obs: 382, groups:  participant, 27
## 
## Fixed effects:
##                         Estimate Std. Error t value
## (Intercept)             -0.71856    0.06916  -10.39
## conditionLabel.williams  0.32626    0.07677    4.25
## 
## Correlation of Fixed Effects:
##             (Intr)
## cndtnLbl.wl 0.002</code></pre>
<p>Under <code>Random effects</code>:</p>
<ul>
<li><p><span class="math inline">\(\hat{\sigma}^2_s = 0.089\)</span> : estimated <strong>by-participant variability</strong> in the intercept.</p></li>
<li><p><span class="math inline">\(\hat{\sigma}^2_e = 0.558\)</span>: estimated <strong>residual error</strong> variance</p></li>
</ul>
<p>Under <code>Fixed effects</code>: estimates of the fixed-effect coefficients:</p>
<ul>
<li><p><span class="math inline">\(\hat{\beta}_0 = -0.71\)</span>: intercept</p></li>
<li><p><span class="math inline">\(\hat{\beta}_1 = 0.326\)</span>: slope</p></li>
</ul>
</div>
<div id="interpretation-1" class="section level4">
<h4><span class="header-section-number">7.2.2.2</span> Interpretation</h4>
<p>The model predicts that for an “average participant”, the relationship between condition and <code>acoustics</code> is:<br />
<span class="math display">\[
\texttt{acoustics = -0.71 + 0.326 x conditionLabel.williams}
\]</span></p>
<p>Because the random intercepts are normally distributed, we expect (approximately) 95% of participants to have intercepts within <span class="math inline">\(2 \sigma_s\)</span> of the overall intercept, <span class="math inline">\(\beta_0\)</span>. Thus, the model predicts that 95% of participants in the population have intercepts between -1.309 and -0.113:</p>
<ul>
<li><p>lower bound: <span class="math inline">\(\hat{\beta}_0 - 2 \cdot \hat{\sigma}_s = -0.71 - 2 \cdot 0.299 = -1.309\)</span></p></li>
<li><p>upper bound: <span class="math inline">\(\hat{\beta}_0 + 2 \cdot \hat{\sigma}_s = -0.113\)</span></p></li>
</ul>
<p>(In addition, <span class="math inline">\(\approx\)</span> 95% of observations are predicted to have an error between -1.5 and 1.5, <span class="math inline">\(=\pm 2 \cdot \hat{\sigma}_e\)</span>.)</p>
<p>We can compare the residual error (how much variance is “left over”) in the simple linear regression Model (1A) and the random-intercept model (1B):</p>
<ul>
<li><p>Model 1A: <span class="math inline">\(\hat{\sigma}^2_e = 0.803\)</span><a href="#fn34" class="footnoteRef" id="fnref34"><sup>34</sup></a></p></li>
<li><p>Model 1B: <span class="math inline">\(\hat{\sigma}^2_e = 0.558\)</span></p></li>
</ul>
<p>The residual error variance is much smaller in the random-intercept model. This is because in the random-intercept model, the error of the simple linear regression model has been partitioned into <strong>participant-level</strong> error and <strong>observation-level</strong> error, captured by <span class="math inline">\(\sigma_s\)</span> and <span class="math inline">\(\sigma_e\)</span>. Intuitively, some error has been “given” to the participant level.</p>
<p>Note that the <strong>fixed-effect</strong> coefficients <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> (the “intercept” and “slope”) are nearly the same in Model 1A and Model 1B.</p>
<blockquote>
<p><strong>Questions</strong>:</p>
<ul>
<li>What does this mean? (That the fixed-effect coefficients are the same in the predictions made by Model 1A and Model 1B?)</li>
</ul>
</blockquote>
</div>
<div id="by-participant-predictions" class="section level4">
<h4><span class="header-section-number">7.2.2.3</span> By-participant predictions</h4>
<p>Although the random effects <span class="math inline">\(\alpha_1, ..., \alpha_J\)</span> aren’t fitted parameters, we can extract the following from the model:</p>
<ul>
<li><p>Estimates of the random effects, the “best linear unbiased predictors” (or <em>BLUPs</em>)</p></li>
<li><p>Standard errors of the BLUPs</p></li>
</ul>
<p>This allows us to extract the model predictions for each participant:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ranef</span>(mod1b)$participant <span class="co"># ranef() found in library(lme4), library(arm)</span></code></pre></div>
<pre><code>##      (Intercept)
## 24   0.085333501
## 297  0.036820472
## 432 -0.411839925
## 524  0.116659623
## 529 -0.485395105
## 530 -0.077424563
## 540  0.244536341
## 541  0.193392070
## 542 -0.126468368
## 544  0.012462384
## 547 -0.335559107
## 548  0.489860310
## 549 -0.135853392
## 550 -0.047183013
## 552 -0.032362240
## 553  0.239715782
## 554 -0.045139278
## 555 -0.204230783
## 556 -0.104644687
## 557  0.199844627
## 558  0.004117646
## 559 -0.336348686
## 560  0.307842484
## 561  0.530048433
## 562 -0.163493454
## 563 -0.072168272
## 564  0.117477201</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">se.ranef</span>(mod1b)$participant <span class="co"># se.ranef found in library(arm)</span></code></pre></div>
<pre><code>##     (Intercept)
## 24    0.1583862
## 297   0.1620712
## 432   0.1620712
## 524   0.1853443
## 529   0.1660260
## 530   0.1620712
## 540   0.1660260
## 541   0.1660260
## 542   0.1583862
## 544   0.1748900
## 547   0.1620712
## 548   0.1660260
## 549   0.1583862
## 550   0.1620712
## 552   0.1748900
## 553   0.1660260
## 554   0.1620712
## 555   0.1620712
## 556   0.1620712
## 557   0.1660260
## 558   0.1660260
## 559   0.1702852
## 560   0.1660260
## 561   0.1620712
## 562   0.1583862
## 563   0.1798897
## 564   0.1702852</code></pre>
<p>These values might be useful if you are interested in which participants have particularly high or low baseline values of your dependent variable, for example.</p>
<p>A simpler way to get predictions is to just use <code>predict(yourmodel, newdata = dataframeToPredictFor)</code>.</p>
<p>(<code>predict</code> is a useful function in general, which also works with <code>lm</code> and <code>glm</code>.)</p>
<p>For example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## MODEL 1B: random intercept only
mod1b &lt;-<span class="st"> </span>lme4::<span class="kw">lmer</span>(acoustics ~<span class="st"> </span>conditionLabel.williams +<span class="st"> </span>(<span class="dv">1</span>|participant), <span class="dt">data=</span>givenness)

## set up a dataframe for which the model should predict new values: 
## each level of conditionLabel, for each participant.

## it&#39;s easiest to understand this if we first refit a version of mod1b using the *factor* version of conditionLabel (rather than the numeric conditionLabel.williams version)

mod1b<span class="fl">.1</span> &lt;-<span class="st"> </span>lme4::<span class="kw">lmer</span>(acoustics ~<span class="st"> </span>conditionLabel +<span class="st"> </span>(<span class="dv">1</span>|participant), <span class="dt">data=</span>givenness)

## set up a dataframe to predict values for: one row per participant/cond label pair
newdata &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">expand.grid</span>(<span class="dt">conditionLabel=</span><span class="kw">unique</span>(givenness$conditionLabel),
                                  <span class="dt">participant=</span><span class="kw">unique</span>(givenness$participant)))

## get the predicted value for each case
newdata$pred &lt;-<span class="st"> </span><span class="kw">predict</span>(mod1b<span class="fl">.1</span>, <span class="dt">newdata=</span>newdata)

## plot the model&#39;s prediction for each participant:
<span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>conditionLabel, <span class="dt">y=</span>pred), <span class="dt">data=</span>newdata) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x=</span>conditionLabel,<span class="dt">y=</span>pred, <span class="dt">group=</span>participant)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Condition&quot;</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Model prediction&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:mod1bFig1"></span>
<img src="07-linear-mixed-models_files/figure-html/mod1bFig1-1.png" alt="Predictions for each participant from Model 1B." width="480" />
<p class="caption">
Figure 7.1: Predictions for each participant from Model 1B.
</p>
</div>
<p>We can also get 95% CIs, as in the example in Sec. <a href="lmem.html#c6extraexamples">7.12</a>.</p>
<p>Let’s compare the predictions made by Model 1A (simple linear regression) and <span style="color:red">Model 1B</span> (random intercept), for each participant:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## get the same predictions but for model 1. first fit a version with the factor for conditionlabel:

mod1a<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="kw">lm</span>(acoustics ~<span class="st"> </span>conditionLabel, <span class="dt">data=</span>givenness)

newdata$pred.mod1a &lt;-<span class="st"> </span><span class="kw">predict</span>(mod1a<span class="fl">.1</span>, <span class="dt">newdata=</span>newdata)

<span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>conditionLabel, <span class="dt">y=</span>acoustics), <span class="dt">data=</span>givenness) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_jitter</span>(<span class="dt">alpha=</span><span class="fl">0.1</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x=</span>conditionLabel,<span class="dt">y=</span>pred, <span class="dt">group=</span>participant), <span class="dt">color=</span><span class="st">&#39;red&#39;</span>, <span class="dt">data=</span>newdata) +
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x=</span>conditionLabel,<span class="dt">y=</span>pred.mod1a, <span class="dt">group=</span>participant), <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">data=</span>newdata) +
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Condition&quot;</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Model prediction&quot;</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">facet_wrap</span>(~participant)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:mod1cFig1"></span>
<img src="07-linear-mixed-models_files/figure-html/mod1cFig1-1.png" alt="Model 1A shown in black, Model 1B shown in red." width="672" />
<p class="caption">
Figure 7.2: Model 1A shown in black, Model 1B shown in red.
</p>
</div>
<p>Model 1A always makes the same prediction—the black line—which moves vertically for different participants (red lines).</p>
<p>To get a sense of what the random-intercept model is doing, we can compare the predicted intercept (participant <span class="math inline">\(i\)</span>: <span class="math inline">\(\hat{\beta}_0 + \hat{\alpha}_{j[i]}\)</span>) with the empirical mean for each participant:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## empirical mean acoustic values by participant
df &lt;-<span class="st"> </span>givenness %&gt;%<span class="st"> </span><span class="kw">group_by</span>(participant) %&gt;%<span class="st"> </span><span class="kw">summarise</span>(<span class="dt">meanAcoustics=</span><span class="kw">mean</span>(acoustics))

## participant random effects
df2 &lt;-<span class="st"> </span><span class="kw">ranef</span>(mod1b)$participant
df$modPred &lt;-<span class="st"> </span><span class="kw">fixef</span>(mod1b)[<span class="dv">1</span>] +<span class="st"> </span>df2[<span class="kw">match</span>(df$participant, <span class="kw">rownames</span>(df2)),<span class="dv">1</span>]

<span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>meanAcoustics, <span class="dt">y=</span>modPred), <span class="dt">data=</span>df) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">lty=</span><span class="dv">2</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Partic. empirical mean&quot;</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Partic. predicted intercept&quot;</span>)</code></pre></div>
<p><img src="07-linear-mixed-models_files/figure-html/unnamed-chunk-8-1.png" width="384" style="display: block; margin: auto;" /></p>
<p>The predicted intercepts are closer to the grand mean than the empirical values, a phenomenon called <em>shrinkage</em>, because the (absolute value of the) random intercepts are “shrunk” from the empirical means. Why?</p>
<ul>
<li><p>The predicted value (random effect) is a weighted average of the participant’s (empirical) mean and the grand mean.</p></li>
<li><p>More observations <span class="math inline">\(\implies\)</span> greater weight given to empirical mean</p></li>
</ul>
<p>Shrinkage improves generalization of the model to data from new participants. But importantly, it also means that BLUPs are <strong>not the fitted values</strong> for each participant. They are <em>intentionally</em> not near the empirical means. In contrast, the fixed-effect coefficients (<span class="math inline">\(\hat{\beta}_0, \hat{\beta}_1\)</span>) should be close to the values you’d estimate from empirical data.</p>
<p>Chapter 12 of <span class="citation">Gelman &amp; Hill (<a href="#ref-gelman2007data">2007</a>)</span> discusses these points.</p>
</div>
</div>
</div>
<div id="c6lmm2" class="section level2">
<h2><span class="header-section-number">7.3</span> Linear mixed models 2: One grouping factor, random intercepts and slopes</h2>
<p><a href="#c6model1B">Model 1B</a> allowed the intercept to differ by participant—each participant’s “baseline” is different. In addition, the <strong>slope</strong> of an effect could differ by participant, which is captured in a mixed model by a <em>random slope</em> term. Fig. <a href="lmem.html#fig:mod1cFig1">7.2</a> shows what the predictions from this kind of model would look like. <!-- one line per participant, and each line has a different height and slope. --></p>
<!-- TODO VANNA: add reference here to first figure in 3.2, which can be thought of as schematic for Model 1C. MORGAN: I'm not sure where you want me to put that. Model 1C hasn't been introduced yet so it might be out of place to say something like "See [this figure] for a schematic of Model 1C" -->
<p>For the <code>givenness</code> example used for Models 1A-1B, there turns out to be no detectable by-participant variation in the slope of <code>conditionLabel.williams</code>—see extra examples in Sec. <a href="lmem.html#c6extraexamples">7.12</a>. Thus, we use a different dataset, <a href="datasets-appendix.html#halfdata"><code>halfrhyme</code></a>, where there is clear by-participant variation.</p>
<!-- To load this data: -->
<!-- ```{r} -->
<!-- halfrhyme <- read.csv("datasets/halfrhyme2.csv") -->
<!-- ## you need to do this because the relDuration variable is only defined when conditionLabel is 'voice' -->
<!-- halfrhyme <-  subset(halfrhyme, conditionLabel=='voice') -->
<!-- ``` -->
<p>We have not discussed this dataset’s interpretation, and you can just think of <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> as arbitrary variables if it’s helpful.</p>
<p>We will fit a model with:</p>
<ul>
<li><p>Response (<span class="math inline">\(Y\)</span>): <code>rhymeRating</code></p></li>
<li><p>Fixed effect (<span class="math inline">\(X\)</span>): <code>relDuration</code> (of vowel)</p></li>
<li><p>Random effects:</p>
<ul>
<li><p>By-participant intercept</p></li>
<li><p>By-participant random slope of <code>relDuration</code></p></li>
</ul></li>
</ul>
<p>As in Models 1A and 1B, there is a single grouping factor: <code>participant</code>.</p>
<p>From the empirical data, it does look like participants might vary in both the intercept and the slope of the <code>relDuration</code> effect:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">halfrhyme$participant &lt;-<span class="st"> </span><span class="kw">as.factor</span>(halfrhyme$participant)

<span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>relDuration, <span class="dt">y=</span>rhymeRating), <span class="dt">data=</span>halfrhyme) +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method=</span><span class="st">&#39;lm&#39;</span>, <span class="dt">se=</span>F, <span class="dt">size=</span><span class="fl">0.75</span>) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size=</span><span class="fl">0.1</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">facet_wrap</span>(~participant) +<span class="st"> </span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position=</span><span class="st">&#39;none&#39;</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;relDuration&quot;</span>)</code></pre></div>
<p><img src="07-linear-mixed-models_files/figure-html/unnamed-chunk-9-1.png" width="672" style="display: block; margin: auto;" /></p>
<div id="c6model1c" class="section level3">
<h3><span class="header-section-number">7.3.1</span> Model 1C</h3>
<p>The model for a simple linear-mixed effects regression with a random intercept and random slope for a single grouping factor is:</p>
<p><span class="math display">\[
y_i = \beta_0 + \alpha_{j[i]} + (\beta_1 + \gamma_{j[i]}) x_i + \epsilon_i, \quad 1, 2, ..., n
\]</span> As in previous models:</p>
<ul>
<li><p><span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span> are the fixed effects (intercept and slope)</p></li>
<li><p>Errors are normally distributed: <span class="math display">\[\epsilon_i \sim N(0, \sigma^2_e), \quad i = 1, ..., n\]</span></p></li>
</ul>
<p>For random effects:</p>
<ul>
<li><p><span class="math inline">\(\alpha_{j[i]}\)</span> is the <strong>random intercept</strong> term, which has the same interpretation as in Model 1B.</p></li>
<li><p><span class="math inline">\(\gamma_{j[i]}\)</span> is the <strong>random slope</strong> term, which captures how much each participant’s slope (for <span class="math inline">\(X\)</span>) differs from the “average” slope across participants (<span class="math inline">\(\beta_1\)</span>).</p></li>
</ul>
The by-participant offsets in intercept and in the slope of <span class="math inline">\(X\)</span> are normally distributed:
<span class="math display">\[\begin{align*}
  \alpha_j &amp;\sim N(0, \sigma^2_{p,0}), \quad j = 1, ..., J \\
  \gamma_j &amp;\sim N(0, \sigma^2_{p,1})
\end{align*}\]</span>
<p>Fitting Model 1C means estimating:</p>
<ul>
<li><p>Fixed-effect coefficients: <span class="math inline">\(\beta_0, \beta_1\)</span></p></li>
<li><p>Variance components:</p>
<ul>
<li><p>Random effect variances <span class="math inline">\(\sigma^2_{p,0}, \sigma^2_{p,1}\)</span></p></li>
<li><p>Error variance <span class="math inline">\(\sigma^2_e\)</span></p></li>
</ul></li>
</ul>
</div>
<div id="fitting-model-1c" class="section level3">
<h3><span class="header-section-number">7.3.2</span> Fitting Model 1C</h3>
<p>To fit the model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod1c &lt;-<span class="st"> </span>lme4::<span class="kw">lmer</span>(rhymeRating ~<span class="st"> </span>relDuration +<span class="st"> </span>(<span class="dv">1</span> +<span class="st"> </span>relDuration ||<span class="st"> </span>participant),  
 <span class="dt">data=</span>halfrhyme)</code></pre></div>
<p>where <code>(1 + relDuration||participant)</code> is <code>lme4</code> notation for:</p>
<ul>
<li><p>Random intercept (<code>1</code> means “intercept”)</p></li>
<li><p>Random slope of <code>relDuration</code> (random effects go to the left of the “pipe” symbol <code>|</code>)</p></li>
<li><p>Grouped by participant (grouping factor goes to the right of the pipe)</p></li>
<li><p>The random intercept and slope are <strong>uncorrelated</strong>—indicated by the double pipe <code>||</code>. (We’ll discuss this further when we introduce <a href="lmem.html#random-effect-correlations">correlated random effects</a>.)</p></li>
</ul>
<p><code>(1 + relDuration||participant)</code> is read “uncorrelated by-subject random intercept and by-subject random slope”.</p>
<p>The model’s output is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(mod1c)</code></pre></div>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: 
## rhymeRating ~ relDuration + ((1 | participant) + (0 + relDuration |  
##     participant))
##    Data: halfrhyme
## 
## REML criterion at convergence: 2578.7
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.4022 -0.6091 -0.1126  0.6138  3.5726 
## 
## Random effects:
##  Groups        Name        Variance Std.Dev.
##  participant   (Intercept) 1.267    1.126   
##  participant.1 relDuration 4.482    2.117   
##  Residual                  1.552    1.246   
## Number of obs: 756, groups:  participant, 31
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)   3.3003     0.2125  15.533
## relDuration   2.7249     0.7816   3.486
## 
## Correlation of Fixed Effects:
##             (Intr)
## relDuration 0.192</code></pre>
<p>The fixed effect coefficients, and the by-participant and <code>Residual</code> rows of “Random effects” have the same interpretation as for the random-intercept-only model (Model 1B). What is new is the random slope variance (second row of “Random effects”), which is estimated to be <span class="math inline">\(\hat{\sigma}^2_{p,1} = 4.482\)</span>.</p>
<p>The interpretation of this “random slope” term is:</p>
<ul>
<li><p><span class="math inline">\(\hat{\sigma}_{p,1}\)</span> = 2.12: degree of variability among participants in the slope of <code>relDuration</code></p></li>
<li><p><span class="math inline">\(\approx 95\%\)</span> of participants (in the population) have slope of <code>relDuration</code> <strong>between -1.51 and 6.96</strong></p>
<ul>
<li><span class="math inline">\(=\hat{\beta}_1 \pm 2 \cdot \hat{\sigma}_{p,1}\)</span></li>
</ul></li>
</ul>
<blockquote>
<p><strong>Questions</strong>:</p>
<ul>
<li>How much variability is there in participants’ intercept values?</li>
</ul>
</blockquote>
<p>Again, we can use <code>ranef()</code> or <code>predict()</code> to get by-participant model predictions, using the estimated random effects (BLUPs).</p>
<p>To get model predictions for each participant, and visualize them:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## get model predictions for mod1c for each participant

## first, set up a prediction frame, say from min to max values of relDuration in the data, for each participant
newdata &lt;-<span class="st"> </span><span class="kw">data.frame</span>(
    <span class="kw">expand.grid</span>(
      <span class="dt">relDuration=</span><span class="kw">seq</span>(<span class="kw">min</span>(halfrhyme$relDuration), 
                      <span class="kw">max</span>(halfrhyme$relDuration), <span class="dt">by=</span><span class="fl">0.01</span>),
      <span class="dt">participant=</span><span class="kw">unique</span>(halfrhyme$participant)
      )
    )

## get the predicted value for each case
newdata$pred &lt;-<span class="st"> </span><span class="kw">predict</span>(mod1c, <span class="dt">newdata=</span>newdata)

## plot the model&#39;s prediction for each participant:
<span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>relDuration, <span class="dt">y=</span>pred), <span class="dt">data=</span>newdata) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x=</span>relDuration,<span class="dt">y=</span>pred, <span class="dt">group=</span>participant)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;relDuration&quot;</span>) +
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Predicted rhymeRating&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-12"></span>
<img src="07-linear-mixed-models_files/figure-html/unnamed-chunk-12-1.png" alt="Predictions for each participant from Model 1C." width="480" />
<p class="caption">
Figure 7.3: Predictions for each participant from Model 1C.
</p>
</div>
<p>Plot model predictions and empirical data for each participant:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>relDuration, <span class="dt">y=</span>pred), <span class="dt">data=</span>newdata) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x=</span>relDuration,<span class="dt">y=</span>pred, <span class="dt">group=</span>participant)) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">x=</span>relDuration,<span class="dt">y=</span>rhymeRating),<span class="dt">data=</span>halfrhyme, <span class="dt">alpha=</span><span class="fl">0.5</span>, <span class="dt">size=</span><span class="dv">1</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">ylim</span>(<span class="dv">1</span>,<span class="dv">7</span>) +
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;relDuration&quot;</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Predicted rhymeRating&quot;</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">facet_wrap</span>(~participant) </code></pre></div>
<p><img src="07-linear-mixed-models_files/figure-html/unnamed-chunk-13-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="linear-mixed-models-3-two-grouping-factors" class="section level2">
<h2><span class="header-section-number">7.4</span> Linear mixed models 3: Two grouping factors</h2>
<p>Often in linguistic data, there is more than one grouping factor. Some common cases:</p>
<ol style="list-style-type: decimal">
<li><p>Laboratory experiments: both participants <strong>and</strong> items are sampled from larger populations.<a href="#fn35" class="footnoteRef" id="fnref35"><sup>35</sup></a></p></li>
<li><p>Corpus data: both speakers/authors <strong>and</strong> words are sampled from larger populations.</p></li>
</ol>
<p>We will focus on the participant/item case (#1) for exposition.</p>
<p>Most experiments have clear by-participant and by-item variability. In RM-ANOVA analyses, this is dealt with by fitting two separate “by-participant” and “by-item” models, an awkward solution that can lower statistical power. In mixed-effects models, it is possible to account for both kinds of variability simultaneously, by including by-participant and by-item random effects.</p>
<p>This is a case of <em>crossed</em> random effects: multiple grouping factors, which vary independently. (As opposed to “nested” random effects, such as by-school variability and by-participant within school.) Crossed random effect structure is necessary to model even simple linguistic experiments. The facility of fitting crossed random effects in <code>lme4</code> makes it well-suited for modeling linguistic data.<a href="#fn36" class="footnoteRef" id="fnref36"><sup>36</sup></a></p>
<p>We introduce crossed random effects for the same case as in Models 1A and 1B: <code>acoustics ~ conditionLabel.williams</code>, for the <code>givenness</code> data.</p>
<div id="c6model2A" class="section level3">
<h3><span class="header-section-number">7.4.1</span> Model 2A: By-participant and by-item random intercepts</h3>
<p>The model for this case is:</p>
<span class="math display">\[\begin{equation*}
  y_i = \beta_0 + \alpha_{par, j[i]} + \alpha_{item, k[i]} + \beta_1 x_i + \epsilon_i
\end{equation*}\]</span>
<p>The fixed effects (<span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>) and errors (<span class="math inline">\(\epsilon_i \sim N(0,\sigma_e)\)</span>) are as in previous models.</p>
<p>The random effects are:</p>
<ul>
<li><p><span class="math inline">\(\alpha_{par, j[i]}\)</span>: by-participant random intercept</p></li>
<li><p><span class="math inline">\(\alpha_{item, k[i]}\)</span>: by-item random intercept</p></li>
</ul>
The random intercepts are assumed to be normally distributed:
<span class="math display">\[\begin{align*}
  \alpha_{par, j[i]} &amp;\sim N(0, \sigma^2_{par}), \quad j = 1,..., J \\
  \alpha_{item, k[i]} &amp;\sim N(0, \sigma^2_{item}), \quad j = 1,..., K \\
\end{align*}\]</span>
<p>where <span class="math inline">\(\sigma^2_{par}\)</span> and <span class="math inline">\(\sigma^2_{item}\)</span> quantify the degree of variability among participants and items in the intercept value.</p>
<blockquote>
<p><strong>Questions</strong>:</p>
<ul>
<li>What is the interpretation of each random intercept in this example? For example, what do <span class="math inline">\(\alpha_{par,2}\)</span> and <span class="math inline">\(\alpha_{item,5}\)</span> mean?</li>
</ul>
</blockquote>
<div id="fitting-model-2a" class="section level4">
<h4><span class="header-section-number">7.4.1.1</span> Fitting Model 2A</h4>
<p>To fit this model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod2a &lt;-<span class="st"> </span>lme4::<span class="kw">lmer</span>(acoustics ~<span class="st"> </span>conditionLabel.williams +<span class="st"> </span>(<span class="dv">1</span>|participant) +<span class="st"> </span>(<span class="dv">1</span>|item), <span class="dt">data=</span>givenness)</code></pre></div>
<p>The model’s output is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(mod2a)</code></pre></div>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: acoustics ~ conditionLabel.williams + (1 | participant) + (1 |  
##     item)
##    Data: givenness
## 
## REML criterion at convergence: 887
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.7608 -0.6026 -0.0187  0.6293  3.2908 
## 
## Random effects:
##  Groups      Name        Variance Std.Dev.
##  participant (Intercept) 0.09266  0.3044  
##  item        (Intercept) 0.04108  0.2027  
##  Residual                0.51732  0.7192  
## Number of obs: 382, groups:  participant, 27; item, 16
## 
## Fixed effects:
##                         Estimate Std. Error t value
## (Intercept)             -0.71655    0.08585  -8.347
## conditionLabel.williams  0.33771    0.07406   4.560
## 
## Correlation of Fixed Effects:
##             (Intr)
## cndtnLbl.wl 0.001</code></pre>
<blockquote>
<p><strong>Questions</strong>:</p>
<ul>
<li><p>Is there more variability among participants or items?</p></li>
<li><p>This pattern is common in laboratory experiments. Why?</p></li>
</ul>
</blockquote>
</div>
<div id="exercise-1-2" class="section level4 unnumbered">
<h4>Exercise 1</h4>
<p>Based on your understanding of the mixed models presented so far (1B, 1C), you should be able to answer these questions.</p>
<ul>
<li><p>In Model 2A:</p>
<ul>
<li><p>95% of subjects have intercepts between _____ and _____</p></li>
<li><p>95% of items have intercepts between _____ and _____</p></li>
</ul></li>
<li><p>What is the residual error for these three models, fit with the same fixed effects (<code>acoustics ~ conditionLabel.williams</code>):</p>
<ul>
<li><p>Model 1A (simple linear regression) (fit <code>mod1a</code>, then <code>summary(mod1a)</code>)</p></li>
<li><p>Model 1B (by-participant random intercept)</p></li>
<li><p>Model 2A (by-participant and by-word random intercept)</p></li>
</ul></li>
<li><p>Why does the pattern you see make sense?</p></li>
</ul>
</div>
</div>
</div>
<div id="evaluating-lmms" class="section level2">
<h2><span class="header-section-number">7.5</span> Evaluating LMMs</h2>
<div id="hypothesis-testing-2" class="section level3">
<h3><span class="header-section-number">7.5.1</span> Hypothesis testing</h3>
<p>In non-mixed models, we see a test statistic and <span class="math inline">\(p\)</span>-value for each term in the regression model, corresponding to the hypothesis test that the term is different from zero.</p>
<p>We might expect to see a test statistic and <span class="math inline">\(p\)</span>-value for each random-effect and fixed-effect term in a mixed model, but we don’t. For example, in Model 2A:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(mod2a)</code></pre></div>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: acoustics ~ conditionLabel.williams + (1 | participant) + (1 |  
##     item)
##    Data: givenness
## 
## REML criterion at convergence: 887
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.7608 -0.6026 -0.0187  0.6293  3.2908 
## 
## Random effects:
##  Groups      Name        Variance Std.Dev.
##  participant (Intercept) 0.09266  0.3044  
##  item        (Intercept) 0.04108  0.2027  
##  Residual                0.51732  0.7192  
## Number of obs: 382, groups:  participant, 27; item, 16
## 
## Fixed effects:
##                         Estimate Std. Error t value
## (Intercept)             -0.71655    0.08585  -8.347
## conditionLabel.williams  0.33771    0.07406   4.560
## 
## Correlation of Fixed Effects:
##             (Intr)
## cndtnLbl.wl 0.001</code></pre>
<ul>
<li><p>Under <code>Random effects</code>, there are no test statistics or <span class="math inline">\(p\)</span>-values.</p></li>
<li><p>Under <code>Fixed effects</code>, there are test statistics but no <span class="math inline">\(p\)</span>-values.</p></li>
</ul>
<p>How can we assess whether a fixed-effect or random-effect term contributes <strong>significantly</strong> to a mixed-effects model?</p>
<p>This turns out to be a contentious question, for practical and philosophical reasons—especially for fixed effects, which we usually care about more.<a href="#fn37" class="footnoteRef" id="fnref37"><sup>37</sup></a> Some (technical) discussion is given <a href="https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#why-doesnt-lme4-display-denominator-degrees-of-freedomp-values-what-other-options-do-i-have">here</a>.</p>
<p>The upshot is that there are several ways to calculate <span class="math inline">\(p\)</span>-values, which vary along an axis from “more approximate, but faster to compute” to “exact, but very time consuming to compute”.</p>
<p>In the opinion of one author (Morgan), any method in Sec. <a href="lmem.html#c6fixedp">7.5.3</a> more complex than “Use <span class="math inline">\(t\)</span> statistic” is probably fine—and as usual, if how you calculate significance matters much to the conclusion you make, you probably shouldn’t put much stock into the effect anyway. However, it’s good to be aware of the issues, not least because other researchers (including reviewers) think it is important to use a more precise method.</p>
</div>
<div id="significance-of-a-random-effect-term" class="section level3">
<h3><span class="header-section-number">7.5.2</span> Significance of a random effect term</h3>
<p>Hypothesis testing for random effects turns out to follow a similar logic to testing the effect of multiple terms in a logistic regression model, where we used a likelihood ratio test, as in Sec. <a href="logistic-regression.html#c4lrt">5.2.1</a>.</p>
<p>For a large enough dataset, the difference in deviance between a model with and without <span class="math inline">\(k\)</span> random effect terms approximately follows a <span class="math inline">\(\chi^2\)</span> distribution with <span class="math inline">\(df=k\)</span>. This means we can use a likelihood ratio test to assess whether these terms significantly contribute.</p>
<div id="example-19" class="section level4 unnumbered">
<h4>Example</h4>
<p>Does the by-item random intercept in Model 2A significantly contribute to model likelihood?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod2a &lt;-<span class="st"> </span>lme4::<span class="kw">lmer</span>(acoustics ~<span class="st"> </span>conditionLabel.williams +<span class="st"> </span>(<span class="dv">1</span>|participant) +<span class="st"> </span>(<span class="dv">1</span>|item), <span class="dt">data=</span>givenness)
mod2a<span class="fl">.1</span> &lt;-<span class="st"> </span>lme4::<span class="kw">lmer</span>(acoustics ~<span class="st"> </span>conditionLabel.williams +<span class="st"> </span>(<span class="dv">1</span>|participant), <span class="dt">data=</span>givenness)

<span class="kw">anova</span>(mod2a, mod2a<span class="fl">.1</span>, <span class="dt">test=</span><span class="st">&#39;Chisq&#39;</span>)</code></pre></div>
<pre><code>## refitting model(s) with ML (instead of REML)</code></pre>
<pre><code>## Data: givenness
## Models:
## mod2a.1: acoustics ~ conditionLabel.williams + (1 | participant)
## mod2a: acoustics ~ conditionLabel.williams + (1 | participant) + (1 | 
## mod2a:     item)
##         Df    AIC    BIC  logLik deviance Chisq Chi Df Pr(&gt;Chisq)   
## mod2a.1  4 899.07 914.86 -445.54   891.07                           
## mod2a    5 890.58 910.31 -440.29   880.58 10.49      1     0.0012 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Thus, <code>acoustics</code> differs significantly between items (<span class="math inline">\(\chi^2(1)=10.49, p=0.0012\)</span>).</p>
<p>Note that this method doesn’t work for testing the significance of a random effect in a model with just <strong>one</strong> random effect, such as Model 1B, because the models being compared are not of the same type (one is a mixed model and one isn’t, so the likelihoods are not directly comparable). In this case we can instead use an “exact restricted likelihood ratio test”, implemented as <code>exactRLRT()</code> in the <code>RLRsim</code> package. This is not a common case.</p>
</div>
</div>
<div id="c6fixedp" class="section level3">
<h3><span class="header-section-number">7.5.3</span> Significance of fixed effects</h3>
<p>Several options are available for calculating <span class="math inline">\(p\)</span>-values for fixed effect terms, including the following, listed in increasing order of precision/computation time:</p>
<ol style="list-style-type: decimal">
<li><p>Use <span class="math inline">\(t\)</span> statistic / Wald test</p></li>
<li><p>Likelihood ratio test</p></li>
<li><p>Satterthwaite approximation (<code>lmerTest</code>)</p></li>
<li><p>Parametric bootstrap</p></li>
</ol>
<p>We show an example of how to do each, with some discussion.</p>
<div id="t-statistic" class="section level4">
<h4><span class="header-section-number">7.5.3.1</span> <span class="math inline">\(t\)</span>-statistic</h4>
<p>The first method is simply to assume that the <span class="math inline">\(t\)</span>-statistic for a coefficient—its estimated value divided by its standard error—follows a normal distribution (equivalent to assuming high <span class="math inline">\(df\)</span>), and calculate a two-sided Wald test on <span class="math inline">\(|t|\)</span>. For example, for <span class="math inline">\(t=-2.03\)</span>, the <span class="math inline">\(p\)</span>-value would be:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">2</span>*(<span class="dv">1</span>-<span class="kw">pt</span>(<span class="kw">abs</span>(-<span class="fl">2.03</span>), <span class="dt">df=</span><span class="dv">1000</span>))</code></pre></div>
<pre><code>## [1] 0.04262085</code></pre>
<p>This is not a good way to get a <span class="math inline">\(p\)</span>-value—it’s very approximate. However, it has a useful corollary: observing that 95% of a <span class="math inline">\(t\)</span> distribution with high <span class="math inline">\(df\)</span> has <span class="math inline">\(|t|&lt;2\)</span> gives a simple <strong>rule of thumb</strong>: fixed effects with <span class="math inline">\(|t|&gt;2\)</span> are (roughly!) significant at the <span class="math inline">\(p&lt;0.05\)</span> level.</p>
<p>Thus, if <span class="math inline">\(|t|\)</span> is much larger than 2 (say <span class="math inline">\(|t|&gt;4\)</span>), the effect is highly significant and you don’t need to bother using a more exact method to get a <span class="math inline">\(p\)</span>-value.</p>
<p>In the literature, <span class="math inline">\(|t|&gt;2\)</span> is sometimes used as a rough “is this term significant?” criterion, without comment.</p>
</div>
<div id="likelihood-ratio-tests" class="section level4">
<h4><span class="header-section-number">7.5.3.2</span> Likelihood ratio tests</h4>
<p>This works exactly like testing the significance of 1+ random effect terms. You fit a model with and without 1+ fixed-effect terms, and compare them with a likelihood ratio test using the <code>anova()</code> command. For large enough datasets, the difference in deviance between the two models follows a <span class="math inline">\(\chi^2\)</span> distribution under the null hypothesis that the fixed-effect coefficients (for the omitted predictors) are zero.</p>
<p>For example, to test whether <code>conditionLabel.williams</code> significantly contributes to Model 2A, using an LR test:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod2a &lt;-<span class="st"> </span>lme4::<span class="kw">lmer</span>(acoustics ~<span class="st"> </span>conditionLabel.williams +<span class="st"> </span>(<span class="dv">1</span>|participant) +<span class="st"> </span>(<span class="dv">1</span>|item), <span class="dt">data=</span>givenness)
 
mod2a<span class="fl">.1</span> &lt;-<span class="st"> </span>lme4::<span class="kw">lmer</span>(acoustics ~<span class="st"> </span><span class="dv">1</span>+<span class="st"> </span>(<span class="dv">1</span>|participant) +<span class="st"> </span>(<span class="dv">1</span>|item), <span class="dt">data=</span>givenness)
 
<span class="kw">anova</span>(mod2a, mod2a<span class="fl">.1</span>)</code></pre></div>
<pre><code>## refitting model(s) with ML (instead of REML)</code></pre>
<pre><code>## Data: givenness
## Models:
## mod2a.1: acoustics ~ 1 + (1 | participant) + (1 | item)
## mod2a: acoustics ~ conditionLabel.williams + (1 | participant) + (1 | 
## mod2a:     item)
##         Df    AIC    BIC  logLik deviance Chisq Chi Df Pr(&gt;Chisq)    
## mod2a.1  4 908.69 924.48 -450.35   900.69                            
## mod2a    5 890.58 910.31 -440.29   880.58 20.11      1  7.313e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>There is a significant effect of <code>conditionLabel.williams</code> (<span class="math inline">\(\chi^2(1)=20.1\)</span>, <span class="math inline">\(p&lt;0.001\)</span>).</p>
</div>
<div id="c6sattapprox" class="section level4">
<h4><span class="header-section-number">7.5.3.3</span> Satterthwaite approximation</h4>
<p>An option which gives a reasonably good <span class="math inline">\(p\)</span>, in reasonable computation time, is using the <em>Satterthwaite approximation</em> to compute (approximately correct) <span class="math inline">\(df\)</span> for each fixed-effect coefficient, which is then used to conduct a two-sided <span class="math inline">\(t\)</span> test. This method has become popular due in part to its handy implementation in the <code>lmerTest</code> package, which redefines the <code>lmer</code> command to calculate these <span class="math inline">\(df\)</span> and <span class="math inline">\(p\)</span>-values, and add them to the standard <code>lmer</code> output.</p>
<p>For example, re-running Model 2A with <code>lmerTest</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(lmerTest)

mod2a &lt;-<span class="st"> </span><span class="kw">lmer</span>(acoustics ~<span class="st"> </span>conditionLabel.williams +<span class="st"> </span>(<span class="dv">1</span>|participant) +<span class="st"> </span>(<span class="dv">1</span>|item), <span class="dt">data=</span>givenness)
<span class="kw">summary</span>(mod2a)</code></pre></div>
<pre><code>## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [
## lmerModLmerTest]
## Formula: acoustics ~ conditionLabel.williams + (1 | participant) + (1 |  
##     item)
##    Data: givenness
## 
## REML criterion at convergence: 887
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.7608 -0.6026 -0.0187  0.6293  3.2908 
## 
## Random effects:
##  Groups      Name        Variance Std.Dev.
##  participant (Intercept) 0.09266  0.3044  
##  item        (Intercept) 0.04108  0.2027  
##  Residual                0.51732  0.7192  
## Number of obs: 382, groups:  participant, 27; item, 16
## 
## Fixed effects:
##                          Estimate Std. Error        df t value Pr(&gt;|t|)
## (Intercept)              -0.71655    0.08585  27.88067  -8.347 4.57e-09
## conditionLabel.williams   0.33771    0.07406 342.45831   4.560 7.14e-06
##                            
## (Intercept)             ***
## conditionLabel.williams ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Correlation of Fixed Effects:
##             (Intr)
## cndtnLbl.wl 0.001</code></pre>
<p>We recommend (in 2018) calculating <span class="math inline">\(p\)</span>-values for fixed-effect terms in linear mixed models using the Satterthwaite approximation, as an easy and reasonably accurate option.</p>
<p><strong>Notes</strong>:</p>
<ul>
<li><p>After loading <code>lmerTest</code>, your <code>lmer</code> models will take about twice as long to run. This is only an issue if you are fitting complicated models or analyzing large datasets.</p></li>
<li><p>If you want to use the <code>lme4</code> of <code>lmer</code> <strong>after</strong> loading <code>lmerTest</code>, you use <code>lme4::lmer</code>. (For example, to run a model that doesn’t take twice as long.)</p></li>
</ul>
</div>
<div id="parametric-bootstrap" class="section level4">
<h4><span class="header-section-number">7.5.3.4</span> Parametric bootstrap</h4>
<p><em>Parametric bootstrapping</em> (PB) is a very accurate method for calculating p-values, which also takes a very long time. One useful implementation of PB is in the <code>afex</code> package; for more general PB computations the <code>bootMer()</code> function of <code>lme4</code> can be used.</p>
<p>Roughly, what PB is doing is simulating many new datasets from your model, assuming a given fixed effect coefficient is set to <strong>zero</strong> (but all other coefficients are kept at their fitted values). It then fits the original model to each of the new datasets, resulting in a distribution of values for the fixed-effect coefficient of interest. The proportion of these re-fitted models where the coefficient (which <strong>should</strong> be zero) is as large or larger than the original estimate of the coefficient’s value (the first time you ran the model, on the real data) is the p-value. This is a very direct implementation of the meaning of Type 1 error (“how often would I wrongly conclude the coefficient was at least this large, if I redid the experiment many times?”).</p>
<p>Thus, PB effectively needs to re-run your model <code>nsim</code> times, and the higher <code>nsim</code> is, the more accurate the p-values are. Here’s an example for Model 2A, with <code>nsim = 1000</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(afex)
mod2a.pb &lt;-<span class="st"> </span><span class="kw">mixed</span>(acoustics ~<span class="st"> </span>conditionLabel.williams +<span class="st"> </span>(<span class="dv">1</span>|participant) +<span class="st"> </span>(<span class="dv">1</span>|item), <span class="dt">args_test =</span> <span class="kw">list</span>(<span class="dt">nsim=</span><span class="dv">1000</span>), <span class="dt">data=</span>givenness, <span class="dt">method=</span><span class="st">&quot;PB&quot;</span>)</code></pre></div>
<pre><code>## Fitting 2 (g)lmer() models:
## [..]
## Obtaining 1 p-values:
## [.]</code></pre>
<p>This takes a couple minutes on a laptop. Results:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod2a.pb</code></pre></div>
<pre><code>## Mixed Model Anova Table (Type 3 tests, PB-method)
## 
## Model: acoustics ~ conditionLabel.williams + (1 | participant) + (1 | 
## Model:     item)
## Data: givenness
##                    Effect df     Chisq p.value
## 1 conditionLabel.williams  1 20.11 ***   .0010
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The <span class="math inline">\(p\)</span>-value is very small in this model, similar to when the Satterthwaite approximation is used, but now it is only 0.001. The true p-value is probably much lower than 0.001, but 0.001 is the lowest value we can get using <code>nsim = 1000</code>. (Why?)</p>
<p>You can try running with <code>nsim = 1000000</code> to get a more accurate <span class="math inline">\(p\)</span> value while you do something else for a couple hours.</p>
</div>
</div>
<div id="evaluating-goodness-of-fit" class="section level3">
<h3><span class="header-section-number">7.5.4</span> Evaluating goodness of fit</h3>
<p>As for all regression models, we would like a measure of goodness-of-fit for linear mixed-effects models (LMMs). However, there is no simple measure for LMMs with the same properties of <span class="math inline">\(R^2\)</span> for linear regression, including interpretability as “proportion of variation explained.” This is because in LMMs there is more than one kind of variation that can be “explained”:</p>
<ul>
<li><p>Residual variation: error in each observation</p></li>
<li><p>Variation among speakers in intercept</p></li>
<li><p>Variation among speakers in the slope of <span class="math inline">\(X\)</span></p></li>
<li><p>etc.</p></li>
</ul>
<p>Thus, capturing “goodness of fit” for LMMs requires more than one measure. <span class="citation">Snijders &amp; Bosker (<a href="#ref-snijders2011multilevel">2011</a>)</span> give useful discussion.</p>
<p>That said, one simple and common recipe is just to take the squared correlation of an LMM’s predictions (<span class="math inline">\(\hat{y}_i\)</span>) with the observations (<span class="math inline">\(y_i\)</span>), and call this “<span class="math inline">\(R^2\)</span>”.</p>
<p>This measure means “amount of variability accounted for by the model in this dataset”, without distinguishing between fixed and random effect terms. Thus, <span class="math inline">\(R^2\)</span> is often rather high for LMMs, simply because much variability in the data comes down to by-participant and by-item variability—even if the model would not actually have much predictive power on new data (<em>unseen</em> participants or items).</p>
<p>For models we have considered so far of <code>givenness ~ conditionLabel.williams</code>:</p>
<ul>
<li>Model 1A:</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor</span>(<span class="kw">predict</span>(mod1a), givenness$acoustics)^<span class="dv">2</span></code></pre></div>
<pre><code>## [1] 0.03779243</code></pre>
<ul>
<li>Model 1B:</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor</span>(<span class="kw">predict</span>(mod1b), givenness$acoustics)^<span class="dv">2</span></code></pre></div>
<pre><code>## [1] 0.2192169</code></pre>
<ul>
<li>Model 2A:</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor</span>(<span class="kw">predict</span>(mod2a), givenness$acoustics)^<span class="dv">2</span></code></pre></div>
<pre><code>## [1] 0.3054109</code></pre>
<p><span class="math inline">\(R^2\)</span> increases with the number of random-effect terms. This makes sense as a measure of goodness-of-fit for this dataset: more variability in the data is “explained”—as being due to participant or item variability, as opposed to noise. It does not make sense if <span class="math inline">\(R^2\)</span> is viewed as a measure of predictive power: the three models have almost identical fixed-effect coefficients (<span class="math inline">\(\hat{\beta}_0\)</span>, <span class="math inline">\(\hat{\beta}_1\)</span>), meaning they will make near-identical predictions for unseen data!</p>
</div>
</div>
<div id="linear-mixed-models-4-multiple-predictors" class="section level2">
<h2><span class="header-section-number">7.6</span> Linear mixed models 4: Multiple predictors</h2>
<p>So far we have considered four types of linear mixed-effects models:</p>
<ul>
<li><p>Linear regression, no random effects (<a href="lmem.html#c6model1A">Model 1A</a>)</p></li>
<li><p>By-participant random intercept (<a href="#c6model1B">Model 1B</a>)</p></li>
<li><p>By-participant random intercept and slope (<a href="#c6model1C">Model 1C</a>)</p></li>
<li><p>By-participant and by-item random intercepts (<a href="lmem.html#c6model2A">Model 2A</a>)</p></li>
</ul>
<p>all with a single predictor.</p>
<p>We now turn to LMMs with multiple predictors:</p>
<ul>
<li><p><a href="lmem.html#c6model3A">Model 3A</a>: random intercepts only</p></li>
<li><p><a href="lmem.html#c6model3B">Model 3B</a>: random intercepts + slopes</p></li>
</ul>
<p>We fit Models 3A and 3B to the same data, to demonstrate LMMs with multiple predictors (= “multiple fixed effects”).</p>
<ul>
<li><p>Data: <a href="datasets-appendix.html#givedata"><code>givenness</code></a></p></li>
<li><p>Response: <code>acoustics</code></p></li>
<li><p>Fixed effects:</p>
<ol style="list-style-type: decimal">
<li><p><code>conditionLabel.williams</code></p></li>
<li><p><code>npType.pron</code></p></li>
<li><p><code>voice.passive</code></p></li>
<li><p><code>order</code></p></li>
<li><p><code>conditionLabel:npType</code> (interaction)</p></li>
</ol></li>
</ul>
<p>These fixed effects are one possible set one could arrive at via exploratory data analysis to analyze the data with the goal of testing whether the Williams effect exists (see <a href="datasets-appendix.html#givedata">dataset description</a>). The <code>conditionLabel.williams</code> term is of primary interest.</p>
<!-- FOR LING 620: it used to say "These fixed effects are one possible set you could have arrived at via exploratory data analysis in Mini Project 1." -->
<p>We will denote the fixed-effect coefficients for (1)–(5) as <span class="math inline">\(\beta_{\texttt{conditionLabel}}\)</span>, and so on.</p>
<div id="types-of-predictors" class="section level3">
<h3><span class="header-section-number">7.6.1</span> Types of predictors</h3>
<p>For fitting and interpreting mixed models, it is important to think of predictors in terms of their <em>level</em>. A predictor which describes something about participants (e.g. participant gender) is <em>participant-level</em>, a predictor which describes a property of items (e.g. word frequency) is <em>item-level</em>, and so on. Participant-level predictors are sometimes called “between-participant”, because they do not vary within participants, while a predictor that varies within participant is “within-participant” (and similarly for “between-item”, etc.). (We will not use within/between-X terminology, but you may be familiar with it, and it’s widely used in the literature.) A predictor which has a different value for every observation—that is, varies within item and participant—is called <em>observation-level</em>.</p>
<p>For the <code>givenness</code> data:</p>
<ul>
<li><p>Every item appears in two conditions (<em>Williams</em>, <em>contrast</em>) and two NP types (<em>pronoun</em>, <em>full NP</em>).</p></li>
<li><p>Every item has <strong>one</strong> voice (<em>active</em> or <em>passive</em>)</p></li>
<li><p>Thus, <code>voice</code> is an item-level predictor</p></li>
<li><p>All other predictors vary within participants and items <span class="math inline">\(\implies\)</span> observation-level predictors:</p>
<ul>
<li><p><code>conditionLabel.williams</code>, <code>order</code>, <code>npType.pron</code></p></li>
<li><p><code>conditionLabel:npType</code> (if both <code>A</code> and <code>B</code> are observation level, so is <code>A:B</code>—why?)</p></li>
</ul></li>
<li><p>There are no participant-level predictors.</p></li>
</ul>
</div>
<div id="c6model3A" class="section level3">
<h3><span class="header-section-number">7.6.2</span> Model 3A: Random intercepts only</h3>
<p>This “intercepts only” model includes by-participant and by-item random intercepts, as well as the fixed effects described above. To fit the model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Model 3A: multiple predictors, by-item and by-partic random effects
## (p-values from lmerTest)
<span class="kw">library</span>(lmerTest)
mod3a &lt;-<span class="st"> </span><span class="kw">lmer</span>(acoustics ~<span class="st"> </span>conditionLabel.williams*npType.pron +<span class="st"> </span>voice.passive +<span class="st"> </span>order.std +<span class="st"> </span>(<span class="dv">1</span>|participant) +<span class="st"> </span>(<span class="dv">1</span>|item), <span class="dt">data=</span>givenness)</code></pre></div>
<p>The model’s output is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(lmerTest)
<span class="kw">summary</span>(mod3a)</code></pre></div>
<pre><code>## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [
## lmerModLmerTest]
## Formula: 
## acoustics ~ conditionLabel.williams * npType.pron + voice.passive +  
##     order.std + (1 | participant) + (1 | item)
##    Data: givenness
## 
## REML criterion at convergence: 758.6
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.7638 -0.6535 -0.0102  0.5750  3.5847 
## 
## Random effects:
##  Groups      Name        Variance Std.Dev.
##  participant (Intercept) 0.11624  0.3409  
##  item        (Intercept) 0.03825  0.1956  
##  Residual                0.34715  0.5892  
## Number of obs: 382, groups:  participant, 27; item, 16
## 
## Fixed effects:
##                                      Estimate Std. Error        df t value
## (Intercept)                          -0.71517    0.08728  29.38442  -8.194
## conditionLabel.williams               0.32769    0.06074 338.37005   5.395
## npType.pron                           0.77529    0.06072 338.22707  12.768
## voice.passive                         0.05087    0.11572  12.11117   0.440
## order.std                            -0.12730    0.10819  17.79197  -1.177
## conditionLabel.williams:npType.pron   0.31916    0.12126 337.32828   2.632
##                                     Pr(&gt;|t|)    
## (Intercept)                         4.45e-09 ***
## conditionLabel.williams             1.29e-07 ***
## npType.pron                          &lt; 2e-16 ***
## voice.passive                        0.66795    
## order.std                            0.25483    
## conditionLabel.williams:npType.pron  0.00888 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Correlation of Fixed Effects:
##             (Intr) cndtL. npTyp. vc.pss ordr.s
## cndtnLbl.wl  0.001                            
## npType.pron  0.001 -0.015                     
## voice.passv  0.009  0.005 -0.023              
## order.std   -0.004  0.015 -0.032  0.107       
## cndtnLb.:T. -0.006 -0.004  0.003  0.003 -0.021</code></pre>
<p>Comparing significances and directions of the fixed effects, Model 3A is mostly similar to a model without the random intercepts:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(<span class="kw">lm</span>(acoustics ~<span class="st"> </span>conditionLabel.williams*npType.pron +<span class="st"> </span>voice.passive +<span class="st"> </span>order.std, <span class="dt">data=</span>givenness))</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = acoustics ~ conditionLabel.williams * npType.pron + 
##     voice.passive + order.std, data = givenness)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.94657 -0.49206  0.00032  0.46756  2.31921 
## 
## Coefficients:
##                                     Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)                         -0.72335    0.03582 -20.193  &lt; 2e-16
## conditionLabel.williams              0.30626    0.07169   4.272 2.46e-05
## npType.pron                          0.75743    0.07171  10.563  &lt; 2e-16
## voice.passive                        0.05866    0.07203   0.814   0.4159
## order.std                           -0.19556    0.07211  -2.712   0.0070
## conditionLabel.williams:npType.pron  0.31954    0.14346   2.227   0.0265
##                                        
## (Intercept)                         ***
## conditionLabel.williams             ***
## npType.pron                         ***
## voice.passive                          
## order.std                           ** 
## conditionLabel.williams:npType.pron *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.7 on 376 degrees of freedom
## Multiple R-squared:  0.2768, Adjusted R-squared:  0.2672 
## F-statistic: 28.78 on 5 and 376 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>With one exception: the <code>order</code> effect, which has smaller effect size and is no longer significant in the mixed model. To see why, note how items were presented in the experiment:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(<span class="dt">data=</span>givenness, <span class="kw">aes</span>(<span class="dt">x=</span><span class="kw">as.factor</span>(item), <span class="dt">y=</span>order)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">position=</span><span class="kw">position_jitter</span>(<span class="dt">w=</span><span class="fl">0.2</span>, <span class="dt">h=</span><span class="fl">0.2</span>)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Item&quot;</span>)</code></pre></div>
<p><img src="07-linear-mixed-models_files/figure-html/unnamed-chunk-29-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Items were presented in four blocks—something that isn’t accounted for unless the regression model allows for by-item variability. <!-- FOR LING 620: we didn't account for in our earlier model (in MP 1).  --> Not accounting for such variability in the multiple linear regression model leads to a spurious <code>order</code> effect.</p>
<p>In this case, the random effects in the mixed model are simply accounting for information we forgot to include in the fixed-effects-only model. (We could have included “block” as a factor.) But more generally, one useful function of random effects is to account for by-participant and by-item variability <strong>beyond sources included as predictors in the model</strong>—either because you don’t know what these sources are (the usual case), or because you forgot to include them (this example). Doing so helps avoid spurious effects.</p>
<div id="bonus-thinking-through-model-predictions" class="section level4 unnumbered">
<h4>Bonus: Thinking through model predictions</h4>
<p>Thinking through mixed-model predictions is confusing at first, but very useful. Once you understand “simple” models like this one, it isn’t hard to generalize to more complex models.</p>
<p>A couple examples of the predictions the model makes:</p>
<ol style="list-style-type: decimal">
<li>Model prediction for an “average subject and average item”:
<span class="math display">\[\begin{align*}
  y_i &amp;= \hat{\beta}_0 + \hat{\beta}_{\texttt{clabel}} \cdot \texttt{clabel}_i + \hat{\beta}_{\texttt{npType}} \cdot \texttt{npType}_i + \\
  &amp;\hphantom{{}={======}} \hat{\beta}_{\texttt{order}} \cdot \texttt{order}_i + \hat{\beta}_{\texttt{clabel:npType}} \cdot \texttt{clabel}_i \texttt{npType}_i
\end{align*}\]</span></li>
</ol>
<p>(Where <code>clabel</code> is an abbreviation for <code>conditionLabel</code>.)</p>
<p>This looks exactly like you’d expect given the model’s fixed effects formula (<code>clabel + npType + order + voice + clabel:npType</code>), except that there is no term for <code>voice</code>. This is because <code>voice</code> is an item-level predictor, so for an “average item”, there is no voice effect—the average is over both <em>active</em> and <em>passive</em> voice items.</p>
<ol start="2" style="list-style-type: decimal">
<li>Model prediction for Subject 5, item 2 (where <code>voice</code> = passive for this item):</li>
</ol>
<span class="math display">\[\begin{align*}
  y_i &amp;= \left( \hat{\beta}_0 + \alpha_5 + \delta_2 + 0.5 \cdot \hat{\beta}_{\texttt{voice}} \right) + \\
  &amp;\hphantom{{}={======}} \hat{\beta}_{\texttt{clabel}} \cdot \texttt{label}_i + \hat{\beta}_{\texttt{npType}} \cdot \texttt{npType}_i + \hat{\beta}_{\texttt{order}} \cdot \texttt{order}_i + \\
  &amp;\hphantom{{}={======}} \hat{\beta}_{\texttt{clabel:npType}} \cdot \texttt{clabel}_i \texttt{npType}_i
\end{align*}\]</span>
<p>In this equation, the “intercept” is: <span class="math display">\[
\left( \hat{\beta}_0 + \alpha_5 + \delta_2 + 0.5 \cdot \hat{\beta}_{\texttt{voice}} \right)
\]</span> this is the <strong>predicted mean value for this participant and item</strong>: the grand mean, with offsets for this participant, this item, and this <code>voice</code> value.</p>
<p>Interpretation aside: because <code>voice</code> is item-level, the by-item random intercept is not the offset from the grand mean. It is the offset of item 2 among <code>voice</code> = passive items.</p>
</div>
</div>
<div id="c6model3B" class="section level3">
<h3><span class="header-section-number">7.6.3</span> Model 3B: Random intercepts and all possible random slopes</h3>
<p><a href="lmem.html#c6model3A">Model 3A</a> had random intercepts (by-participant and by-item) only. We now add “all possible random slopes”. What this means is:</p>
<ul>
<li><p>By-participant random slopes for all predictors that vary within participants (i.e. not participant-level predictors)</p></li>
<li><p>By-item random slopes for all predictors that vary within items (i.e. not item-level predictors)</p></li>
</ul>
<p>For example, suppose that the model included a predictor for participant gender. This predictor estimates the difference between male and female participants. The effect of this predictor cannot vary within participants—each participant has only one value of gender, so it doesn’t make sense to estimate “difference between male and female for participant 3”. Thus, there can be no by-participant random slope term. More generally, there can be no by-participant random slopes for participant-level predictors.</p>
<p>By similar logic, there can be no by-item random slopes for item-level predictors.</p>
<p>Thus, the current model could include:</p>
<ul>
<li><p>By-participant random slopes for all predictors</p></li>
<li><p>By-item random slopes for all predictors <strong>except <code>voice</code></strong>.</p></li>
</ul>
<p>However, for simplicity we leave out the random slope for <code>order</code>. The fixed-effect term is not significant, and we don’t care about the <span class="math inline">\(\hat{\beta}_{\texttt{order}}\)</span> estimate anyway. (We will <a href="lmem.html#adding-a-random-slope">discuss motivation</a> for which random slope terms to include, soon.)</p>
<p>Technical note: we are again using “uncorrelated” random effects, denoted by the double-pipe (<code>||</code>) symbol. (We will discuss what this means <a href="lmem.html#random-effect-correlations">soon</a> as well.)</p>
<p>To fit the model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Model 3B
mod3b &lt;-<span class="st"> </span><span class="kw">lmer</span>(acoustics ~<span class="st"> </span>conditionLabel.williams*npType.pron +<span class="st"> </span>voice.passive +<span class="st"> </span>order.std +
<span class="st">                     </span>(<span class="dv">1</span> +<span class="st"> </span>conditionLabel.williams*npType.pron +<span class="st"> </span>voice.passive ||<span class="st"> </span>participant)  +
<span class="st">                     </span>(<span class="dv">1</span> +<span class="st"> </span>conditionLabel.williams*npType.pron ||<span class="st"> </span>item), <span class="dt">data=</span>givenness)</code></pre></div>
<p>In the model formula:</p>
<ul>
<li><p><code>(1 + conditionLabel.williams*npType.pron + voice.passive || participant)</code> are the by-participant random effects</p></li>
<li><p><code>(1 + conditionLabel.williams*npType.pron || item)</code> are the by-item random effects.</p></li>
</ul>
<p>The model output is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(mod3b)</code></pre></div>
<pre><code>## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [
## lmerModLmerTest]
## Formula: 
## acoustics ~ conditionLabel.williams * npType.pron + voice.passive +  
##     order.std + (1 + conditionLabel.williams * npType.pron +  
##     voice.passive || participant) + (1 + conditionLabel.williams *  
##     npType.pron || item)
##    Data: givenness
## 
## REML criterion at convergence: 722.6
## 
## Scaled residuals: 
##      Min       1Q   Median       3Q      Max 
## -2.97005 -0.61342 -0.00089  0.55381  2.98506 
## 
## Random effects:
##  Groups        Name                                Variance Std.Dev.
##  participant   (Intercept)                         0.11589  0.3404  
##  participant.1 conditionLabel.williams             0.01278  0.1131  
##  participant.2 npType.pron                         0.01298  0.1139  
##  participant.3 voice.passive                       0.08281  0.2878  
##  participant.4 conditionLabel.williams:npType.pron 0.03131  0.1770  
##  item          (Intercept)                         0.04089  0.2022  
##  item.1        conditionLabel.williams             0.00000  0.0000  
##  item.2        npType.pron                         0.20854  0.4567  
##  item.3        conditionLabel.williams:npType.pron 0.00000  0.0000  
##  Residual                                          0.26762  0.5173  
## Number of obs: 382, groups:  participant, 27; item, 16
## 
## Fixed effects:
##                                     Estimate Std. Error       df t value
## (Intercept)                         -0.71909    0.08702 29.69336  -8.263
## conditionLabel.williams              0.30897    0.05894 19.87033   5.242
## npType.pron                          0.78377    0.12831 15.26567   6.108
## voice.passive                        0.06037    0.12802 16.52472   0.472
## order.std                           -0.12414    0.10625 18.30847  -1.168
## conditionLabel.williams:npType.pron  0.31522    0.11341 21.95021   2.780
##                                     Pr(&gt;|t|)    
## (Intercept)                         3.45e-09 ***
## conditionLabel.williams             4.03e-05 ***
## npType.pron                         1.85e-05 ***
## voice.passive                         0.6434    
## order.std                             0.2576    
## conditionLabel.williams:npType.pron   0.0109 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Correlation of Fixed Effects:
##             (Intr) cndtL. npTyp. vc.pss ordr.s
## cndtnLbl.wl  0.002                            
## npType.pron  0.000 -0.005                     
## voice.passv  0.008  0.004 -0.009              
## order.std   -0.004  0.019 -0.013  0.097       
## cndtnLb.:T. -0.005  0.001  0.002  0.003 -0.019</code></pre>
<p>Compare to the results of Model 3A:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(mod3a)</code></pre></div>
<pre><code>## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [
## lmerModLmerTest]
## Formula: 
## acoustics ~ conditionLabel.williams * npType.pron + voice.passive +  
##     order.std + (1 | participant) + (1 | item)
##    Data: givenness
## 
## REML criterion at convergence: 758.6
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.7638 -0.6535 -0.0102  0.5750  3.5847 
## 
## Random effects:
##  Groups      Name        Variance Std.Dev.
##  participant (Intercept) 0.11624  0.3409  
##  item        (Intercept) 0.03825  0.1956  
##  Residual                0.34715  0.5892  
## Number of obs: 382, groups:  participant, 27; item, 16
## 
## Fixed effects:
##                                      Estimate Std. Error        df t value
## (Intercept)                          -0.71517    0.08728  29.38442  -8.194
## conditionLabel.williams               0.32769    0.06074 338.37005   5.395
## npType.pron                           0.77529    0.06072 338.22707  12.768
## voice.passive                         0.05087    0.11572  12.11117   0.440
## order.std                            -0.12730    0.10819  17.79197  -1.177
## conditionLabel.williams:npType.pron   0.31916    0.12126 337.32828   2.632
##                                     Pr(&gt;|t|)    
## (Intercept)                         4.45e-09 ***
## conditionLabel.williams             1.29e-07 ***
## npType.pron                          &lt; 2e-16 ***
## voice.passive                        0.66795    
## order.std                            0.25483    
## conditionLabel.williams:npType.pron  0.00888 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Correlation of Fixed Effects:
##             (Intr) cndtL. npTyp. vc.pss ordr.s
## cndtnLbl.wl  0.001                            
## npType.pron  0.001 -0.015                     
## voice.passv  0.009  0.005 -0.023              
## order.std   -0.004  0.015 -0.032  0.107       
## cndtnLb.:T. -0.006 -0.004  0.003  0.003 -0.021</code></pre>
<p>Comparing the model with random slopes (3B) to the model without random slopes (3A):</p>
<ul>
<li><p>Fixed-effect coefficient values are similar</p></li>
<li><p><span class="math inline">\(t\)</span> values for coefficients which were significant (<span class="math inline">\(|t| &gt;&gt; 2\)</span>) are still high.</p></li>
</ul>
<p>This means there are solid overall effects (of <code>conditionLabel</code>, <code>npType</code>, and their interaction), after accounting for between-item and between-participant variability.</p>
<p>Note that the <span class="math inline">\(t\)</span> value for <code>npType</code> is much lower in Model 3B (6.1 versus 12.8)—though still highly significant. This lower <span class="math inline">\(t\)</span> value is OK, and in fact a good thing, as explained in Sec. <a href="lmem.html#more-on-random-slopes">7.7</a>.</p>
</div>
<div id="assessing-variability" class="section level3">
<h3><span class="header-section-number">7.6.4</span> Assessing variability</h3>
<p>Random slopes capture variability among participants or items in the size of an effect. Thus, we can test whether participants/items significantly differ in an effect by performing model comparison of models with and without the random slope term.<br />
<!-- (This is needed for Mini Project 2.) --></p>
<p>The random slope variances for <code>conditionLabel.williams</code> in Model 3B are:</p>
<ul>
<li><p>By-participant: 0.0127849</p></li>
<li><p>By-item: 0</p></li>
</ul>
<p>These are both very small (corresponding to <span class="math inline">\(\sigma =\)</span> 0.1130703 and 0), compared to the fixed effect coefficient for <code>conditionLabel.williams</code>—in fact, no variability among items is detected. But let’s check whether there is significant by-participant and by-item variability anyway, as an example.</p>
<p>By participant:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## model comparisons to check whether conditionLabel.williams variability 
## by-participant

mod3b.no.partic.slope &lt;-<span class="st"> </span><span class="kw">update</span>(mod3b, . ~<span class="st"> </span>. -<span class="st"> </span>(<span class="dv">0</span>+conditionLabel.williams|participant))
<span class="kw">anova</span>(mod3b, mod3b.no.partic.slope)</code></pre></div>
<pre><code>## refitting model(s) with ML (instead of REML)</code></pre>
<pre><code>## Data: givenness
## Models:
## mod3b.no.partic.slope: acoustics ~ conditionLabel.williams + npType.pron + voice.passive + 
## mod3b.no.partic.slope:     order.std + (1 | participant) + (0 + npType.pron | participant) + 
## mod3b.no.partic.slope:     (0 + voice.passive | participant) + (0 + conditionLabel.williams:npType.pron | 
## mod3b.no.partic.slope:     participant) + (1 | item) + (0 + conditionLabel.williams | 
## mod3b.no.partic.slope:     item) + (0 + npType.pron | item) + (0 + conditionLabel.williams:npType.pron | 
## mod3b.no.partic.slope:     item) + conditionLabel.williams:npType.pron
## mod3b: acoustics ~ conditionLabel.williams * npType.pron + voice.passive + 
## mod3b:     order.std + (1 + conditionLabel.williams * npType.pron + 
## mod3b:     voice.passive || participant) + (1 + conditionLabel.williams * 
## mod3b:     npType.pron || item)
##                       Df   AIC    BIC  logLik deviance  Chisq Chi Df
## mod3b.no.partic.slope 15 735.9 795.08 -352.95    705.9              
## mod3b                 16 737.8 800.92 -352.90    705.8 0.0993      1
##                       Pr(&gt;Chisq)
## mod3b.no.partic.slope           
## mod3b                     0.7527</code></pre>
<p>Thus, there is no significant by-participant variability in the Williams effect (<span class="math inline">\(\chi^2(1)=0.09\)</span>, <span class="math inline">\(p=0.75\)</span>)</p>
<p>By item:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## by-item
mod3b.no.item.slope &lt;-<span class="st"> </span><span class="kw">update</span>(mod3b, . ~<span class="st"> </span>. -<span class="st"> </span>(<span class="dv">0</span>+conditionLabel.williams|item))
<span class="kw">anova</span>(mod3b, mod3b.no.item.slope)</code></pre></div>
<pre><code>## refitting model(s) with ML (instead of REML)</code></pre>
<pre><code>## Data: givenness
## Models:
## mod3b.no.item.slope: acoustics ~ conditionLabel.williams + npType.pron + voice.passive + 
## mod3b.no.item.slope:     order.std + (1 | participant) + (0 + conditionLabel.williams | 
## mod3b.no.item.slope:     participant) + (0 + npType.pron | participant) + (0 + voice.passive | 
## mod3b.no.item.slope:     participant) + (0 + conditionLabel.williams:npType.pron | 
## mod3b.no.item.slope:     participant) + (1 | item) + (0 + npType.pron | item) + (0 + 
## mod3b.no.item.slope:     conditionLabel.williams:npType.pron | item) + conditionLabel.williams:npType.pron
## mod3b: acoustics ~ conditionLabel.williams * npType.pron + voice.passive + 
## mod3b:     order.std + (1 + conditionLabel.williams * npType.pron + 
## mod3b:     voice.passive || participant) + (1 + conditionLabel.williams * 
## mod3b:     npType.pron || item)
##                     Df   AIC    BIC logLik deviance Chisq Chi Df
## mod3b.no.item.slope 15 735.8 794.98 -352.9    705.8             
## mod3b               16 737.8 800.92 -352.9    705.8     0      1
##                     Pr(&gt;Chisq)
## mod3b.no.item.slope           
## mod3b                        1</code></pre>
<p>So there is also no significant by-item variability in the Williams effect (<span class="math inline">\(\chi^2(1)=0\)</span>, <span class="math inline">\(p=1\)</span>)</p>
<p>For the sake of this example, let’s calculate the predicted Williams effect (<code>conditionLabel.williams</code> slope) for each participant anyway:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## examine distibution of participant clabel.wiliams coefficents:
participantRanefs &lt;-<span class="st"> </span><span class="kw">ranef</span>(mod3b)$participant

## fixed effect for conditionLabel
beta &lt;-<span class="st"> </span><span class="kw">fixef</span>(mod3b)[[<span class="st">&#39;conditionLabel.williams&#39;</span>]]

## each participant&#39;s offset 
offsets &lt;-<span class="st"> </span>participantRanefs$conditionLabel.williams

participantEffects &lt;-<span class="st"> </span>(beta +<span class="st"> </span>offsets)

<span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>participantEffects), <span class="dt">data=</span><span class="kw">data.frame</span>(<span class="dt">participantEffects=</span>participantEffects)) +
<span class="st">  </span><span class="kw">geom_histogram</span>() +<span class="st"> </span>
<span class="st">  </span><span class="kw">xlim</span>(<span class="dv">0</span>,<span class="fl">0.4</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="kw">aes</span>(<span class="dt">xintercept=</span><span class="dv">0</span>),<span class="dt">lty=</span><span class="dv">2</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="kw">aes</span>(<span class="dt">xintercept=</span>beta),<span class="dt">color=</span><span class="st">&#39;red&#39;</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Predicted participant Williams Effect coeffs&quot;</span>)</code></pre></div>
<p><img src="07-linear-mixed-models_files/figure-html/unnamed-chunk-36-1.png" width="480" style="display: block; margin: auto;" /> In this plot, the red line is the fixed effect coefficient value—the overall effect, across participants—and the dotted line is at 0. Thus, there is a clear Williams effect (red line far from zero), but minor by-participant variability. Participants differ in the <strong>magnitude</strong> but not <strong>direction</strong> of the Williams effect.</p>
</div>
</div>
<div id="more-on-random-slopes" class="section level2">
<h2><span class="header-section-number">7.7</span> More on random slopes</h2>
<p>Random slopes are crucial to using mixed models, but can be confusing to use and interpret.</p>
<div id="what-does-adding-a-random-slope-term-do" class="section level3">
<h3><span class="header-section-number">7.7.1</span> What does adding a random slope term do?</h3>
<p>To see why random slopes are so important, let’s consider an example where (unlike Model 3B) there is significant by-participant variability in an effect: the <code>halfrhyme</code> example used in <a href="lmem.html#c6model1c">Model 1C</a>.</p>
<p>As a reminder, this model predicts <code>rhymeRating</code> with a fixed effect of <code>relDuration</code>, and a by-participant random intercept and random slope of <code>relDuration</code>.</p>
<p>Let Model 1D be the same model, without the random slope:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Model 1D: half-rhyme data model of 1 var, with by-partic random intercept only
mod1d &lt;-<span class="st"> </span><span class="kw">lmer</span>(rhymeRating ~<span class="st"> </span>relDuration +<span class="st"> </span>(<span class="dv">1</span> |<span class="st"> </span>participant), <span class="dt">data=</span>halfrhyme)</code></pre></div>
<p>To get a sense of what a random slope does, we compare the two models:</p>
<ul>
<li><p>Model 1D: By-subject <strong>random intercept</strong> (only)</p></li>
<li><p>Model 1C: By-subject <strong>random intercept</strong>, <strong>random slope</strong> of <code>relDuration</code></p></li>
</ul>
<p>(We ignore by-item variability.)</p>
<p>The intercept-only model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(mod1d)</code></pre></div>
<pre><code>## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [
## lmerModLmerTest]
## Formula: rhymeRating ~ relDuration + (1 | participant)
##    Data: halfrhyme
## 
## REML criterion at convergence: 2580
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.4624 -0.6545 -0.1025  0.6348  3.6438 
## 
## Random effects:
##  Groups      Name        Variance Std.Dev.
##  participant (Intercept) 1.222    1.106   
##  Residual                1.575    1.255   
## Number of obs: 756, groups:  participant, 31
## 
## Fixed effects:
##             Estimate Std. Error       df t value Pr(&gt;|t|)    
## (Intercept)   3.2918     0.2088  33.0815  15.765  &lt; 2e-16 ***
## relDuration   2.6600     0.6703 728.5548   3.968 7.96e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Correlation of Fixed Effects:
##             (Intr)
## relDuration 0.219</code></pre>
<p>models <code>rhymeRating</code> as a linear function of <code>relDuration</code>, with the <strong>same slope</strong> for every participant. Thus, every participant’s data is contributing to estimating one number: <span class="math inline">\(\beta_1\)</span>, the slope for <code>relDuration</code>. (Setting aside estimating the intercept terms: <span class="math inline">\(\beta_0\)</span> and random intercepts.)</p>
<p>The intercept + slope model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(mod1c)</code></pre></div>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: 
## rhymeRating ~ relDuration + ((1 | participant) + (0 + relDuration |  
##     participant))
##    Data: halfrhyme
## 
## REML criterion at convergence: 2578.7
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.4022 -0.6091 -0.1126  0.6138  3.5726 
## 
## Random effects:
##  Groups        Name        Variance Std.Dev.
##  participant   (Intercept) 1.267    1.126   
##  participant.1 relDuration 4.482    2.117   
##  Residual                  1.552    1.246   
## Number of obs: 756, groups:  participant, 31
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)   3.3003     0.2125  15.533
## relDuration   2.7249     0.7816   3.486
## 
## Correlation of Fixed Effects:
##             (Intr)
## relDuration 0.192</code></pre>
<p>models <code>rhymeRating</code> as a linear function of <code>relDuration</code>, with a <strong>different slope</strong> for every participant. Thus, every participant’s data is contributing to estimating two numbers: <span class="math inline">\(\beta_1\)</span>, and the offset of that participant’s slope from <span class="math inline">\(\beta_1\)</span>. Intuitively, this results in the model being less certain about the <strong>overall</strong> slope (<span class="math inline">\(\beta_1\)</span>).</p>
<p>We can see the result by comparing the <code>relDuration</code> fixed-effect rows for the two models:</p>
<table>
<thead>
<tr class="header">
<th>Model</th>
<th>Estimate</th>
<th>Std. Error</th>
<th><span class="math inline">\(t\)</span> value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Intercept-only</td>
<td>2.66</td>
<td>0.21</td>
<td>3.99</td>
</tr>
<tr class="even">
<td>Intercept + slope</td>
<td>2.72</td>
<td><strong>0.78</strong></td>
<td>3.49</td>
</tr>
</tbody>
</table>
<p>Both models show similar estimated slopes (<span class="math inline">\(\hat{\beta}_1\)</span>) for <code>relDuration</code>. But the intercept + slope model has a much larger standard error for this effect, resulting in a smaller <span class="math inline">\(t\)</span> value, and a less significant effect. (Recall that higher <span class="math inline">\(|t|\)</span> <span class="math inline">\(\implies\)</span> more significant.)</p>
</div>
<div id="adding-a-random-slope" class="section level3">
<h3><span class="header-section-number">7.7.2</span> Discussion: Adding a random slope</h3>
<p>It makes sense that the standard error of the <code>relDuration</code> fixed effect goes up when a random slope is added: the model detects significant variability among participants in the effect of <code>relDuration</code>, which makes it less sure of the <strong>overall</strong> effect of <code>relDuration</code>.</p>
<p>This is a good thing, from the perspective of not finding spurious effects. In general, participants (and items) will differ in the effect of a given predictor, <span class="math inline">\(X\)</span>. Thus, if a by-participant (or item) random slope for <span class="math inline">\(X\)</span> is not included, we are underestimating the uncertainty in the fixed effect for <span class="math inline">\(X\)</span>, and can easily falsely conclude there is a significant effect—for example, based on a subset of participants who show large effects. In general, not including a random slope for <span class="math inline">\(X\)</span> is anti-conservative (for evaluating whether the fixed-effect coefficient of <span class="math inline">\(X\)</span> is 0)—it <strong>increases Type I error</strong>.</p>
<p>Similar logic holds for by-item random slopes, and so on: whenever the effect of a predictor <span class="math inline">\(X\)</span> could vary among levels of a grouping factor <span class="math inline">\(Z\)</span>, it is anti-conservative to not include a by-<span class="math inline">\(Z\)</span> random slope for <span class="math inline">\(X\)</span>. This issue is discussed at length by <span class="citation">Barr et al. (<a href="#ref-barr2013random">2013</a>)</span>.</p>
<p>On the other hand, adding a random slope for <span class="math inline">\(X\)</span> also <strong>increases Type II error</strong>—that is, lowers power to detect a non-zero (fixed) effect of <span class="math inline">\(X\)</span>. This is especially true if there is little by-participant variability in the effect of <span class="math inline">\(X\)</span>. This issue is emphasized by <span class="citation">Matuschek, Kliegl, Vasishth, Baayen, &amp; Bates (<a href="#ref-matuschek2017balancing">2017</a>)</span> and <span class="citation">D. Bates, Kliegl, Vasishth, &amp; Baayen (<a href="#ref-bates2015parsimonious">2015</a>)</span>.</p>
<p>These points seem to imply that adding random slopes can be both good (lower Type I error) and bad (higher Type II error). What practical advice can be given on when to include a given random slope term? The more general issue is: how do we decide on a random effect structure? This usually means, what random slope terms (and correlation terms—see below) should we include in our model?</p>
<p>This is a <strong>model selection</strong> problem (with respect to random effect terms)—and as we saw when discussing model selection for fixed effects in the context of multiple linear regression (Sec. <a href="linear-regression.html#lm-model-comparison">3.5</a>), there is no “best” answer. What model selection procedure you use depends on the goals of your analysis. There are two broad perspectives:</p>
<ul>
<li><p><strong>Perspective 1</strong> (<span class="citation">Barr et al. (<a href="#ref-barr2013random">2013</a>)</span> advice):<a href="#fn38" class="footnoteRef" id="fnref38"><sup>38</sup></a> include random slope(s) for any fixed effect coefficient you care about</p>
<ul>
<li><p>Ideal: “Maximal” random effect structure, meaning all possible random effect terms (modulo issues with model convergence)</p></li>
<li><p>Guards against <em>Type 1 errors</em></p></li>
</ul></li>
<li><p><strong>Perspective 2</strong> (<span class="citation">D. Bates et al. (<a href="#ref-bates2014fitting">2014</a>)</span> advice): only include random slopes that contribute significantly to model likelihood, using a likelihood ratio test</p>
<ul>
<li><p>(a.k.a. random effect terms “justified by the data”)</p></li>
<li><p>Guards against <em>Type II errors</em></p></li>
</ul></li>
</ul>
<p>Type I and Type II error always trade off in model selection. Thus, there is no “correct” answer—it depends on whether you care more about Type I and Type II error. No consensus exists on how to arrive at a random-effect structure, and this is a major issue for users fitting these models in practice, since often the “maximal” random effect structure is too complex for the data, and leads to fitting issues.</p>
<p>Still, a few guidelines can be given:</p>
<ol style="list-style-type: decimal">
<li><p>It is crucial to <strong>consider</strong> random slope terms for all fixed effects of interest for your research questions—either by including them in the model (Perspective 1), or testing whether they should be added (Perspective 2). Otherwise, you run the risk of seriously inflated <span class="math inline">\(p\)</span>-values.</p></li>
<li><p>If a random slope for an interaction is included, it is crucial to include the corresponding random slopes for all subsets of the interaction—for the same reason that you must include all subset of an interaction as fixed effects. (For example, a by-participant <code>X:Y</code> random slope <span class="math inline">\(\implies\)</span> by-participant <code>X</code> and <code>Y</code> random slopes must be included.)</p></li>
<li><p>It is not as important to consider random slope terms for fixed effects not of interest, such as those included as controls.</p></li>
<li><p>It is not as important to consider random slope terms for fixed effects which do not reach significance in an intercepts-only model. Adding these random slopes will often have little effect on estimates of other fixed effects.</p></li>
<li><p>Only so many random slope terms can be properly estimated, given the size and structure of your dataset. Thus, it’s important to prioritize random slopes you <strong>must</strong> consider (#1 and #2).</p></li>
</ol>
<div id="extended-exercise-lmm-with-random-slopes" class="section level4 unnumbered">
<h4>Extended exercise: LMM with random slopes</h4>
<p>See the Appendix, Sec. <a href="lmem.html#c6extendedexercise">7.13</a>.</p>
</div>
</div>
</div>
<div id="random-effect-correlations" class="section level2">
<h2><span class="header-section-number">7.8</span> Random effect correlations</h2>
<p>In examples so far, we have always used “uncorrelated” random effects, written using the <code>||</code> notation in <code>lme4</code>. For example, in Model 1C, the random effect term is <code>(1 + relDuration.std || participant)</code>.</p>
<p>Uncorrelated random effects assume that there is no relationship between different random-effect terms for the same grouping factor. In Model 1C, it is assumed that there is no relationship between</p>
<ol style="list-style-type: decimal">
<li><p>a participant’s offset from the overall intercept (random intercept)</p></li>
<li><p>a participant’s offset from the overall slope of <code>relDuration</code> (random slope)</p></li>
</ol>
<p>If there were a relationship between (1) and (2), it would take the form of a positive or negative <strong>correlation</strong>—hence the name “uncorrelated random effects”.</p>
<blockquote>
<p><strong>Questions</strong>:</p>
<ul>
<li>What would it mean, intuitively, if there were a strong positive relationship (“positive correlation”) between (1) and (2)? (“Participants who ____ also have a higher ____.”)</li>
</ul>
</blockquote>
<p>To see whether this assumption is realistic, we can examine the prediction of Model 1C for each subject:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## model predictions by-subject for Model 1C:
newdata &lt;-<span class="st"> </span><span class="kw">data.frame</span>(
    <span class="kw">expand.grid</span>(<span class="dt">relDuration=</span><span class="kw">seq</span>(
      <span class="kw">min</span>(halfrhyme$relDuration), <span class="kw">max</span>(halfrhyme$relDuration), <span class="dt">by=</span><span class="fl">0.01</span>),
                <span class="dt">participant=</span><span class="kw">unique</span>(halfrhyme$participant)))

## get the predicted value for each case
newdata$pred &lt;-<span class="st"> </span><span class="kw">predict</span>(mod1c, <span class="dt">newdata=</span>newdata)

## plot the model&#39;s prediction for each participant:
<span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>relDuration, <span class="dt">y=</span>pred), <span class="dt">data=</span>newdata) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x=</span>relDuration,<span class="dt">y=</span>pred, <span class="dt">group=</span>participant)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;relDuration&quot;</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Predicted rhymeRating&quot;</span>)</code></pre></div>
<p><img src="07-linear-mixed-models_files/figure-html/unnamed-chunk-40-1.png" width="384" style="display: block; margin: auto;" /></p>
<p>A relationship between the height of a line (= random intercept) and its slope (= random slope) doesn’t jump out, but we can check this more carefully by plotting the relationship between each participant’s offsets for the intercept and the <code>relDuration</code> slope:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod1c.ranefs &lt;-<span class="st"> </span><span class="kw">ranef</span>(mod1c)$participant
<span class="kw">pairscor.fnc</span>(mod1c.ranefs)</code></pre></div>
<p><img src="07-linear-mixed-models_files/figure-html/unnamed-chunk-41-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>This plot suggests there may be a weak positive correlation (<span class="math inline">\(r = 0.28, p = 0.13\)</span>): participants who rate half-rhymes better (higher <code>rhymeRating</code>) may also have a larger effect of the acoustic cue (<code>relDuration</code>) on their rating. Alternatively, this may be a spurious correlation, not significantly different from no correlation. We can test whether there’s evidence for a “real” correlation between participants’ slopes and intercepts by including this term in the mixed model.</p>
<div id="model-1e-correlated-random-slope-intercept" class="section level3">
<h3><span class="header-section-number">7.8.1</span> Model 1E: <strong>Correlated</strong> random slope &amp; intercept</h3>
<p>The regression model is now:</p>
<span class="math display">\[\begin{equation*}
  y_i = \beta_0 + \alpha_{j[i]} + \left(\beta_1 + \gamma_{j[i]}\right) x_i + \epsilon_i, \quad i = 1, ..., n
\end{equation*}\]</span>
<p>Where the random effects are:</p>
<ul>
<li><p><span class="math inline">\(\alpha_{j[i]}\)</span>: random intercept coefficient</p></li>
<li><p><span class="math inline">\(\gamma_{j[i]}\)</span>: <strong>random slope</strong> coefficient</p></li>
</ul>
<p>and the random intercept and slope follow a multivariate normal distribution:</p>
<span class="math display">\[\begin{equation*}
\begin{pmatrix}
  \alpha_j \\
  \gamma_j
\end{pmatrix} 
\sim
N \left( 
\begin{pmatrix}
  0 \\
  0
\end{pmatrix}, 
\begin{pmatrix}
  \sigma_{s,0} &amp; \rho \\
  \rho &amp; \sigma_{s,1}
\end{pmatrix}
\right), \quad j = 1, ..., J
\end{equation*}\]</span>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(\sigma_{s,0}\)</span> is the amount of by-subject variability in the intercept</p></li>
<li><p><span class="math inline">\(\sigma_{s,1}\)</span> is the amount of by-subject variability in the slope of <code>relDuration</code></p></li>
<li><p><span class="math inline">\(\rho\)</span> is the <strong>correlation between random intercept and random slope</strong>, across participants.</p></li>
</ul>
<p>The parameters estimated in this model are:</p>
<ol style="list-style-type: decimal">
<li><p>Two fixed-effect coefficients:</p>
<ul>
<li><p><span class="math inline">\(\beta_0\)</span></p></li>
<li><p><span class="math inline">\(\beta_1\)</span></p></li>
</ul></li>
<li><p>Four variance components:</p>
<ul>
<li><p>Random intercept: <span class="math inline">\(\sigma^2_{s,0}\)</span></p></li>
<li><p>Random slope (of <code>relDuration</code>): <span class="math inline">\(\sigma^2_{s,1}\)</span></p></li>
<li><p>Correlation between them: <span class="math inline">\(\rho\)</span></p></li>
<li><p>Residual error variance: <span class="math inline">\(\sigma^2_e\)</span></p></li>
</ul></li>
</ol>
<p><span class="math inline">\(\rho\)</span> is the only new parameter in this model, compared to Model 1C. The only difference between the two models is:</p>
<ul>
<li><p>Model 1C assumes that <span class="math inline">\(\rho=0\)</span></p></li>
<li><p>Model 1E fits <span class="math inline">\(\rho \in (-1,1)\)</span></p></li>
</ul>
<p>In R, the “single pipe” notation (<code>|</code>) is used for correlated random effects.</p>
<p>To fit Model 1E:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod1e &lt;-<span class="st"> </span><span class="kw">lmer</span>(rhymeRating ~<span class="st"> </span>relDuration +<span class="st"> </span>(<span class="dv">1</span> +<span class="st"> </span>relDuration |<span class="st"> </span>participant), <span class="dt">data=</span>halfrhyme)</code></pre></div>
<p>Note the syntax:</p>
<ul>
<li><p>Correlated random effects: <code>(1 + A + B | participant)</code></p></li>
<li><p>Uncorrelated random effects: <code>(1 + A + B || participant)</code></p></li>
</ul>
<p>Recall that the formula for Model 1C was:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod1c &lt;-<span class="st"> </span><span class="kw">lmer</span>(rhymeRating ~<span class="st"> </span>relDuration +<span class="st"> </span>(<span class="dv">1</span> +<span class="st"> </span>relDuration ||<span class="st"> </span>participant), <span class="dt">data=</span>halfrhyme)</code></pre></div>
<p>The new model’s output is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(mod1e)</code></pre></div>
<pre><code>## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [
## lmerModLmerTest]
## Formula: rhymeRating ~ relDuration + (1 + relDuration | participant)
##    Data: halfrhyme
## 
## REML criterion at convergence: 2571.9
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.4723 -0.6129 -0.1562  0.5897  3.4874 
## 
## Random effects:
##  Groups      Name        Variance Std.Dev. Corr
##  participant (Intercept) 1.496    1.223        
##              relDuration 5.728    2.393    0.82
##  Residual                1.548    1.244        
## Number of obs: 756, groups:  participant, 31
## 
## Fixed effects:
##             Estimate Std. Error      df t value Pr(&gt;|t|)    
## (Intercept)   3.3068     0.2291 30.4810  14.433 3.62e-15 ***
## relDuration   2.8161     0.8017 24.8569   3.513  0.00172 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Correlation of Fixed Effects:
##             (Intr)
## relDuration 0.591</code></pre>
<p>We see a positive correlation term under <code>Random effects</code>/<code>Corr</code> (<span class="math inline">\(\rho = 0.82\)</span>)—the direction guessed from the scatterplot of the Model 1C random effects.</p>
<p>We can repeat that plot using the random effects from Model 1E, to see the relationship predicted by this model between participant offsets for intercept and <code>relDuration</code> slope:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod1e.ranefs &lt;-<span class="st"> </span><span class="kw">ranef</span>(mod1e)$participant
<span class="kw">pairscor.fnc</span>(mod1e.ranefs)</code></pre></div>
<p><img src="07-linear-mixed-models_files/figure-html/unnamed-chunk-45-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>A strong relationship is predicted—compare to the plot for <a href="lmem.html#c6model1c">Model 1C</a>. It seems that the weak positive correlation observed in that plot hinted at a much stronger relationship that is detected by including a correlation term. To evaluate whether this correlation is significantly different from zero, we can do a model comparison:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(mod1c, mod1e)</code></pre></div>
<pre><code>## refitting model(s) with ML (instead of REML)</code></pre>
<pre><code>## Data: halfrhyme
## Models:
## mod1c: rhymeRating ~ relDuration + ((1 | participant) + (0 + relDuration | 
## mod1c:     participant))
## mod1e: rhymeRating ~ relDuration + (1 + relDuration | participant)
##       Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)   
## mod1c  5 2588.7 2611.8 -1289.3   2578.7                            
## mod1e  6 2583.7 2611.5 -1285.8   2571.7 6.9728      1   0.008276 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>There is a significant improvement in model likelihood (<span class="math inline">\(\chi^2(1) = 7.0\)</span>, <span class="math inline">\(p=0.008\)</span>), meaning there is a significant positive correlation between participants’ random intercepts and (<code>relDuration</code>) slopes.</p>
<p>Despite the correlation term, the fixed effects for the two models (with and without this term) are very similar:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## model 1C (no correlation)
<span class="kw">summary</span>(mod1c)$coefficients</code></pre></div>
<pre><code>##             Estimate Std. Error   t value
## (Intercept) 3.300261  0.2124740 15.532541
## relDuration 2.724922  0.7816163  3.486265</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## model 1E (correlation)
<span class="kw">summary</span>(mod1e)$coefficients</code></pre></div>
<pre><code>##             Estimate Std. Error       df   t value     Pr(&gt;|t|)
## (Intercept) 3.306768  0.2291117 30.48098 14.432996 3.624195e-15
## relDuration 2.816111  0.8016868 24.85693  3.512732 1.720269e-03</code></pre>
<p>in terms of both coefficient estimates and standard errors.</p>
<p>The variance components differ somewhat: <span class="math inline">\(\rho &gt; 0\)</span> for Model 1E (obviously), and in addition the <code>relDuration</code> random slope is larger.</p>
</div>
<div id="c6discuss" class="section level3">
<h3><span class="header-section-number">7.8.2</span> Dicussion: Adding a correlation</h3>
<p>Comparing Model 1C to Model 1E: the <strong>fixed-effect</strong> coefficients are very similar, while the <strong>random effects</strong> change to some extent. The model with a correlation term is slightly “better” in terms of likelihood (significant increase), as a result of change in the random effects.</p>
<p>Empirically, this often happens when random-effect correlation terms are added to a model, at least when all predictors have been centered and are orthogonal: adding correlation terms can lead to a significantly better model fit, but has little effect on the fixed-effect estimates, which are usually what we are interested in.</p>
<p>This observation turns out to matter a lot in practice, by the following logic. Including (or at least considering) random <strong>slope</strong> terms is very important, <a href="lmem.html#adding-a-random-slope">for reasons described above</a>. However, “maximal random effect structure” <span class="citation">(Barr et al., <a href="#ref-barr2013random">2013</a>)</span>—meaning all possible random slopes, and all possible correlations between random effect terms—often leads to a model that will not converge (or does so very slowly), because the number of terms to be estimated grows quadratically with the number of fixed-effect predictors included: (<span class="math inline">\(\frac{k(k+1)}{2}\)</span> random effect terms for <span class="math inline">\(k\)</span> predictors). For example, “maximal” random effect structure for five predictors gives 16 terms to be estimated: 1 intercept, 5 slopes, and <strong>10</strong> correlations. Usually, you do not have enough data to fit all these parameters—which are mostly correlation terms.<a href="#fn39" class="footnoteRef" id="fnref39"><sup>39</sup></a> Researchers following the advice to construct “maximal” models typically use correlated random-effect structure, as this is R’s default (using <code>(1+X+Y+...|participant)</code> syntax). They often find that in practice these models do not converge, or have unrealistic correlation parameter estimates (near 1 or <span class="math inline">\(-1\)</span>), and assume that the problem has to do with adding random <strong>slopes</strong>. In our experience, this is usually not the case: the model is probably too complex to support the many <strong>correlations</strong> between random-effect terms, which are less important than random slopes. Even if the maximal model does fit (and doesn’t have unrealistic correlation estimates), it may do so extremely slowly, hindering the data analysis process which inevitably involves fitting several models.</p>
<p>To deal with these issues, it is often useful to <strong>first try models with uncorrelated random effects</strong>—including slopes, but no correlations. You can then add in correlations as needed (e.g. significantly increases model likelihood), or fit a final “full” model with all random slopes if your data supports it. In practice, models with all possible random slopes but without correlation terms usually <strong>do</strong> converge, and fit fairly quickly. (For example, a model with five predictors would have just 6 random effect terms in the “maximal” model without correlations.)</p>
<p>Note that this is all the personal opinion of one author (Morgan). Although these issues sound esoteric, modeling issues (non-convergence or degenerate random effects: correlation terms near 1 or -1) due to complex random-effects structure is one of the most frequent issues that researchers have with using mixed models in practice for linguistic data.</p>
</div>
</div>
<div id="model-criticism-for-linear-mixed-models" class="section level2">
<h2><span class="header-section-number">7.9</span> Model criticism for linear mixed models</h2>
<p>So far, we have neglected performing model criticism when fitting linear mixed models.<br />
<!-- (If you are taking LING 620: you don't need to do model criticism for mini-project 2.)  --> However, model criticism is just as important for linear mixed models as for (non-mixed) linear regressions, we just don’t go into it in detail here because many aspects of model criticism for LMMs are similar to <a href="linear-regression.html#linear-regression-assumptions">the linear regression case</a>.</p>
<ul>
<li><p>Checking for outliers (in predictors, response)</p></li>
<li><p>Residual plots: model predictions versus residuals, QQ plot, fixed-effect predictors versus residuals.</p></li>
</ul>
<p>A new type of model criticism involves <strong>random effects</strong>: each random-effect term is assumed to be normally distributed, and whether this assumption is (roughly) correct should be checked.</p>
<div id="model-3b-residual-plots" class="section level3">
<h3><span class="header-section-number">7.9.1</span> Model 3B: Residual plots</h3>
<p>The logic of checking residual plots for an LMM is similar to linear regressions. For example, for normality of residuals: the model assumes that the residuals are normally distributed:</p>
<p><span class="math display">\[
\epsilon_i \sim N(0, \sigma^2_e)
\]</span></p>
<p><strong>If</strong> this is true for an LMM, then the <em>Pearson residuals</em> of the model are normally distributed (calculated in R using <code>residuals(myMixedModel)</code>). So we can check a QQ plot to see if the residuals are not normally distributed (which would mean the assumption is false):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## QQ plot for Model 3B
<span class="kw">qqnorm</span>(<span class="kw">residuals</span>(mod3b))
<span class="kw">qqline</span>(<span class="kw">residuals</span>(mod3b))</code></pre></div>
<p><img src="07-linear-mixed-models_files/figure-html/unnamed-chunk-48-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>The distribution of (Pearson) residuals looks normal, except for some points which should be flagged as potential outliers.</p>
<p>(It turns out that removing these outliers <strong>strengthens</strong> all “significant” effects—those with <span class="math inline">\(t &gt; 2\)</span>.)</p>
<p>We can also plot residuals versus fitted values (<code>fitted(mod3b)</code>) or predictor values—similar diagnostics to the same plots for linear regressions.</p>
</div>
<div id="model-3b-random-effect-distribution" class="section level3">
<h3><span class="header-section-number">7.9.2</span> Model 3B: Random effect distribution</h3>
<p>A key assumption of mixed models is that random effects are drawn from a particular distribution. For our purposes, it is always assumed that the random effects are drawn from a <strong>normal</strong> distribution, with mean 0 and some variance.</p>
<p>We can check this assumption by examining the distribution of either the empirical data (empirical participant means or item means), or the actual random-effect estimates (the BLUPs). Like checking residual plots, checking normality of random effects has benefits besides making sure your model is OK—it also often shows unusual participants or items, who should be investigated further.</p>
<p>To examine empirical participant and item means for <code>acoustics</code> for the <code>givenness</code> data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">givennessByPartic &lt;-givenness %&gt;%<span class="st"> </span><span class="kw">group_by</span>(participant) %&gt;%<span class="st"> </span><span class="kw">summarise</span>(<span class="dt">mean=</span><span class="kw">mean</span>(acoustics))
givennessByItem &lt;-givenness %&gt;%<span class="st"> </span><span class="kw">group_by</span>(item) %&gt;%<span class="st"> </span><span class="kw">summarise</span>(<span class="dt">mean=</span><span class="kw">mean</span>(acoustics))

<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
<span class="kw">qqnorm</span>(givennessByPartic$mean,  <span class="dt">main =</span> <span class="st">&quot;participant means&quot;</span>)
<span class="kw">qqline</span>(givennessByPartic$mean)

<span class="kw">qqnorm</span>(givennessByItem$mean,  <span class="dt">main =</span> <span class="st">&quot;item means&quot;</span>)
<span class="kw">qqline</span>(givennessByItem$mean)</code></pre></div>
<p><img src="07-linear-mixed-models_files/figure-html/unnamed-chunk-49-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>These distributions look basically normal, possibly with a weird participant or two (left plot).</p>
<p>We can also examine Q-Q plots for the by-participant and by-item random intercepts:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))

givennessByParticRanef &lt;-<span class="st"> </span><span class="kw">ranef</span>(mod3b)$participant[[<span class="st">&#39;(Intercept)&#39;</span>]]
<span class="kw">qqnorm</span>(givennessByParticRanef,  <span class="dt">main =</span> <span class="st">&quot;participant means&quot;</span>)
<span class="kw">qqline</span>(givennessByParticRanef)

givennessByItemRanef &lt;-<span class="st"> </span><span class="kw">ranef</span>(mod3b)$item[[<span class="st">&#39;(Intercept)&#39;</span>]]
<span class="kw">qqnorm</span>(givennessByItemRanef,  <span class="dt">main =</span> <span class="st">&quot;item means&quot;</span>)
<span class="kw">qqline</span>(givennessByItemRanef)</code></pre></div>
<p><img src="07-linear-mixed-models_files/figure-html/unnamed-chunk-50-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Again, there do not seem to be any particularly odd participants or items, though a couple participants may warrant a closer look.</p>
<p><strong>Detour</strong>: What would serious non-normality of random effects look like? As an example, let’s add three weird participants to dataset, who have <strong>positive</strong> mean value of acoustics (opposite of other participants), and examine a Q-Q plot for participant means:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
<span class="co"># find participant summary stats to generate weird participant</span>
n_weird &lt;-<span class="st"> </span><span class="dv">3</span>
n_obs &lt;-<span class="st"> </span><span class="kw">floor</span>(<span class="kw">nrow</span>(givenness)/<span class="kw">length</span>(<span class="kw">unique</span>(givenness$participant))) <span class="co"># avg number of observations</span>
mn_acoustics &lt;-<span class="st"> </span><span class="kw">mean</span>(givenness$acoustics)
sd_acoustics &lt;-<span class="st"> </span><span class="kw">sd</span>(givenness$acoustics)
<span class="co"># generate weird participants</span>
<span class="kw">set.seed</span>(<span class="dv">2903</span>)
acoustics_w &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n_weird *<span class="st"> </span>n_obs, <span class="dt">mean =</span> -mn_acoustics, <span class="dt">sd =</span> sd_acoustics)
participant_w &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="kw">paste0</span>(<span class="st">&quot;wp_&quot;</span>, <span class="kw">seq.int</span>(n_weird)), <span class="dt">each =</span> n_obs)
<span class="co"># place into data frame</span>
df_w &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">acoustics =</span> acoustics_w, <span class="dt">participant =</span> participant_w)
<span class="co"># merge with givenness data</span>
givenness_w &lt;-<span class="st"> </span><span class="kw">rbind</span>(dplyr::<span class="kw">select</span>(givenness, acoustics, participant), df_w)

givennessByPartic_w &lt;-<span class="st"> </span>givenness_w %&gt;%<span class="st"> </span><span class="kw">group_by</span>(participant) %&gt;%<span class="st"> </span><span class="kw">summarise</span>(<span class="dt">mean=</span><span class="kw">mean</span>(acoustics))

## make partiicpant means plot
<span class="kw">qqnorm</span>(givennessByPartic_w$mean, <span class="dt">main =</span> <span class="st">&quot;participant means&quot;</span>)
<span class="kw">qqline</span>(givennessByPartic_w$mean)</code></pre></div>
<p><img src="07-linear-mixed-models_files/figure-html/unnamed-chunk-51-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>There are now three outlier participants, visible at the top right of the plot. If a mixed model is fitted to this data, the estimated by-participant random intercept variance (<span class="math inline">\(\hat{\sigma}^2_{s,0}\)</span>) will be too high. (Because the model assumes random effects are normally distributed, the only way to account for a few outlier participants or items is to increase the random effect variance.)</p>
</div>
</div>
<div id="c6factorsissue" class="section level2">
<h2><span class="header-section-number">7.10</span> Random slopes for factors</h2>
<p>Technical issues, related to R’s implementation of factors, come up if you want to use factors as predictors in models with uncorrelated random effects. This is an important issue in practice if you’re building models with uncorrelated random effects. We advise skipping this section until this issue actually comes up in practice for you.</p>
<div id="model-with-random-effect-correlations" class="section level3">
<h3><span class="header-section-number">7.10.1</span> Model with random-effect correlations</h3>
<p>We have so far only used factors with two levels as predictors in mixed-effects models, which we’ve handled by coding the factor as a single numeric variable. We have not considered factors not converted to numeric predictors, including factors with <span class="math inline">\(&gt;2\)</span> levels. Using such factors in mixed models can be tricky, especially if you want to use uncorrelated random effects.</p>
<p>As an example, we will use the <code>halfrhyme</code> data. To load the data and code contrasts:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">halfrhyme &lt;-<span class="st"> </span><span class="kw">mutate</span>(halfrhyme,
                    <span class="dt">conditionLabel =</span> 
                      <span class="kw">factor</span>(conditionLabel, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;bad&quot;</span>, <span class="st">&quot;voice&quot;</span>, <span class="st">&quot;good&quot;</span>)))
<span class="kw">contrasts</span>(halfrhyme$conditionLabel) &lt;-<span class="st"> </span><span class="kw">contr.helmert</span>(<span class="dv">3</span>)</code></pre></div>
<p>Our example models rhyme rating as a function of experimental condition:</p>
<ul>
<li><p>Response: <code>rhymeRating</code></p></li>
<li><p>Fixed effect: <code>conditionLabel</code> (single predictor)</p>
<ul>
<li><p>Three levels: <em>bad</em>, <em>voice</em>, <em>good</em></p></li>
<li><p>Coded using two Helmert contrasts.</p></li>
</ul></li>
<li><p>Random effects: by-participant intercept and slope for <code>conditionLabel</code></p></li>
</ul>
<blockquote>
<p><strong>Questions</strong>:</p>
<ul>
<li>What is the interpretation of each contrast?</li>
</ul>
</blockquote>
<p>Suppose we fit the model with “maximal” random effects:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">modEx2 &lt;-<span class="st"> </span><span class="kw">lmer</span>(rhymeRating ~<span class="st"> </span>conditionLabel +<span class="st"> </span>(<span class="dv">1</span> +<span class="st"> </span>conditionLabel |<span class="st"> </span>participant), <span class="dt">data =</span> halfrhyme)
<span class="kw">summary</span>(modEx2)</code></pre></div>
<pre><code>## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [
## lmerModLmerTest]
## Formula: rhymeRating ~ conditionLabel + (1 + conditionLabel | participant)
##    Data: halfrhyme
## 
## REML criterion at convergence: 3826.5
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -4.6442 -0.4262 -0.0567  0.5333  3.8688 
## 
## Random effects:
##  Groups      Name            Variance Std.Dev. Corr       
##  participant (Intercept)     0.26067  0.5106              
##              conditionLabel1 0.20112  0.4485    0.91      
##              conditionLabel2 0.05675  0.2382   -0.73 -0.55
##  Residual                    1.25335  1.1195              
## Number of obs: 1205, groups:  participant, 31
## 
## Fixed effects:
##                 Estimate Std. Error       df t value Pr(&gt;|t|)    
## (Intercept)      3.63504    0.09918 30.04544  36.652  &lt; 2e-16 ***
## conditionLabel1  0.89985    0.09086 29.89488   9.904 5.94e-11 ***
## conditionLabel2  1.42441    0.05166 30.13975  27.573  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Correlation of Fixed Effects:
##             (Intr) cndtL1
## conditnLbl1  0.677       
## conditnLbl2 -0.511 -0.335</code></pre>
<p>The fixed-effect and random-effect table rows for <code>conditionLabel1</code> and <code>conditionLabel2</code> describe the slopes for the two contrasts of <code>conditionLabel</code>:</p>
<ul>
<li><p>Fixed effects: “overall” slope for each contrast</p></li>
<li><p>Random effects: degree of by-participant variability in the slope for each contrast (under <code>Variance</code> and <code>Std. Dev</code>)</p></li>
</ul>
<p>The correlation terms capture how much different random effect terms are correlated, across participants. For example, speakers who give a higher <code>rhymeRating</code> on average (the random intercept) also tend to show a larger rating difference between <em>bad</em> and <em>voice</em> rhymes (the first contrast).</p>
<p>Although there don’t seem to be any issues with fitting this model—it converges, and no random effect correlations are too high—often fitting maximal models with multi-level factors <em>does</em> lead to such issues, particularly if a non-orthogonal contrast coding scheme is used (such as dummy coding, R’s default).</p>
<!-- TODO FUTURE: use an example where we actually see these issues -->
</div>
<div id="lmem-mwrec" class="section level3">
<h3><span class="header-section-number">7.10.2</span> Models without random-effect correlations</h3>
<p>If we instead try to fit a model with uncorrelated random effects, we get a warning from R:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">modEx3 &lt;-<span class="st"> </span><span class="kw">lmer</span>(rhymeRating ~<span class="st"> </span>conditionLabel +<span class="st"> </span>(<span class="dv">1</span> +<span class="st"> </span>conditionLabel||participant), <span class="dt">data=</span>halfrhyme)</code></pre></div>
<pre><code>## Warning in checkConv(attr(opt, &quot;derivs&quot;), opt$par, ctrl = control$checkConv, : Model is nearly unidentifiable: large eigenvalue ratio
##  - Rescale variables?</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(modEx3)</code></pre></div>
<pre><code>## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [
## lmerModLmerTest]
## Formula: 
## rhymeRating ~ conditionLabel + (1 + conditionLabel || participant)
##    Data: halfrhyme
## 
## REML criterion at convergence: 3826.5
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -4.6442 -0.4262 -0.0567  0.5333  3.8688 
## 
## Random effects:
##  Groups        Name                Variance  Std.Dev. Corr       
##  participant   (Intercept)         8.833e-05 0.009398            
##  participant.1 conditionLabelbad   1.623e-01 0.402815            
##                conditionLabelvoice 1.230e+00 1.109149  0.66      
##                conditionLabelgood  1.320e-01 0.363301 -0.23  0.37
##  Residual                          1.253e+00 1.119530            
## Number of obs: 1205, groups:  participant, 31
## 
## Fixed effects:
##                 Estimate Std. Error       df t value Pr(&gt;|t|)    
## (Intercept)      3.63504    0.09918 30.04544  36.652  &lt; 2e-16 ***
## conditionLabel1  0.89985    0.09086 29.89489   9.904 5.94e-11 ***
## conditionLabel2  1.42441    0.05166 30.13975  27.573  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Correlation of Fixed Effects:
##             (Intr) cndtL1
## conditnLbl1  0.677       
## conditnLbl2 -0.511 -0.335
## convergence code: 0
## Model is nearly unidentifiable: large eigenvalue ratio
##  - Rescale variables?</code></pre>
<p>which tells us something is wrong, and we should not trust this model. A “nearly unidentifiable” model usually means “you’ve included variables that are not independent”.</p>
<p>In this case, the issue is with the random effects. Rather than the random-effect table showing one row for each of the two <code>conditionLabel</code> contrasts, it contains one row per <em>level</em> of <code>conditionLabel</code>. R is incorrectly interpreting the notation <code>(1 + conditionLabel||participant)</code> as 3 independent levels, rather than 2 contrasts.<a href="#fn40" class="footnoteRef" id="fnref40"><sup>40</sup></a></p>
<p>This issue will arise <strong>whenever a factor is used in an uncorrelated random effect formula</strong> (one that contains <code>||</code>, or that uses <code>(0+X|group)</code> notation). To include factors in a model with uncorrelated random effects, we must first turn them into numeric variables—that is, the contrasts. (This is what R usually does under the hood, but not in this case.) We manually make a numeric variable for each contrast:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">halfrhyme$clabel.c1 &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(~conditionLabel, halfrhyme)[,<span class="dv">2</span>]
halfrhyme$clabel.c2 &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(~conditionLabel, halfrhyme)[,<span class="dv">3</span>]</code></pre></div>
<p>then include these variables in the random-effect formula, rather than the actual factor:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">modEx4 &lt;-<span class="st"> </span><span class="kw">lmer</span>(rhymeRating ~<span class="st"> </span>conditionLabel +<span class="st"> </span>(<span class="dv">1</span> +<span class="st"> </span>clabel.c1 +<span class="st"> </span>clabel.c2||participant), <span class="dt">data=</span>halfrhyme)
<span class="kw">summary</span>(modEx4)</code></pre></div>
<pre><code>## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [
## lmerModLmerTest]
## Formula: rhymeRating ~ conditionLabel + (1 + clabel.c1 + clabel.c2 ||  
##     participant)
##    Data: halfrhyme
## 
## REML criterion at convergence: 3860
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -4.2957 -0.5188 -0.0262  0.5393  3.8347 
## 
## Random effects:
##  Groups        Name        Variance Std.Dev.
##  participant   (Intercept) 0.30396  0.5513  
##  participant.1 clabel.c1   0.23903  0.4889  
##  participant.2 clabel.c2   0.06519  0.2553  
##  Residual                  1.24808  1.1172  
## Number of obs: 1205, groups:  participant, 31
## 
## Fixed effects:
##                 Estimate Std. Error       df t value Pr(&gt;|t|)    
## (Intercept)      3.63410    0.10596 30.02772  34.297  &lt; 2e-16 ***
## conditionLabel1  0.90151    0.09733 30.01033   9.262 2.63e-10 ***
## conditionLabel2  1.42488    0.05420 30.28367  26.289  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Correlation of Fixed Effects:
##             (Intr) cndtL1
## conditnLbl1 -0.061       
## conditnLbl2  0.043  0.059</code></pre>
<p>This model has no convergence issues, and shows the expected random effect structure: one row for each of the <code>conditionLabel</code> contrast random slopes, and no correlations between random-effect terms.</p>
<p>Comparing the fixed effects of this “maximal model with uncorrelated random effects” to the actual “maximal model” (model <code>modEx2</code> above), we see they are very similar—modulo the lack of correlations.<a href="#fn41" class="footnoteRef" id="fnref41"><sup>41</sup></a></p>
<p>Summary:</p>
<ul>
<li><p>To include a random slope for any factor (even if just 2 levels) in a model with uncorrelated random effects, you need to code as numeric variable(s).</p></li>
<li><p>For a factor with 2 levels, you can just use a centered version (e.g. <code>arm::rescale</code>), which is essentially sum coding.</p></li>
<li><p>For factors with more levels (or to use dummy coding for a two-level factor), you manually extract the contrasts to make numeric variables.</p></li>
</ul>
</div>
</div>
<div id="other-readings-1" class="section level2">
<h2><span class="header-section-number">7.11</span> Other readings</h2>
<p>Mixed-effects models are widely used in behavioral and social sciences. Many full-length treatments and shorter introductions are available, including:</p>
<ul>
<li><p>Books</p>
<ul>
<li><p>About half of <span class="citation">Gelman &amp; Hill (<a href="#ref-gelman2007data">2007</a>)</span> is focused on mixed models, called “hierarchical” (different name, same models).</p></li>
<li><p>Textbooks on statistics for linguists, such as <span class="citation">R. Baayen (<a href="#ref-baayen2008analyzing">2008</a>)</span>, <span class="citation">R. Levy (<a href="#ref-levy2012probabilistic">2012</a>)</span>, <span class="citation">Johnson (<a href="#ref-johnson2008quantitative">2008</a>)</span> have mixed model sections.</p></li>
<li><p><span class="citation">Speelman, Heylen, &amp; Geeraerts (<a href="#ref-speelman2018mixed">2018</a>)</span> is an edited volume of case studies which apply mixed-effects models to linguistic data.</p></li>
</ul></li>
<li><p>Shorter</p>
<ul>
<li><p>Bodo Winter’s introductory <a href="http://www.bodowinter.com/tutorials.html">tutorials</a> on linear mixed models</p></li>
<li><p>First two lectures of Martijn Wieling’s <a href="http://www.let.rug.nl/~wieling/statscourse/">“advanced regression for linguists”</a> course.</p></li>
</ul></li>
<li><p>Troubleshooting/specfic questions:</p>
<ul>
<li><p>Ben Bolker’s <a href="https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html">FAQ</a> for mixed models, largely focused on R.</p></li>
<li><p>The <code>r-sig-mixed-models</code> mailing list. (Search the archives)</p></li>
</ul></li>
</ul>
</div>
<div id="c6extraexamples" class="section level2">
<h2><span class="header-section-number">7.12</span> Appendix: Extra examples</h2>
<div id="lmm-simulation-confint" class="section level3">
<h3><span class="header-section-number">7.12.1</span> Predicting confidence intervals by simulation</h3>
<p>To get 95% CIs of our Model 1B:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## a simple recipe for simulating 95% confidence intervals over model predictions

## set up dataframe to predict for (every participant, every value of the predictor)
newdata &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">expand.grid</span>(<span class="dt">conditionLabel=</span><span class="kw">unique</span>(givenness$conditionLabel),
                                  <span class="dt">participant=</span><span class="kw">unique</span>(givenness$participant)))

## (simulate 10k times from the model, for newdata)
preds &lt;-<span class="st"> </span><span class="kw">simulate</span>(mod1b<span class="fl">.1</span>, <span class="dt">newdata=</span>newdata,<span class="dt">nsim=</span><span class="dv">10000</span>)

## lower and upper 95% CIs
newdata$lower &lt;-<span class="st"> </span><span class="kw">apply</span>(preds, <span class="dv">1</span>, function(x){<span class="kw">quantile</span>(x,<span class="fl">0.025</span>)})
newdata$upper &lt;-<span class="st"> </span><span class="kw">apply</span>(preds, <span class="dv">1</span>, function(x){<span class="kw">quantile</span>(x,<span class="fl">0.975</span>)})
newdata$pred &lt;-<span class="st"> </span><span class="kw">apply</span>(preds, <span class="dv">1</span>, function(x){<span class="kw">quantile</span>(x,<span class="fl">0.50</span>)})


## newdata now contains lower and upper bounds of 95% CI, with &#39;prediction&#39; = median from simulations</code></pre></div>
</div>
<div id="random-intercept-and-slope-model-for-givenness-data" class="section level3">
<h3><span class="header-section-number">7.12.2</span> Random intercept and slope model for <code>givenness</code> data</h3>
<p>We would like to check whether the <code>conditionLabel.williams</code> effect differs by participant as well:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">givenness$participant &lt;-<span class="st"> </span><span class="kw">as.factor</span>(givenness$participant)

<span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>conditionLabel, <span class="dt">y=</span>acoustics), <span class="dt">data=</span>givenness) +
<span class="st">  </span><span class="kw">stat_summary</span>(<span class="dt">fun.data=</span><span class="st">&quot;mean_cl_boot&quot;</span>, <span class="kw">aes</span>(<span class="dt">color=</span>participant),<span class="dt">size=</span><span class="fl">0.1</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">stat_summary</span>(<span class="dt">fun.y=</span>mean, <span class="dt">geom=</span><span class="st">&#39;line&#39;</span>, <span class="kw">aes</span>(<span class="dt">group=</span>participant,<span class="dt">color=</span>participant)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">facet_wrap</span>(~participant) +<span class="st"> </span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position=</span><span class="st">&#39;none&#39;</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Condition&quot;</span>)</code></pre></div>
<p><img src="07-linear-mixed-models_files/figure-html/unnamed-chunk-58-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>To do so, we fit a model with a by-participant random intercept and a by-participant random slope for <code>conditionLabel.williams</code> (as explained in Sec. <a href="lmem.html#c6lmm2">7.3</a>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod1c.not &lt;-<span class="st"> </span><span class="kw">lmer</span>(acoustics ~<span class="st"> </span>conditionLabel.williams +<span class="st"> </span>(<span class="dv">1</span> +<span class="st"> </span>conditionLabel.williams||participant), <span class="dt">data=</span>givenness)</code></pre></div>
<p><code>(1 + conditionLabel.williams||participant)</code> is the term for an (uncorrelated) by-subject random intercept and by-subject random slope.</p>
<p>Model output:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(mod1c.not)</code></pre></div>
<pre><code>## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [
## lmerModLmerTest]
## Formula: 
## acoustics ~ conditionLabel.williams + (1 + conditionLabel.williams ||  
##     participant)
##    Data: givenness
## 
## REML criterion at convergence: 897.9
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -3.0538 -0.7129  0.0083  0.6540  3.3136 
## 
## Random effects:
##  Groups        Name                    Variance Std.Dev.
##  participant   (Intercept)             0.08937  0.299   
##  participant.1 conditionLabel.williams 0.00000  0.000   
##  Residual                              0.55800  0.747   
## Number of obs: 382, groups:  participant, 27
## 
## Fixed effects:
##                          Estimate Std. Error        df t value Pr(&gt;|t|)
## (Intercept)              -0.71856    0.06916  26.21655  -10.39 8.66e-11
## conditionLabel.williams   0.32626    0.07677 356.75848    4.25 2.73e-05
##                            
## (Intercept)             ***
## conditionLabel.williams ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Correlation of Fixed Effects:
##             (Intr)
## cndtnLbl.wl 0.002</code></pre>
<p>Under <code>Random effects</code> we find the variance estimator for the by-participant random slope to be effectively 0 (<span class="math inline">\(6.255\cdot 10^{-16}\)</span>). Thus, the model’s predictions are the same as <a href="#c6model1B">Model 1B</a>—there is no (detectable) by-subject variability in the effect of <code>conditionLabel.williams</code>.</p>
</div>
</div>
<div id="c6extendedexercise" class="section level2">
<h2><span class="header-section-number">7.13</span> Appendix: Extended exercise</h2>
<p>In this exercise you’ll build a linear mixed-effects model using:</p>
<ul>
<li>Data: <a href="mixed-effects-logistic-regression.html#c7tapdata">tapping data</a></li>
</ul>
<p>of how <code>vowelduration</code> (response) depends on the predictors <code>syntax</code> and <code>speechrate</code>.</p>
<ul>
<li>Response: <code>vowelduration</code></li>
</ul>
<p>The empirical effect is: <img src="07-linear-mixed-models_files/figure-html/unnamed-chunk-61-1.png" width="384" style="display: block; margin: auto;" /></p>
<p>The fixed effects to be included in the model are <code>syntax</code>, <code>speechrate</code>, and their interaction.</p>
<p><em>Part 1</em></p>
<ul>
<li><p>Make normalized predictors (values: <span class="math inline">\(\approx -0.5, +0.5\)</span>):</p>
<ul>
<li><p><code>syntax.trans</code> (+ = transitive, - = intransitive)</p></li>
<li><p><code>speechrate.slow</code> (+ = slow, - = fast)</p></li>
</ul></li>
<li><p>For fixed effects <code>syntax.trans</code>, <code>speechrate.slow</code>, <code>syntax.trans:speechrate.slow</code>:</p>
<ul>
<li><p>What are all possible random slope terms?</p></li>
<li><p>Hint: both by-participant and by-item random effects are possible, but some random slopes are not possible.</p></li>
</ul></li>
</ul>
<p><em>Part 2</em>:</p>
<p>Fit an LME for these fixed effects</p>
<ul>
<li><p>With by-item, by-participant random intercepts and all possible slopes</p></li>
<li><p>Uncorrelated random effects</p></li>
</ul>
<p><em>Part 3</em>:</p>
<ul>
<li><p>Interpret the results of the model: are the fixed-effect coefficients in the expected direction, given the empirical data?</p></li>
<li><p>Compare the random slope sizes to the fixed-effect sizes for the same predictors (e.g. compare the fixed effect for <code>speechrate.slow</code> to the the by-participant random slope effect “standard deviation” for <code>speechrate.slow</code>). Do the three effects ever switch directions (for some participants, items)?</p></li>
</ul>
<p><strong>Solution</strong> model:</p>
<div class="fold s o">

<pre><code>## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [
## lmerModLmerTest]
## Formula: 
## vowelduration ~ syntax.trans * speechrate.slow + (1 + syntax.trans *  
##     speechrate.slow || item) + (1 + syntax.trans * speechrate.slow ||  
##     participant)
##    Data: tapped
## 
## REML criterion at convergence: -2911.9
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -3.8002 -0.5148 -0.0247  0.5248  5.3484 
## 
## Random effects:
##  Groups        Name                         Variance  Std.Dev.
##  participant   syntax.trans:speechrate.slow 0.000e+00 0.000000
##  participant.1 speechrate.slow              4.459e-04 0.021115
##  participant.2 syntax.trans                 4.842e-05 0.006958
##  participant.3 (Intercept)                  3.966e-04 0.019916
##  item          syntax.trans:speechrate.slow 0.000e+00 0.000000
##  item.1        speechrate.slow              1.245e-04 0.011158
##  item.2        syntax.trans                 4.895e-05 0.006996
##  item.3        (Intercept)                  1.428e-03 0.037785
##  Residual                                   7.831e-04 0.027983
## Number of obs: 721, groups:  participant, 23; item, 8
## 
## Fixed effects:
##                                Estimate Std. Error         df t value
## (Intercept)                    0.101448   0.014029   8.381835   7.232
## syntax.trans                  -0.028215   0.003546   9.113087  -7.956
## speechrate.slow                0.043487   0.006269  18.830504   6.937
## syntax.trans:speechrate.slow  -0.029350   0.004173 631.448408  -7.033
##                              Pr(&gt;|t|)    
## (Intercept)                  7.08e-05 ***
## syntax.trans                 2.15e-05 ***
## speechrate.slow              1.37e-06 ***
## syntax.trans:speechrate.slow 5.25e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Correlation of Fixed Effects:
##             (Intr) syntx. spchr.
## syntax.trns  0.000              
## spechrt.slw  0.000  0.002       
## syntx.trn:.  0.001 -0.001  0.000</code></pre>
<p>The rest of the solution is omitted.</p>
</div>
<div id="c6solns" class="section level2">
<h2><span class="header-section-number">7.14</span> Solutions</h2>
<p><strong>Q</strong>: What does this mean? (That the fixed-effect coefficients are the same in the predictions made by Model 1A and Model 1B?)</p>
<p><strong>A</strong>: The predictions of Model 1A (simple linear regression) are (basically) the same as the Model 1B makes for an “average speaker”.</p>
<hr />
<p><strong>Q</strong>: How much variability is there in participants’ intercept values?</p>
<p><strong>A</strong>: The estimated by-participant intercept variance is <span class="math inline">\(\hat{\sigma}_{s,0} = 1.267\)</span>.</p>
<hr />
<p><strong>Q</strong>: What is the interpretation of each random intercept in this example? For example, what do <span class="math inline">\(\alpha_{par,2}\)</span> and <span class="math inline">\(\alpha_{item,5}\)</span> mean?</p>
<p><strong>A</strong>: The random intercepts are the offsets to the intercept for each participant and item. <span class="math inline">\(\alpha_{par,2}\)</span> is the offset for participant 2 (the difference between their intercept value and <span class="math inline">\(\beta_0\)</span>), and <span class="math inline">\(\alpha_{item,5}\)</span> is the offset for item 5.</p>
<hr />
<p><strong>Q</strong>:</p>
<ul>
<li><p>Is there more variability among participants or items?</p></li>
<li><p>This pattern is common in laboratory experiments. Why?</p></li>
</ul>
<p><strong>A</strong>: There is more variability among participants. This pattern is common in laboratory experiments because items are constructed by the researcher to vary in a particular way, while the participants in an experiment are a truly random sample. This pattern might reverse if we had all linguistics majors whose first language is English read a carefully chosen word list, then examined vowel formants in these words, for example.</p>
<hr />
<p><strong>Exercise 1</strong>:</p>
<ul>
<li><p>In Model 2A:</p>
<ul>
<li><p>95% of subjects have intercepts between _____ and _____</p></li>
<li><p>95% of items have intercepts between _____ and _____</p></li>
</ul></li>
<li><p>What is the residual error for these three models, fit with the same fixed effects (<code>acoustics ~ conditionLabel.williams</code>):</p>
<ul>
<li><p>Model 1A (simple linear regression) (fit <code>mod1a</code>, then <code>summary(mod1a)</code>)</p></li>
<li><p>Model 1B (by-participant random intercept)</p></li>
<li><p>Model 2A (by-participant and by-word random intercept)</p></li>
</ul></li>
<li><p>Why does the pattern you see make sense?</p></li>
</ul>
<p><strong>A</strong>: (Not shown: 95% bounds, and the actual residual error for each model.) The pattern is that the more random intercept terms are included, the more the residual error goes down. This is because the error in the original simple regression model has been “parcelled out” to variability among participants and items.</p>
<!-- TODO FUTURE:95% bounds, and the actual residual error for each model. -->
<hr />
<p><strong>Q</strong>: What would it mean, intuitively, if there were a strong positive relationship (“positive correlation”) between (1) and (2)? (“Participants who ____ also have a higher ____.”)</p>
<p><strong>A</strong>: Participants with higher values of <code>rhymeRating</code> (the response) also have a steeper slope of <code>relDuration</code> (the predictor).</p>
<hr />
<p><strong>Q</strong>: What is the interpretation of each contrast?</p>
<p><strong>A</strong>: Contrast 1: <em>bad</em> versus <em>voice</em>. Contrast 2: <em>good</em> versus <em>bad</em>/<em>voice</em>.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-baayen2008mixed">
<p>Baayen, R., Davidson, D., &amp; Bates, D. (2008). Mixed-effects modeling with crossed random effects for subjects and items. <em>Journal of Memory and Language</em>, <em>59</em>(4), 390–412.</p>
</div>
<div id="ref-salverda2003role">
<p>Salverda, A. P., Dahan, D., &amp; McQueen, J. M. (2003). The role of prosodic boundaries in the resolution of lexical embedding in speech comprehension. <em>Cognition</em>, <em>90</em>(1), 51–89.</p>
</div>
<div id="ref-barr2013random">
<p>Barr, D. J., Levy, R., Scheepers, C., &amp; Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. <em>Journal of Memory and Language</em>, <em>68</em>(3), 255–278.</p>
</div>
<div id="ref-baayen2008analyzing">
<p>Baayen, R. (2008). <em>Analyzing linguistic data</em>. Cambridge: Cambridge University Press.</p>
</div>
<div id="ref-bates2014fitting">
<p>Bates, D., Mächler, M., Bolker, B., &amp; Walker, S. (2014). Fitting linear mixed-effects models using lme4. <em>ArXiv Preprint ArXiv:1406.5823</em>.</p>
</div>
<div id="ref-gelman2007data">
<p>Gelman, A., &amp; Hill, J. (2007). <em>Data analysis using regression and multilevel/hierarchical models</em>. Cambridge: Cambridge University Press.</p>
</div>
<div id="ref-snijders2011multilevel">
<p>Snijders, T., &amp; Bosker, R. (2011). <em>Multilevel analysis</em> (2nd ed.). London: Sage.</p>
</div>
<div id="ref-matuschek2017balancing">
<p>Matuschek, H., Kliegl, R., Vasishth, S., Baayen, H., &amp; Bates, D. (2017). Balancing Type I error and power in linear mixed models. <em>Journal of Memory and Language</em>, <em>94</em>, 305–315.</p>
</div>
<div id="ref-bates2015parsimonious">
<p>Bates, D., Kliegl, R., Vasishth, S., &amp; Baayen, H. (2015). Parsimonious mixed models. <em>ArXiv Preprint ArXiv:1506.04967</em>.</p>
</div>
<div id="ref-levy2012probabilistic">
<p>Levy, R. (2012). <em>Probabilistic methods in the study of language</em>.</p>
</div>
<div id="ref-johnson2008quantitative">
<p>Johnson, K. (2008). <em>Quantitative methods in linguistics</em>. Malden, MA: Wiley-Blackwell.</p>
</div>
<div id="ref-speelman2018mixed">
<p>Speelman, D., Heylen, K., &amp; Geeraerts, D. (Eds.). (2018). <em>Mixed-effects regression models in linguistics</em>. Springer.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="29">
<li id="fn29"><p>This distinction is discussed, along with an introduction to EDA and motivation for its importance in data analysis, in a lecture that has not yet been turned into a chapter of this book. Until it is, this section just assumes you have read about EDA and CDA somewhere, such as the first sections of <span class="citation">Behrens (<a href="#ref-behrens1997principles">1997</a>)</span>. Tukey’s original manifesto for combining EDA and CDA <span class="citation">(Tukey, <a href="#ref-tukey1980we">1980</a>)</span> is still valuable.<a href="lmem.html#fnref29">↩</a></p></li>
<li id="fn30"><p>See for example: <span class="citation">Johnson (<a href="#ref-johnson2008quantitative">2008</a>)</span>, Sec. 7.8 of <a href="https://www.sas.upenn.edu/~baron/from_cattell/rpsych/rpsych.html">this tutorial</a> (by Jonathan Baron and Yuelin Li), <a href="https://people.umass.edu/bwdillon/LING609/Lectures/Section3/Lecture21.html">these notes</a> by Brian Dillon, many other tutorials online.<a href="lmem.html#fnref30">↩</a></p></li>
<li id="fn31"><p>Grouping factors are often called <em>random factors</em> in RM-ANOVA analyses.<a href="lmem.html#fnref31">↩</a></p></li>
<li id="fn32"><p>Mixed models have become (by 2017) sufficiently popular that many language scientists, including journal article reviewers, think that an analysis of grouped data that doesn’t use mixed models is somehow “incorrect” or “not standard”. It is important to remember that there is always more than one valid way to analyze the same data. Mixed models are often appropriate for analyzing grouped data, but simpler methods—such as RM-ANOVA or even paired <span class="math inline">\(t\)</span> tests—may also be suitable in many cases, especially for simple statistical analyses where a mixed model may be overkill.<a href="lmem.html#fnref32">↩</a></p></li>
<li id="fn33"><p>More precisely: to compare two models with different fixed-effect terms, using any likelihood-based method (such as a LR test or AIC), the models must be fitted using maximum likelihood <span class="citation">(Faraway, <a href="#ref-faraway2016extending">2016</a>; Zuur, <a href="#ref-zuur2009extensions">2009</a>)</span>. See <a href="https://stats.stackexchange.com/questions/116770/reml-or-ml-to-compare-two-mixed-effects-models-with-differing-fixed-effects-but">here</a> for some discussion.<a href="lmem.html#fnref33">↩</a></p></li>
<li id="fn34"><p>This is the “Residual standard error” in the R summary of the model output (<code>summary(mod1a)</code>).<a href="lmem.html#fnref34">↩</a></p></li>
<li id="fn35"><p>Not accounting for by-item variability was famously termed the “language-as-fixed-effect fallacy” by <span class="citation">Clark (<a href="#ref-clark1973language">1973</a>)</span>.<a href="lmem.html#fnref35">↩</a></p></li>
<li id="fn36"><p>In many other fields where mixed models are used (e.g. education, ecology), crossed random effects only come up in complex designs. This is why <code>lme4</code> is particularly well-suited for modeling linguistic data—in part as a result of collaboration between its primary architect, the statistician Doug Bates, and the psycholinguist R. Harald Baayen. Many mixed-model software packages assume only one grouping factor, or nested grouping factors.<a href="lmem.html#fnref36">↩</a></p></li>
<li id="fn37"><p>The short version of “practical” is: if we want to use a <span class="math inline">\(t\)</span> test to calculate the significance of fixed effects—as we did for regression coefficients in a non-mixed-effects linear regression—it is unclear what the degrees of freedom should be, because it’s unclear how many independent pieces of information <span class="math inline">\(n\)</span> observations from the same group level (e.g., from a single participant) give: somewhere between 1 (because they’re all from 1 participant) and <span class="math inline">\(n-1\)</span> (the answer in linear regression). The short version of “philosophical” is: mixed-effects models can be thought of as Bayesian models, and in Bayesian statistics <span class="math inline">\(p\)</span>-values are not a meaningful concept.<a href="lmem.html#fnref37">↩</a></p></li>
<li id="fn38"><p>See also <span class="citation">Schielzeth &amp; Forstmeier (<a href="#ref-schielzeth2009conclusions">2009</a>)</span>, who had earlier emphasized the importance of random slopes for analyzing ecological data.<a href="lmem.html#fnref38">↩</a></p></li>
<li id="fn39"><p>This is one motivation for Bayesian mixed models: see <span class="citation">Vasishth &amp; Nicenboim (<a href="#ref-vasishth2016statistical">2016</a>)</span>.<a href="lmem.html#fnref39">↩</a></p></li>
<li id="fn40"><p>This is a bug, but one that follows from the way uncorrelated random effects are treated in <code>lmer</code> and how R deals with factors in model formulas without intercept terms.<a href="lmem.html#fnref40">↩</a></p></li>
<li id="fn41"><p>The random effect variances are slightly larger in the “uncorrelated” model, indicating that the amount of variability in the intercept and slopes is being over-estimated—some of this variability is actually shared between random-effect terms, as captured by the correlation terms.<a href="lmem.html#fnref41">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="practical-regression-topics-1-multi-level-factors-contrast-coding-interactions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="mixed-effects-logistic-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
